{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "    \n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, base_processing_class, combine_info_utils, further_processing_class\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class, cluster_analysis, patterns_and_features_class, compare_two_monkeys_class, monkey_landing_in_ff\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, process_GUAT_trials_class, GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data_utils\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics, monkey_heading_utils\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from planning_analysis.test_params_for_planning import params_test_combos_class, params_utils\n",
    "from visualization.plotly_tools import plotly_for_monkey, plotly_for_time_series, plotly_preparation, plotly_for_correlation\n",
    "from visualization.dash_tools import dash_prep_class, dash_utils, dash_utils, dash_comparison_class, dash_params_class\n",
    "from visualization.dash_tools.dash_main_class_methods import dash_main_class\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils\n",
    "from planning_analysis.only_cur_ff import only_cur_ff_utils, only_cur_ff_class, only_cur_ff_utils\n",
    "from planning_analysis.plan_factors import plan_factors_utils, build_factor_comp, plan_factors_class, monkey_plan_factors_x_sess_class\n",
    "from planning_analysis.agent_analysis import compare_monkey_and_agent_utils, agent_plan_factors_class, agent_plan_factors_x_sess_class\n",
    "from planning_analysis.plan_factors import test_vs_control_utils\n",
    "from planning_analysis.factors_vs_indicators import make_variations_utils, plot_variations_utils, process_variations_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_class, show_planning_utils, examine_null_arcs\n",
    "from planning_analysis.show_planning.cur_vs_nxt_ff import cvn_helper_class, find_cvn_utils, plot_cvn_class, plot_cvn_utils, plot_monkey_heading_helper_class, cvn_from_ref_class\n",
    "from machine_learning.ml_methods import ml_methods_class, prep_ml_data_utils\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_utils\n",
    "from pattern_discovery.learning import show_learning, analyze_ratio_trend, print_results\n",
    "import numpy as np, pandas as pd, statsmodels.api as sm, statsmodels.formula.api as smf\n",
    "from data_wrangling import specific_utils, process_monkey_information, retrieve_raw_data, time_calib_utils\n",
    "from pattern_discovery import make_ff_dataframe\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from importlib import reload\n",
    "from eye_position_analysis import eye_positions\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import cm\n",
    "from os.path import exists\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "from scipy.stats import rankdata\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, multilabel_confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import pi\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import plotly.graph_objects as go\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = compare_two_monkeys_class.CompareTwoMonkeys()\n",
    "tm.compare_monkeys(exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(further_processing_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Schro')\n",
    "\n",
    "all_trial_durations_df = pd.DataFrame()\n",
    "all_stop_df = pd.DataFrame()\n",
    "VBLO_df = pd.DataFrame()\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    if row['finished'] is True:\n",
    "        continue\n",
    "\n",
    "    data_name = row['data_name']\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], data_name)\n",
    "    print(raw_data_folder_path)\n",
    "    data_item = further_processing_class.FurtherProcessing(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "    \n",
    "    # disable printing\n",
    "    data_item.retrieve_or_make_monkey_data()\n",
    "    data_item.make_or_retrieve_ff_dataframe()\n",
    "    \n",
    "    # data_item.ff_caught_T_sorted, data_item.ff_index_sorted, data_item.ff_real_position_sorted, data_item.ff_believed_position_sorted, data_item.ff_life_sorted, \\\n",
    "    #     data_item.ff_flash_end_sorted = retrieve_raw_data.make_or_retrieve_ff_info_from_txt_data(\n",
    "    #         data_item.raw_data_folder_path)\n",
    "    # # data_item.make_or_retrieve_closest_stop_to_capture_df()\n",
    "    # # data_item.make_ff_caught_T_new()\n",
    "    \n",
    "    \n",
    "    trial_durations = np.diff(data_item.ff_caught_T_new)\n",
    "    trial_durations_df = pd.DataFrame(\n",
    "        {'duration_sec': trial_durations, 'trial_index': np.arange(len(trial_durations))})\n",
    "    trial_durations_df['data_name'] = data_name\n",
    "    all_trial_durations_df = pd.concat(\n",
    "        [all_trial_durations_df, trial_durations_df])\n",
    "    \n",
    "    num_stops = data_item.monkey_information.loc[data_item.monkey_information['whether_new_distinct_stop'] == True, ['time']].shape[0]     \n",
    "    num_captures = len(data_item.ff_caught_T_new)\n",
    "    stop_df = pd.DataFrame(\n",
    "        {\n",
    "            'stops': [num_stops],\n",
    "            'captures': [num_captures],\n",
    "            'data_name': [data_name],\n",
    "        }\n",
    "    )\n",
    "    all_stop_df = pd.concat([all_stop_df, stop_df])\n",
    "    \n",
    "    data_item.get_visible_before_last_one_trials_info()\n",
    "    num_VBLO_trials = len(data_item.vblo_target_cluster_df)\n",
    "    all_selected_base_trials = len(data_item.selected_base_trials)\n",
    "    VBLO_df = pd.DataFrame(\n",
    "        {\n",
    "            'VBLO_trials': [num_VBLO_trials],\n",
    "            'base_trials': [all_selected_base_trials],\n",
    "            'data_name': [data_name],\n",
    "        }\n",
    "    )\n",
    "    VBLO_df = pd.concat([VBLO_df, VBLO_df])\n",
    "\n",
    "    \n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df, 'session')\n",
    "all_stop_df = make_variations_utils.assign_session_id(all_stop_df, 'session')\n",
    "VBLO_df = make_variations_utils.assign_session_id(VBLO_df, 'session')\n",
    "\n",
    "capture_and_stop_df = all_stop_df[['session', 'captures']].copy()\n",
    "capture_and_stop_df['stops'] = 1\n",
    "\n",
    "os.makedirs(f'{data_item.monkey_name}', exist_ok=True)\n",
    "all_trial_durations_df.to_csv(f'{data_item.monkey_name}/all_trial_durations_df.csv', index=False)\n",
    "all_stop_df.to_csv(f'{data_item.monkey_name}/all_stop_df.csv', index=False)\n",
    "VBLO_df.to_csv(f'{data_item.monkey_name}/VBLO_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trial_durations_df = pd.read_csv(f'{data_item.monkey_name}/all_trial_durations_df.csv', index_col=False)\n",
    "all_stop_df = pd.read_csv(f'{data_item.monkey_name}/all_stop_df.csv', index_col=False)\n",
    "VBLO_df = pd.read_csv(f'{data_item.monkey_name}/VBLO_df.csv', index_col=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = compare_two_monkeys_class.CompareTwoMonkeys()\n",
    "tm.compare_monkeys(exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monkey = tm.combd_pattern_frequencies[tm.combd_pattern_frequencies['Monkey'] == 'Schro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = 'two_in_a_row'\n",
    "\n",
    "def show_event_ratio(df_monkey, event):\n",
    "    df_event = df_monkey[df_monkey['Item'] == event].sort_values(by='Session').reset_index(drop=True)\n",
    "\n",
    "    event_count_col = event\n",
    "    denom_count_col = \"all_trial_count\"\n",
    "\n",
    "    df_event.rename(columns={'Session': 'session',\n",
    "                            'Frequency':event_count_col,\n",
    "                            'N_total': denom_count_col,\n",
    "                            }, inplace=True)\n",
    "\n",
    "    analyze_ratio_trend.evaluate_ratio_trend(df_event, event_count_col=event_count_col, denom_count_col=denom_count_col)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # df_event = df_monkey[df_monkey['Item'] == event].sort_values(by='Session').reset_index(drop=True)\n",
    "\n",
    "    # event_count_col = event\n",
    "    # denom_count_col = \"all_trial_count\"\n",
    "\n",
    "    # df_event.rename(columns={'Session': 'session',\n",
    "    #                         'Frequency':event_count_col,\n",
    "    #                         'N_total': denom_count_col,\n",
    "    #                         }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in df_monkey['Item'].unique():\n",
    "    try:\n",
    "        show_event_ratio(df_monkey, event)\n",
    "    except:\n",
    "        print(f\"Error for event: {event}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_changes_in_pattern_frequencies_over_time(self, multiple_monkeys=False):\n",
    "    plot_change_over_time.plot_the_changes_over_time_in_long_df(self.combd_pattern_frequencies, x=\"Session\", y=\"Rate\",\n",
    "                                                                multiple_monkeys=multiple_monkeys, monkey_name='monkey_Bruno',\n",
    "                                                                category_order=self.pattern_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VBLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(analyze_ratio_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_col = \"VBLO_trials\"\n",
    "denom_count_col = \"base_trials\"\n",
    "\n",
    "analyze_ratio_trend.evaluate_ratio_trend(all_VBLO_df, event_count_col=event_count_col, denom_count_col=denom_count_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captures over stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_col = \"captures\"\n",
    "denom_count_col = \"stops\"\n",
    "\n",
    "analyze_ratio_trend.evaluate_ratio_trend(all_stop_df, event_count_col=event_count_col, denom_count_col=denom_count_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some more things to try here:\n",
    "https://chatgpt.com/g/g-p-68a34392716c8191839b81db389836c7-multiff/c/68a7b6a5-04e0-8332-aa89-8c4235fe4c4b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, statsmodels.api as sm, statsmodels.formula.api as smf\n",
    "\n",
    "# 1) Filter and clean durations FIRST\n",
    "df_trials = all_trial_durations_df.query(\"duration_sec < 30\").copy()\n",
    "\n",
    "df_trials[\"duration_sec\"] = df_trials[\"duration_sec\"].clip(lower=1e-6)\n",
    "\n",
    "\n",
    "# 2) Build session-level aggregates from the cleaned trials\n",
    "df_sessions = (\n",
    "    df_trials.groupby(\"session\", as_index=False)\n",
    "    .agg(captures=(\"duration_sec\", \"size\"),\n",
    "         total_duration=(\"duration_sec\", \"sum\"))\n",
    ")\n",
    "\n",
    "# If you used drop (A), total_duration should be >0. Still, be safe:\n",
    "df_sessions[\"total_duration\"] = df_sessions[\"total_duration\"].clip(lower=1e-12)\n",
    "\n",
    "# 3) Logs (after cleaning)\n",
    "df_trials[\"logT\"] = np.log(df_trials[\"duration_sec\"])\n",
    "offset = np.log(df_sessions[\"total_duration\"])\n",
    "\n",
    "# 4) Models\n",
    "po = smf.glm(\n",
    "    \"captures ~ session\",\n",
    "    data=df_sessions,\n",
    "    family=sm.families.Poisson(),\n",
    "    offset=offset\n",
    ").fit(cov_type=\"HC0\")\n",
    "\n",
    "\n",
    "ols = smf.ols(\"logT ~ session\", data=df_trials).fit(\n",
    "    cov_type=\"cluster\", cov_kwds={\"groups\": df_trials[\"session\"]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Poisson ---\n",
    "po_ci = po.conf_int().loc[\"session\"]\n",
    "po_dict = {\n",
    "    \"model\": \"Poisson (captures/min)\",\n",
    "    \"estimate_per_session\": float(po.params[\"session\"]),\n",
    "    \"effect_per_10_sessions\": float(np.exp(10 * po.params[\"session\"])),\n",
    "    \"95%_CI\": (np.exp(10 * po_ci[0]), np.exp(10 * po_ci[1])),\n",
    "    \"p_value\": float(po.pvalues[\"session\"]),\n",
    "    \"scale\": \"rate ratio\"\n",
    "}\n",
    "\n",
    "# --- OLS ---\n",
    "ols_ci = ols.conf_int().loc[\"session\"]\n",
    "ols_dict = {\n",
    "    \"model\": \"OLS (log-duration)\",\n",
    "    \"estimate_per_session\": float(ols.params[\"session\"]),\n",
    "    \"effect_per_10_sessions\": float((np.exp(10 * ols.params[\"session\"]) - 1) * 100),\n",
    "    \"95%_CI\": (\n",
    "        (np.exp(10 * ols_ci[0]) - 1) * 100,\n",
    "        (np.exp(10 * ols_ci[1]) - 1) * 100\n",
    "    ),\n",
    "    \"p_value\": float(ols.pvalues[\"session\"]),\n",
    "    \"scale\": \"% change\"\n",
    "}\n",
    "\n",
    "# Combine into one table\n",
    "summary_df = pd.DataFrame([po_dict, ols_dict])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_learning.plot_poisson_rate_fit(df_sessions, po)\n",
    "show_learning.plot_duration_fit(df_trials, ols)\n",
    "\n",
    "# Get early vs late results\n",
    "(rate_phase, rate_ttest, rate_glm, rate_effect) = show_learning.summarize_early_late_rate_with_glm(df_sessions)\n",
    "(dur_phase,  dur_ttest,  dur_glm,  dur_effect ) = show_learning.summarize_early_late_duration_with_glm(df_trials, df_sessions)\n",
    "print_results.show_all_pretty_tables(rate_phase, rate_ttest, rate_glm, rate_effect,\n",
    "                       dur_phase,  dur_ttest,  dur_glm,  dur_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly get all_trial_durations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Schro')\n",
    "\n",
    "all_trial_durations_df = pd.DataFrame()\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    if row['finished'] is True:\n",
    "        continue\n",
    "\n",
    "    data_name = row['data_name']\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], data_name)\n",
    "    print(raw_data_folder_path)\n",
    "    data_item = further_processing_class.FurtherProcessing(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "    # data_item.retrieve_or_make_monkey_data()\n",
    "    \n",
    "    data_item.ff_caught_T_sorted, data_item.ff_index_sorted, data_item.ff_real_position_sorted, data_item.ff_believed_position_sorted, data_item.ff_life_sorted, \\\n",
    "        data_item.ff_flash_end_sorted = retrieve_raw_data.make_or_retrieve_ff_info_from_txt_data(\n",
    "            data_item.raw_data_folder_path)\n",
    "    # data_item.make_or_retrieve_closest_stop_to_capture_df()\n",
    "    # data_item.make_ff_caught_T_new()\n",
    "    \n",
    "    trial_durations = np.diff(data_item.ff_caught_T_sorted)\n",
    "    trial_durations_df = pd.DataFrame(\n",
    "        {'duration_sec': trial_durations, 'trial_index': np.arange(len(trial_durations))})\n",
    "    trial_durations_df['data_name'] = data_name\n",
    "    all_trial_durations_df = pd.concat(\n",
    "        [all_trial_durations_df, trial_durations_df])\n",
    "    \n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df)\n",
    "\n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df, 'session')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonparametric trend checks (and others)\n",
    "(a) Spearman: session vs geometric‑mean duration per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# geometric mean per session = exp(mean(log T))\n",
    "g = (\n",
    "    df_trials\n",
    "    .assign(logT=np.log(df_trials[\"duration_sec\"]))\n",
    "    .groupby(\"session\")[\"logT\"]\n",
    "    .mean()\n",
    "    .pipe(np.exp)\n",
    "    .rename(\"geom_mean_T\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rho, pval = spearmanr(g[\"session\"].values, g[\"geom_mean_T\"].values)\n",
    "print(f\"Spearman rho = {rho:.3f}, p = {pval:.3g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theil–Sen slope on log T (trial level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "X = df_trials[[\"session\"]].astype(float).values\n",
    "y = np.log(df_trials[\"duration_sec\"].values)\n",
    "\n",
    "ts = TheilSenRegressor(random_state=0)\n",
    "ts.fit(X, y)\n",
    "\n",
    "slope = ts.coef_[0]                    # per session (on log-scale)\n",
    "per10_pct = (np.exp(10*slope) - 1)*100 # % change per 10 sessions\n",
    "print(f\"Theil–Sen slope (logT vs session) = {slope:.5f} per session \"\n",
    "      f\"⇒ {per10_pct:+.1f}% per 10 sessions\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
