{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "    \n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, base_processing_class, combine_info_utils, further_processing_class, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class, cluster_analysis, patterns_and_features_class, compare_two_monkeys_class, monkey_landing_in_ff\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, add_features_GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data, process_agent_data\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics, monkey_heading_utils\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from planning_analysis.test_params_for_planning import params_test_combos_class, params_utils\n",
    "from visualization.plotly_tools import plotly_for_monkey, plotly_for_time_series, plotly_preparation, plotly_for_correlation\n",
    "from visualization.dash_tools import dash_prep_class, dash_utils, dash_utils, dash_comparison_class, dash_params_class\n",
    "from visualization.dash_tools.dash_main_class_methods import dash_main_class\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils\n",
    "from planning_analysis.only_cur_ff import only_cur_ff_utils, only_cur_ff_class, only_cur_ff_utils\n",
    "from planning_analysis.plan_factors import plan_factors_utils, build_factor_comp, plan_factors_class, monkey_plan_factors_x_sess_class\n",
    "from planning_analysis.agent_analysis import compare_monkey_and_agent_utils, agent_plan_factors_class, agent_plan_factors_x_sess_class\n",
    "from planning_analysis.plan_factors import test_vs_control_utils\n",
    "from planning_analysis.factors_vs_indicators import make_variations_utils, process_variations_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_class, show_planning_utils, examine_null_arcs\n",
    "from planning_analysis.show_planning.cur_vs_nxt_ff import cvn_helper_class, find_cvn_utils, plot_cvn_class, plot_cvn_utils, plot_monkey_heading_helper_class, cvn_from_ref_class\n",
    "from machine_learning.ml_methods import ml_methods_class, prep_ml_data_utils\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_utils\n",
    "from pattern_discovery.learning.capture_rate_and_duration import capture_stats_by_phase, capture_stats_trend, print_fit_results\n",
    "from pattern_discovery.learning.proportion_trend import analyze_proportion_trend, plot_stacked_bars_by_phase\n",
    "from pattern_discovery.learning.retry_analysis import prep_retry_data\n",
    "from pattern_discovery.learning import prep_learning_data\n",
    "\n",
    "import numpy as np, pandas as pd, statsmodels.api as sm, statsmodels.formula.api as smf\n",
    "from data_wrangling import specific_utils, process_monkey_information, retrieve_raw_data, time_calib_utils\n",
    "from pattern_discovery import make_ff_dataframe\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from importlib import reload\n",
    "from eye_position_analysis import eye_positions\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import cm\n",
    "from os.path import exists\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "from scipy.stats import rankdata\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, multilabel_confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import pi\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import plotly.graph_objects as go\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## key data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trial_durations_df, all_stop_df, all_VBLO_df = prep_learning_data.get_key_learning_data(monkey_name='monkey_Bruno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cluster_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = compare_two_monkeys_class.CompareTwoMonkeys()\n",
    "tm.compare_monkeys(exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Frequencies (As proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUAT and TAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give df_monkey to chatGPT and ask for stacked bar plot of event frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(analyze_proportion_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monkey = 'Schro'\n",
    "monkey = 'Schro'\n",
    "\n",
    "df_monkey = tm.combd_pattern_frequencies[tm.combd_pattern_frequencies['monkey'] == monkey]\n",
    "analyze_proportion_trend.show_event_proportion(df_monkey, \n",
    "                                     event='retry_over_miss',\n",
    "                                     title=f\"{monkey}: Retries vs. All Initial Misses\",\n",
    "                                     ylabel=\"\"\n",
    "                                     )\n",
    "\n",
    "analyze_proportion_trend.show_event_proportion(df_monkey, \n",
    "                                     event='retry_fail_over_miss',\n",
    "                                     title=f\"{monkey}: Retry-Then-Fail vs. Initial Misses\",\n",
    "                                     ylabel=\"\"\n",
    "                                     )\n",
    "\n",
    "analyze_proportion_trend.show_event_proportion(df_monkey, \n",
    "                                     event='retry_capture_over_miss',\n",
    "                                     title=f\"{monkey}: Retry-Then-Capture vs. Initial Misses\",\n",
    "                                     ylabel=\"\"\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_label_mapping = {\n",
    "    # Break down attempts\n",
    "    'first_shot_capture_over_attempt': 'Capture',\n",
    "    'no_retry_over_attempt': 'No Retry',\n",
    "    'retry_capture_over_attempt': 'Retry + Capture',\n",
    "    'retry_fail_over_attempt': 'Retry + Fail',\n",
    "    # Capture vs miss\n",
    "    'capture_over_miss': 'Capture',\n",
    "    'miss_over_attempt': 'Miss',\n",
    "\n",
    "}\n",
    "\n",
    "combd_pattern_frequencies = tm.combd_pattern_frequencies.copy()\n",
    "combd_pattern_frequencies['new_label'] = combd_pattern_frequencies['item'].map(new_label_mapping).fillna('')\n",
    "\n",
    "category_order = ['Capture', 'Miss']\n",
    "title = 'Attempt Outcomes Across Early and Late Sessions'\n",
    "y_label = 'Proportion of Attempts'\n",
    "plot_stacked_bars_by_phase.plot_outcomes_by_phase_side_by_side(combd_pattern_frequencies, category_order, title, y_label,\n",
    "                                             category_colors=['#0072B2', '#C76E00'])\n",
    "\n",
    "\n",
    "category_order = ['Capture', 'Retry + Capture', 'Retry + Fail', 'No Retry']\n",
    "title = 'Attempt Outcomes Across Early and Late Sessions'\n",
    "y_label = 'Proportion of Attempts'\n",
    "plot_stacked_bars_by_phase.plot_outcomes_by_phase_side_by_side(combd_pattern_frequencies, category_order, title, y_label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_mapping = {\n",
    "    'retry_capture_over_miss': 'Retry + Capture',\n",
    "    'retry_fail_over_miss': 'Retry + Fail',\n",
    "    'no_retry_over_miss': 'No Retry',\n",
    "}\n",
    "\n",
    "combd_pattern_frequencies = tm.combd_pattern_frequencies.copy()\n",
    "combd_pattern_frequencies['new_label'] = combd_pattern_frequencies['item'].map(new_label_mapping).fillna('')\n",
    "\n",
    "\n",
    "category_order = ['Retry + Capture', 'Retry + Fail', 'No Retry']\n",
    "title = 'Actions After a Miss Across Early and Late Sessions'\n",
    "y_label = 'Proportion of Misses'\n",
    "plot_stacked_bars_by_phase.plot_outcomes_by_phase_side_by_side(combd_pattern_frequencies, category_order, title, y_label,\n",
    "                                                          category_colors = ['#CC79A7', '#E69F00', '#009E73'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_label_mapping = {\n",
    "    'eventual_capture_over_attempt': 'Capture',\n",
    "    'eventual_miss_over_attempt': 'Miss',\n",
    "}\n",
    "\n",
    "combd_pattern_frequencies = tm.combd_pattern_frequencies.copy()\n",
    "combd_pattern_frequencies['new_label'] = combd_pattern_frequencies['item'].map(new_label_mapping).fillna('')\n",
    "\n",
    "category_order = ['Capture', 'Miss']\n",
    "title = 'Eventual Attempt Outcomes Across Early and Late Sessions'\n",
    "y_label = 'Proportion of Attempts'\n",
    "plot_stacked_bars_by_phase.plot_outcomes_by_phase_side_by_side(combd_pattern_frequencies, category_order, title, y_label,\n",
    "                                                               category_colors=['#4e79a7', '#98df8a'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_mapping = {\n",
    "    'TAFT_over_both': 'Retry + Capture',\n",
    "    'GUAT_over_both': 'Retry + Fail',\n",
    "}\n",
    "\n",
    "combd_pattern_frequencies = tm.combd_pattern_frequencies.copy()\n",
    "combd_pattern_frequencies['new_label'] = combd_pattern_frequencies['item'].map(new_label_mapping).fillna('')\n",
    "\n",
    "category_order = ['Retry + Capture', 'Retry + Fail']\n",
    "title = 'Actions After a Miss Across Early and Late Sessions'\n",
    "y_label = 'Proportion of Misses'\n",
    "plot_stacked_bars_by_phase.plot_outcomes_by_phase_side_by_side(combd_pattern_frequencies, category_order, title, y_label,\n",
    "                                                          category_colors = ['#CC79A7', '#E69F00', '#009E73'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monkey['item'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monkey = tm.combd_pattern_frequencies[tm.combd_pattern_frequencies['monkey'] == 'Bruno']\n",
    "\n",
    "#for event in df_monkey['item'].unique():\n",
    "# for event in ['miss_over_attempt', 'first_shot_capture_over_attempt', 'retry_over_miss', 'retry_fail_over_miss', 'retry_capture_over_miss']:\n",
    "for event in ['GUAT_over_both', 'TAFT_over_both']:\n",
    "# for event in ['stop_success_rate', 'ff_capture_rate']:\n",
    "    print('--------------------------------')\n",
    "    print(f'Event: {event}')\n",
    "    try:\n",
    "        analyze_proportion_trend.show_event_proportion(df_monkey, event)\n",
    "    except:\n",
    "        print(f\"Error for event: {event}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retry window capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just retry window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exists_ok = True\n",
    "\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame()\n",
    "\n",
    "combd_retry_window_captures = pd.DataFrame()\n",
    "po_dict = {}\n",
    "\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    \n",
    "    monkey = monkey_name.split(\"monkey_\")[1]\n",
    "    file_dir = f'all_monkey_data/learning/{monkey_name}'\n",
    "    os.makedirs(file_dir, exist_ok=True)\n",
    "    try:\n",
    "        if not exists_ok:\n",
    "            raise Exception('')\n",
    "        all_retries_df = pd.read_csv(f'{file_dir}/all_retries_df.csv', index_col=False)\n",
    "    except:\n",
    "        all_retries_df = prep_retry_data.get_retries_data_across_sessions(monkey_name=monkey_name)\n",
    "        all_retries_df.to_csv(f'{file_dir}/all_retries_df.csv', index=False)\n",
    "        \n",
    "    retries_summary = prep_retry_data.summarize_retry_data(all_retries_df)\n",
    "    retry_window_captures = all_retries_df[all_retries_df['type'].isin(['GUAT', 'TAFT'])].copy()\n",
    "    retry_window_captures = retry_window_captures[['session', 'capture', 'stop_window']].groupby('session').sum().reset_index(drop=False)\n",
    "    retry_window_captures.rename(columns={'capture': 'captures', 'stop_window': 'total_duration'}, inplace=True)\n",
    "\n",
    "\n",
    "    po = capture_stats_trend.fit_poisson_by_session(retry_window_captures)\n",
    "    po_dict[monkey] = po\n",
    "    \n",
    "    retry_window_captures['monkey'] = monkey\n",
    "    combd_retry_window_captures = pd.concat([combd_retry_window_captures, retry_window_captures])\n",
    "\n",
    "capture_stats_trend.plot_poisson_rate_fit(combd_retry_window_captures, po_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_window_captures = combd_retry_window_captures[combd_retry_window_captures['monkey'] == 'Bruno']\n",
    "\n",
    "(rate_phase_tbl, rate_ttest_tbl, rate_glm_tbl, rate_effect_summary_tbl\n",
    "     ) = capture_stats_by_phase.summarize_early_late_event_rate_with_glm(retry_window_captures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine with overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame()\n",
    "\n",
    "combd_df_sessions = pd.DataFrame()\n",
    "combd_df_trials = pd.DataFrame()\n",
    "combd_retry_window_captures = pd.DataFrame()\n",
    "po_dict = {}\n",
    "po_retry_dict = {}\n",
    "ols_dict = {}\n",
    "\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    \n",
    "    monkey = monkey_name.split(\"monkey_\")[1]\n",
    "    all_trial_durations_df, all_stop_df, all_VBLO_df = prep_learning_data.get_key_learning_data(monkey_name=monkey_name)\n",
    "    try:\n",
    "        if not exists_ok:\n",
    "            raise Exception('')\n",
    "        all_retries_df = pd.read_csv(f'{monkey_name}/all_retries_df.csv', index_col=False)\n",
    "    except:\n",
    "        all_retries_df = prep_retry_data.get_retries_data_across_sessions(monkey_name=monkey_name)\n",
    "        os.makedirs(f'{monkey_name}', exist_ok=True)\n",
    "        all_retries_df.to_csv(f'{monkey_name}/all_retries_df.csv', index=False)\n",
    "        \n",
    "    df_trials, df_sessions = prep_learning_data.process_all_trial_durations_df(all_trial_durations_df)\n",
    "    po = capture_stats_trend.fit_poisson_by_session(df_sessions)\n",
    "    df_sessions['monkey'] = monkey\n",
    "    combd_df_sessions = pd.concat([combd_df_sessions, df_sessions])\n",
    "    po_dict[monkey] = po\n",
    "\n",
    "\n",
    "    retry_window_captures = prep_retry_data.get_retry_window_captures(all_retries_df)\n",
    "    po_retry = capture_stats_trend.fit_poisson_by_session(retry_window_captures)\n",
    "    po_retry_dict[monkey] = po_retry\n",
    "    retry_window_captures['monkey'] = monkey\n",
    "    combd_retry_window_captures = pd.concat([combd_retry_window_captures, retry_window_captures])\n",
    "\n",
    "\n",
    "combd_df_sessions['condition'] = 'captures_per_min_overall'\n",
    "combd_retry_window_captures['condition'] = 'captures_per_min_in_retry_window'\n",
    "combd_captures = pd.concat([combd_df_sessions, combd_retry_window_captures])\n",
    "\n",
    "combd_po = {}\n",
    "for monkey in ['Bruno', 'Schro']:\n",
    "    combd_po[monkey] = {'captures_per_min_overall': po_dict[monkey]}\n",
    "    combd_po[monkey].update({'captures_per_min_in_retry_window': po_retry_dict[monkey]})\n",
    "    \n",
    "capture_stats_trend.plot_poisson_rate_fit(combd_captures, combd_po)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## median duration now (if prior type is XXX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_df['prior_type'] = all_retries_df['type'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def geometric_mean(x, axis=None):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if np.any(x < 0):\n",
    "        raise ValueError(\"Geometric mean is undefined for negative values.\")\n",
    "    with np.errstate(divide='ignore'):\n",
    "        logs = np.log(x)\n",
    "    gm = np.exp(np.mean(logs, axis=axis))\n",
    "    return gm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('geometric mean: ', geometric_mean(all_retries_df['new_duration'].clip(lower=1e-6)))\n",
    "all_retries_df['new_duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_df[all_retries_df['new_duration'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_sub = all_retries_df[all_retries_df['prior_type']=='TAFT'].copy()\n",
    "print('geometric mean: ', geometric_mean(all_retries_sub['new_duration'].clip(lower=1e-6)))\n",
    "all_retries_sub['new_duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_retries_sub = all_retries_df[all_retries_df['prior_type']=='GUAT'].copy()\n",
    "all_retries_sub = all_retries_df[(all_retries_df['prior_type']=='GUAT') & (all_retries_df['type']!='GUAT')].copy()\n",
    "print('geometric mean: ', geometric_mean(all_retries_sub['new_duration'].clip(lower=1e-6)))\n",
    "all_retries_sub['new_duration'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_sub = all_retries_df[all_retries_df['prior_type']=='GUAT'].copy()\n",
    "all_retries_sub['new_duration'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_retries_sub['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration / capture rate (as continuous var & event rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## both monkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame()\n",
    "\n",
    "combd_df_sessions = pd.DataFrame()\n",
    "combd_df_trials = pd.DataFrame()\n",
    "po_dict = {}\n",
    "ols_dict = {}\n",
    "\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    \n",
    "    monkey = monkey_name.split(\"monkey_\")[1]\n",
    "    all_trial_durations_df, all_stop_df, all_VBLO_df = prep_learning_data.get_key_learning_data(monkey_name=monkey_name)\n",
    "    df_trials, df_sessions = prep_learning_data.process_all_trial_durations_df(all_trial_durations_df)\n",
    "    \n",
    "    out = capture_stats_trend.fit_and_plot_capture_rate_and_duration(\n",
    "        df_trials, df_sessions,\n",
    "        session_col=\"session\",\n",
    "        value_col=\"duration_sec\",\n",
    "        count_col=\"captures\",\n",
    "        exposure_col=\"total_duration\",\n",
    "        rate_per=\"minute\",     # or \"second\"\n",
    "        make_plots=False,\n",
    "        title_prefix=f'{monkey}: '\n",
    "    )\n",
    "    po  = out[\"po\"]\n",
    "    ols = out[\"ols\"]\n",
    "    \n",
    "    df_sessions['monkey'] = monkey\n",
    "    df_trials['monkey'] = monkey\n",
    "    combd_df_sessions = pd.concat([combd_df_sessions, df_sessions])\n",
    "    combd_df_trials = pd.concat([combd_df_trials, df_trials])\n",
    "    po_dict[monkey] = po\n",
    "    ols_dict[monkey] = ols\n",
    "\n",
    "    po_df = capture_stats_trend.extract_estimates_from_poisson_fit(po)\n",
    "    ols_df = capture_stats_trend.extract_estimates_from_ols_fit(ols)\n",
    "    \n",
    "    po_df['monkey'] = monkey\n",
    "    ols_df['monkey'] = monkey\n",
    "\n",
    "    # Combine into one table\n",
    "    summary_df = pd.concat([summary_df, po_df, ols_df])\n",
    "\n",
    "capture_stats_trend.plot_poisson_rate_fit(combd_df_sessions, po_dict)\n",
    "capture_stats_trend.plot_duration_fit(combd_df_trials, ols_dict)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = summary_df[summary_df['model'] == 'OLS (log-duration)'].copy()\n",
    "sum_df = sum_df[['monkey'] + [c for c in sum_df.columns if c != 'monkey']]\n",
    "sum_df.drop(columns=['model', 'scale'], inplace=True)\n",
    "sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = summary_df[summary_df['model'] == 'Poisson (captures/min)'].copy()\n",
    "sum_df = sum_df[['monkey'] + [c for c in sum_df.columns if c != 'monkey']]\n",
    "sum_df.drop(columns=['model', 'scale'], inplace=True)\n",
    "sum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early vs late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_stats_by_phase.analyze_early_late_capture_and_duration(df_sessions, df_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one monkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials, df_sessions = prep_learning_data.process_all_trial_durations_df(all_trial_durations_df)\n",
    "\n",
    "# Fit both + make the same plots you were making before\n",
    "out = capture_stats_trend.fit_and_plot_capture_rate_and_duration(\n",
    "    df_trials, df_sessions,\n",
    "    session_col=\"session\",\n",
    "    value_col=\"duration_sec\",\n",
    "    count_col=\"captures\",\n",
    "    exposure_col=\"total_duration\",\n",
    "    rate_per=\"minute\",     # or \"second\"\n",
    "    make_plots=True,\n",
    "    title_prefix=f'{monkey_name.split(\"monkey_\")[1]}: '\n",
    ")\n",
    "po  = out[\"po\"]\n",
    "ols = out[\"ols\"]\n",
    "\n",
    "# # If you just want the models (no plots):\n",
    "# models = fit_both_models(df_trials, df_sessions)\n",
    "# po2, ols2 = models[\"po\"], models[\"ols\"]\n",
    "\n",
    "po_df = capture_stats_trend.extract_estimates_from_poisson_fit(po)\n",
    "ols_df = capture_stats_trend.extract_estimates_from_ols_fit(ols)\n",
    "\n",
    "# Combine into one table\n",
    "summary_df = pd.concat([po_df, ols_df])\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretty print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rate_phase, rate_ttest, rate_glm, rate_effect) = capture_stats_by_phase.summarize_early_late_event_rate_with_glm(df_sessions)\n",
    "(dur_phase,  dur_ttest,  dur_glm,  dur_effect ) = capture_stats_by_phase.summarize_early_late_duration_with_glm(df_trials, df_sessions)\n",
    "print_fit_results.show_all_pretty_tables(rate_phase, rate_ttest, rate_glm, rate_effect,\n",
    "                       dur_phase,  dur_ttest,  dur_glm,  dur_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose duration cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def outlier_cutoff(durations, method='logmad', k=3.5, iqr_k=3.0, q=0.995):\n",
    "    \"\"\"\n",
    "    One-sided high cutoff for durations. Returns (cutoff, mask).\n",
    "    - Zeros are never 'too large' and are included in the mask.\n",
    "    - Negatives raise (shouldn't exist for durations).\n",
    "    \"\"\"\n",
    "    x = np.asarray(durations, float)\n",
    "    if np.any(x < 0):\n",
    "        raise ValueError(\"Durations must be >= 0.\")\n",
    "\n",
    "    pos = x[x > 0]  # ignore zeros for cutoff calc\n",
    "    if pos.size == 0:\n",
    "        return np.inf, np.ones_like(x, dtype=bool)\n",
    "\n",
    "    if method == 'logmad':\n",
    "        g = np.log(pos)\n",
    "        med = np.median(g)\n",
    "        mad = np.median(np.abs(g - med))\n",
    "        mad_normal = 1.4826 * mad\n",
    "        cutoff = np.exp(med + k * mad_normal)\n",
    "    elif method == 'iqr':\n",
    "        q1, q3 = np.percentile(pos, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        cutoff = q3 + iqr_k * iqr\n",
    "    elif method == 'quantile':\n",
    "        cutoff = np.quantile(pos, q)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'logmad', 'iqr', or 'quantile'.\")\n",
    "\n",
    "    mask = x <= cutoff\n",
    "    return float(cutoff), mask\n",
    "\n",
    "for method in ['logmad', 'iqr', 'quantile']:\n",
    "    cutoff, mask = general_utils.outlier_cutoff(all_trial_durations_df['duration_sec'], method=method)\n",
    "    print(f'{method}: {cutoff}')\n",
    "    \n",
    "cutoff, mask = general_utils.outlier_cutoff(all_trial_durations_df['duration_sec'], method='quantile', q=0.99)\n",
    "print(f'quantile 0.99: {cutoff}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VBLO (as proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(analyze_proportion_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_col = \"VBLO_trials\"\n",
    "denom_count_col = \"base_trials\"\n",
    "ylabel='Percentage'\n",
    "\n",
    "ratio_po_dict = {}\n",
    "combd_df_VBLO = pd.DataFrame()\n",
    "\n",
    "for monkey_name in ['monkey_Schro', 'monkey_Bruno']:\n",
    "    monkey = monkey_name.split(\"monkey_\")[1]\n",
    "    all_trial_durations_df, all_stop_df, all_VBLO_df = prep_learning_data.get_key_learning_data(monkey_name=monkey_name)\n",
    "    glm_pois = smf.glm(\n",
    "        f\"{event_count_col} ~ session\",\n",
    "        data=all_VBLO_df,\n",
    "        family=sm.families.Poisson(),\n",
    "        offset=np.log(all_VBLO_df[denom_count_col].clip(\n",
    "            lower=1))  # keep your offset guard\n",
    "    ).fit(cov_type=\"HC0\")\n",
    "    \n",
    "    \n",
    "    all_VBLO_df['monkey'] = monkey\n",
    "    combd_df_VBLO = pd.concat([combd_df_VBLO, all_VBLO_df])\n",
    "    ratio_po_dict[monkey] = glm_pois\n",
    "\n",
    "title = f'Fraction of Targets Visible Before Last Capture'\n",
    "analyze_proportion_trend.plot_poisson_proportion_fit(\n",
    "        combd_df_VBLO, ratio_po_dict,\n",
    "        session_col=\"session\", event_count_col=event_count_col, denom_count_col=denom_count_col,\n",
    "        title=title, ylabel=ylabel,\n",
    "        pval=None\n",
    "    )\n",
    "    \n",
    "# analyze_proportion_trend.evaluate_proportion_trend(all_VBLO_df, event_count_col=event_count_col, denom_count_col=denom_count_col,\n",
    "#                                         title=title, ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_col = \"VBLO_trials\"\n",
    "denom_count_col = \"base_trials\"\n",
    "ylabel='percentage'\n",
    "\n",
    "for monkey_name in ['monkey_Schro', 'monkey_Bruno']:\n",
    "    all_trial_durations_df, all_stop_df, all_VBLO_df = prep_learning_data.get_key_learning_data(monkey_name=monkey_name)\n",
    "    title = f'{monkey_name.split(\"monkey_\")[1]}: Fraction of Targets Visible Before Previous Capture'\n",
    "    analyze_proportion_trend.evaluate_proportion_trend(all_VBLO_df, event_count_col=event_count_col, denom_count_col=denom_count_col,\n",
    "                                            title=title, ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_col = \"VBLO_trials\"\n",
    "denom_count_col = \"base_trials\"\n",
    "title = f'{monkey_name.split(\"monkey_\")[1]}: Percentage of Visible-Before-Last-Target Trials'\n",
    "ylabel=\"Ratio\"\n",
    "\n",
    "analyze_proportion_trend.evaluate_proportion_trend(all_VBLO_df, event_count_col=event_count_col, denom_count_col=denom_count_col, title=title, ylabel=ylabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captures over stops (as proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_col = \"captures\"\n",
    "denom_count_col = \"stops\"\n",
    "\n",
    "sns.set_style(style=\"darkgrid\")\n",
    "for monkey_name in ['monkey_Schro', 'monkey_Bruno']:\n",
    "    all_trial_durations_df, all_stop_df, all_VBLO_df = prep_learning_data.get_key_learning_data(monkey_name=monkey_name)\n",
    "    analyze_proportion_trend.evaluate_proportion_trend(all_stop_df, event_count_col=event_count_col, denom_count_col=denom_count_col,\n",
    "                                            title=f'{monkey_name.split(\"monkey_\")[1]}: Ratio of Captures vs Stops')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually get capture vs miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captures_df = df_monkey.loc[df_monkey['item'] == 'ff_capture_rate', ['session', 'frequency', 'monkey']].rename(columns={'frequency': 'n_capture'})\n",
    "misses_df = df_monkey.loc[df_monkey['item'] == 'retry_capture_over_miss', ['session', 'denom_count', 'monkey']].rename(columns={'denom_count': 'n_miss'})\n",
    "\n",
    "outcomes_df = pd.merge(captures_df, misses_df, on=['session', 'monkey'], how='outer')\n",
    "outcomes_df['denom_count'] = outcomes_df['n_capture'] + outcomes_df['n_miss']\n",
    "outcomes_df['capture'] = outcomes_df['n_capture'] / outcomes_df['denom_count']\n",
    "outcomes_df['miss'] = outcomes_df['n_miss'] / outcomes_df['denom_count']\n",
    "outcomes_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = outcomes_df.melt(\n",
    "    id_vars=['session', 'monkey' , 'denom_count'],         # keep session as-is\n",
    "    value_vars=['n_capture','n_miss'],  # columns to unpivot\n",
    "    var_name='label',        # new column name\n",
    "    value_name='frequency'         # new values column name\n",
    ")\n",
    "\n",
    "long['ratio'] = long['frequency'] / long['denom_count']\n",
    "\n",
    "fig = px.bar(\n",
    "    long,\n",
    "    x='session',\n",
    "    y='ratio',\n",
    "    color='label',\n",
    "    title='Outcome (stacked)',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses = analyze_proportion_trend.tertile_phase(long)\n",
    "phase_df = ses.groupby(['phase', 'label', 'monkey'], observed=True)[['frequency', 'denom_count']].sum().reset_index(drop=False)\n",
    "phase_df['ratio'] = phase_df['frequency'] / phase_df['denom_count']\n",
    "phase_df\n",
    "\n",
    "fig = px.bar(\n",
    "    phase_df,\n",
    "    x='phase',\n",
    "    y='ratio',\n",
    "    color='label',\n",
    "    title='Attempts by session (stacked)',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly for stacked bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = ['retry_fail_over_miss', 'retry_capture_over_miss', 'no_retry_over_miss', 'first_shot_capture_over_attempt']\n",
    "# #categories = ['miss_over_attempt', 'first_shot_capture_over_attempt']\n",
    "categories = ['retry_fail_over_attempt', 'retry_capture_over_attempt', 'no_retry_over_attempt', 'first_shot_capture_over_attempt']\n",
    "\n",
    "new_label_mapping = {\n",
    "    # Break down attempts\n",
    "    'first_shot_capture_over_attempt': 'Capture',\n",
    "    'no_retry_over_attempt': 'No Retry',\n",
    "    'retry_capture_over_attempt': 'Retry + Capture',\n",
    "    'retry_fail_over_attempt': 'Retry + Fail',\n",
    "    # Capture vs miss\n",
    "    'capture_over_miss': 'Capture',\n",
    "    'miss_over_attempt': 'Miss',\n",
    "    # Break down misses\n",
    "    'retry_capture_over_miss': 'Retry + Capture over Miss',\n",
    "    'retry_fail_over_miss': 'Retry + Fail over Miss',\n",
    "    'no_retry_over_miss': 'No Retry over Miss',\n",
    "    \n",
    "}\n",
    "\n",
    "# category_order = ['Capture', 'Retry + Capture', 'Retry + Fail', 'No Retry']\n",
    "category_order = ['Capture', 'Miss']\n",
    "category_order = ['Retry + Capture over Miss', 'Retry + Fail over Miss', 'No Retry over Miss']\n",
    "\n",
    "\n",
    "for monkey in ['Bruno', 'Schro']:\n",
    "    df_monkey = tm.combd_pattern_frequencies[tm.combd_pattern_frequencies['monkey'] == monkey].copy()\n",
    "    df_monkey['new_label'] = df_monkey['item'].map(new_label_mapping).fillna('')\n",
    "    df_monkey_sub = df_monkey[df_monkey['new_label'].isin(category_order)].copy()\n",
    "    \n",
    "\n",
    "    ses = analyze_proportion_trend.tertile_phase(df_monkey_sub)\n",
    "    phase_df = ses.groupby(['phase', 'new_label', 'item'], observed=True)[['frequency', 'denom_count']].sum().reset_index(drop=False)\n",
    "    phase_df['denom_count'] = phase_df['denom_count'].astype(int)\n",
    "    phase_df['ratio'] = phase_df['frequency'] / phase_df['denom_count']\n",
    "\n",
    "    p_values = {}\n",
    "    for new_label in category_order: \n",
    "        phase_df_sub = phase_df[phase_df['new_label']==new_label]\n",
    "        pval_el, test_name = analyze_proportion_trend.test_early_late(phase_df_sub, new_label)\n",
    "        print(f'{new_label}: {round(pval_el, 5)}')\n",
    "        p_values[new_label] = pval_el\n",
    "\n",
    "\n",
    "    phase_df_sub = phase_df[phase_df['phase'].isin(['early', 'late'])].copy()\n",
    "    \n",
    "\n",
    "    fig = px.bar(\n",
    "        phase_df_sub,\n",
    "        x='phase',\n",
    "        y='ratio',\n",
    "        color='new_label',\n",
    "        title=f'{monkey}: Attempt Outcomes Across Early and Late Sessions',\n",
    "        category_orders={\n",
    "            'phase': ['early', 'late'],\n",
    "            'new_label': ['Capture', 'Retry + Capture', 'Retry + Fail', 'No Retry']\n",
    "        },\n",
    "        # >>> INSERT FIXES HERE (exact code above) <<<\n",
    "        color_discrete_map={\n",
    "            'Capture': px.colors.qualitative.Plotly[0],\n",
    "            'Retry + Capture': px.colors.qualitative.Plotly[1],\n",
    "            'Retry + Fail': px.colors.qualitative.Plotly[2],\n",
    "            'No Retry': px.colors.qualitative.Plotly[3],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=620,\n",
    "        height=520,\n",
    "        bargap=0.18,\n",
    "        bargroupgap=0.05,\n",
    "        barmode='relative',          # stacked\n",
    "        legend_traceorder='reversed',# legend top matches stack top; use 'normal' to flip\n",
    "        legend_title_text='',\n",
    "        legend=dict(x=1.02, y=0.5, yanchor='middle'),  # keep to the right, centered\n",
    "        yaxis=dict(title='Proportion of Attempts',tickformat='.0%'),\n",
    "        xaxis_title='Session Phase',\n",
    "        margin=dict(l=90, r=140, t=70, b=60),\n",
    "    )\n",
    "\n",
    "    # Thin white borders between stack segments → clearer separation\n",
    "    fig.update_traces(marker_line_width=0.6, marker_line_color='white')\n",
    "\n",
    "    # Cleaner hover text\n",
    "    fig.update_traces(\n",
    "        hovertemplate=(\n",
    "            'phase=%{x}<br>'\n",
    "            '%{fullData.name}: %{y:.1%}<extra></extra>'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(rangemode='tozero', automargin=True)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly get all_trial_durations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Bruno')\n",
    "\n",
    "all_trial_durations_df = pd.DataFrame()\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    if row['finished'] is True:\n",
    "        continue\n",
    "\n",
    "    data_name = row['data_name']\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], data_name)\n",
    "    print(raw_data_folder_path)\n",
    "    data_item = further_processing_class.FurtherProcessing(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "    # data_item.retrieve_or_make_monkey_data()\n",
    "    \n",
    "    data_item.ff_caught_T_sorted, data_item.ff_index_sorted, data_item.ff_real_position_sorted, data_item.ff_believed_position_sorted, data_item.ff_life_sorted, \\\n",
    "        data_item.ff_flash_end_sorted = retrieve_raw_data.make_or_retrieve_ff_info_from_txt_data(\n",
    "            data_item.raw_data_folder_path)\n",
    "    # data_item.make_or_retrieve_closest_stop_to_capture_df()\n",
    "    # data_item.make_ff_caught_T_new()\n",
    "    \n",
    "    trial_durations = np.diff(data_item.ff_caught_T_sorted)\n",
    "    trial_durations_df = pd.DataFrame(\n",
    "        {'duration_sec': trial_durations, 'trial_index': np.arange(len(trial_durations))})\n",
    "    trial_durations_df['data_name'] = data_name\n",
    "    all_trial_durations_df = pd.concat(\n",
    "        [all_trial_durations_df, trial_durations_df])\n",
    "    \n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df)\n",
    "\n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df, 'session')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonparametric trend checks (and others)\n",
    "(a) Spearman: session vs geometric‑mean duration per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# geometric mean per session = exp(mean(log T))\n",
    "g = (\n",
    "    df_trials\n",
    "    .assign(logT=np.log(df_trials[\"duration_sec\"]))\n",
    "    .groupby(\"session\")[\"logT\"]\n",
    "    .mean()\n",
    "    .pipe(np.exp)\n",
    "    .rename(\"geom_mean_T\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rho, pval = spearmanr(g[\"session\"].values, g[\"geom_mean_T\"].values)\n",
    "print(f\"Spearman rho = {rho:.3f}, p = {pval:.3g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theil–Sen slope on log T (trial level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "X = df_trials[[\"session\"]].astype(float).values\n",
    "y = np.log(df_trials[\"duration_sec\"].values)\n",
    "\n",
    "ts = TheilSenRegressor(random_state=0)\n",
    "ts.fit(X, y)\n",
    "\n",
    "slope = ts.coef_[0]                    # per session (on log-scale)\n",
    "per10_pct = (np.exp(10*slope) - 1)*100 # % change per 10 sessions\n",
    "print(f\"Theil–Sen slope (logT vs session) = {slope:.5f} per session \"\n",
    "      f\"⇒ {per10_pct:+.1f}% per 10 sessions\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
