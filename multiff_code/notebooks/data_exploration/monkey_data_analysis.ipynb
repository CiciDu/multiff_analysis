{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "D7ubA2G0ql0K"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29094,
     "status": "ok",
     "timestamp": 1683238612245,
     "user": {
      "displayName": "Cici Du",
      "userId": "17701548280142155870"
     },
     "user_tz": 300
    },
    "id": "3klBkPC-Q4rw",
    "outputId": "9b955f29-f614-4ccb-c49f-3b041dbcb6a9"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "    \n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, base_processing_class, combine_info_utils, further_processing_class\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class, cluster_analysis, patterns_and_features_class, compare_two_monkeys_class, monkey_landing_in_ff\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, process_GUAT_trials_class, GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data_utils\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics, monkey_heading_utils\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from planning_analysis.test_params_for_planning import params_test_combos_class, params_utils\n",
    "from visualization.plotly_tools import plotly_for_monkey, plotly_for_time_series, plotly_preparation, plotly_for_correlation\n",
    "from visualization.dash_tools import dash_prep_class, dash_utils, dash_utils, dash_comparison_class, dash_params_class\n",
    "from visualization.dash_tools.dash_main_class_methods import dash_main_class\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils\n",
    "from planning_analysis.only_cur_ff import only_cur_ff_utils, only_cur_ff_class, only_cur_ff_utils\n",
    "from planning_analysis.plan_factors import plan_factors_utils, build_factor_comp, plan_factors_class, monkey_plan_factors_x_sess_class\n",
    "from planning_analysis.agent_analysis import compare_monkey_and_agent_utils, agent_plan_factors_class, agent_plan_factors_x_sess_class\n",
    "from planning_analysis.plan_factors import test_vs_control_utils\n",
    "from planning_analysis.factors_vs_indicators import make_variations_utils, plot_variations_utils, process_variations_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_class, show_planning_utils, examine_null_arcs\n",
    "from planning_analysis.show_planning.cur_vs_nxt_ff import cvn_helper_class, find_cvn_utils, plot_cvn_class, plot_cvn_utils, plot_monkey_heading_helper_class, cvn_from_ref_class\n",
    "from machine_learning.ml_methods import ml_methods_class, prep_ml_data_utils\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_utils\n",
    "from pattern_discovery.learning import show_learning\n",
    "import numpy as np, pandas as pd, statsmodels.api as sm, statsmodels.formula.api as smf\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from importlib import reload\n",
    "from eye_position_analysis import eye_positions\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import cm\n",
    "from os.path import exists\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "from scipy.stats import rankdata\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, multilabel_confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import pi\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import plotly.graph_objects as go\n",
    "import gc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7CKlu77jK9mL"
   },
   "source": [
    "# Individual monkey patterns and statistics data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KjctDk7r_lK0"
   },
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patterns and features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruno = patterns_and_features_class.PatternsAndFeatures(monkey_name='monkey_Bruno')\n",
    "bruno.combine_or_retrieve_patterns_and_features(verbose=False, exists_ok=True)\n",
    "schro = patterns_and_features_class.PatternsAndFeatures(monkey_name='monkey_Schro')\n",
    "schro.combine_or_retrieve_patterns_and_features(verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IiHVwOfM72GK"
   },
   "source": [
    "## barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruno.plot_pattern_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schro.plot_pattern_frequencies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vcI3yQ6YFp49"
   },
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruno.plot_feature_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schro.plot_feature_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boxplots (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = bruno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['t', 't_last_vis', 'd_last_vis','abs_angle_last_vis', 'num_stops', 'num_stops_near_target']\n",
    "titles_of_columns = {'t': 'Trial Duration',\n",
    "                     't_last_vis': 'Trial Duration Since Target Last Visible',\n",
    "                     'd_last_vis':'Distance of Target Since Target Last Visible',\n",
    "                     'abs_angle_last_vis': 'Abs Angle of Target Boundary Since Target Last Visible',\n",
    "                     'num_stops': 'Number of Stops during a Trial',\n",
    "                     'num_stops_near_target': 'Number of Stops near a Target',\n",
    "}\n",
    "x_lables_of_columns = {'t': 'Duration in s',\n",
    "                      't_last_vis': 'Duration in s',\n",
    "                      'd_last_vis':'cum_distance',\n",
    "                      'abs_angle_last_vis': 'Angle in radians',\n",
    "                      'num_stops': 'Number of Stops',\n",
    "                      'num_stops_near_target': 'Number of Stops'}\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    feature_data = monkey.combd_all_trial_features[column]\n",
    "    feature_data = feature_data[feature_data!=9999]\n",
    "    title = titles_of_columns[column]\n",
    "    x_label = x_lables_of_columns[column]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.boxplot(x=feature_data)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjlhxi4WQ6bD"
   },
   "source": [
    "## changes over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruno.plot_the_changes_in_pattern_frequencies_over_time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3jTzAF8TF9Mx"
   },
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruno.plot_the_changes_in_feature_statistics_over_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scatter \n",
    "scatter around target center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey.combd_scatter_around_target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for monkey in [bruno, schro]:\n",
    "    for y_column_list in [['distance_mean', 'distance_50%'],\n",
    "                          ['abs_angle_mean', 'abs_angle_50%'],\n",
    "                          #['distance_mean', 'distance_Q1', 'distance_median', 'distance_Q3']\n",
    "                          ]:\n",
    "        monkey.plot_the_changes_in_scatter_around_target_over_time(y_columns=y_column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/68a7b6a5-04e0-8332-aa89-8c4235fe4c4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name='all_monkey_data/raw_monkey_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import specific_utils, process_monkey_information, retrieve_raw_data, time_calib_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Schro')\n",
    "\n",
    "all_trial_durations_df = pd.DataFrame()\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    if row['finished'] is True:\n",
    "        continue\n",
    "\n",
    "    data_name = row['data_name']\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], data_name)\n",
    "    print(raw_data_folder_path)\n",
    "    data_item = further_processing_class.FurtherProcessing(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "    # data_item.retrieve_or_make_monkey_data()\n",
    "    \n",
    "    data_item.ff_caught_T_sorted, data_item.ff_index_sorted, data_item.ff_real_position_sorted, data_item.ff_believed_position_sorted, data_item.ff_life_sorted, \\\n",
    "        data_item.ff_flash_end_sorted = retrieve_raw_data.make_or_retrieve_ff_info_from_txt_data(\n",
    "            data_item.raw_data_folder_path)\n",
    "    # data_item.make_or_retrieve_closest_stop_to_capture_df()\n",
    "    # data_item.make_ff_caught_T_new()\n",
    "    \n",
    "    trial_durations = np.diff(data_item.ff_caught_T_sorted)\n",
    "    trial_durations_df = pd.DataFrame(\n",
    "        {'duration_sec': trial_durations, 'trial_index': np.arange(len(trial_durations))})\n",
    "    trial_durations_df['data_name'] = data_name\n",
    "    all_trial_durations_df = pd.concat(\n",
    "        [all_trial_durations_df, trial_durations_df])\n",
    "    \n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some more things to try here:\n",
    "https://chatgpt.com/g/g-p-68a34392716c8191839b81db389836c7-multiff/c/68a7b6a5-04e0-8332-aa89-8c4235fe4c4b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, statsmodels.api as sm, statsmodels.formula.api as smf\n",
    "\n",
    "# 1) Filter and clean durations FIRST\n",
    "df_trials = all_trial_durations_df.query(\"duration_sec < 30\").copy()\n",
    "\n",
    "df_trials[\"duration_sec\"] = df_trials[\"duration_sec\"].clip(lower=1e-6)\n",
    "\n",
    "# Standardize names\n",
    "df_trials = df_trials.rename(columns={\"session_id\": \"session\"})\n",
    "\n",
    "# 2) Build session-level aggregates from the cleaned trials\n",
    "df_sessions = (\n",
    "    df_trials.groupby(\"session\", as_index=False)\n",
    "    .agg(captures=(\"duration_sec\", \"size\"),\n",
    "         total_duration=(\"duration_sec\", \"sum\"))\n",
    ")\n",
    "\n",
    "# If you used drop (A), total_duration should be >0. Still, be safe:\n",
    "df_sessions[\"total_duration\"] = df_sessions[\"total_duration\"].clip(lower=1e-12)\n",
    "\n",
    "# 3) Logs (after cleaning)\n",
    "df_trials[\"logT\"] = np.log(df_trials[\"duration_sec\"])\n",
    "offset = np.log(df_sessions[\"total_duration\"])\n",
    "\n",
    "# 4) Models\n",
    "po = smf.glm(\n",
    "    \"captures ~ session\",\n",
    "    data=df_sessions,\n",
    "    family=sm.families.Poisson(),\n",
    "    offset=offset\n",
    ").fit(cov_type=\"HC0\")\n",
    "RR10 = float(np.exp(10 * po.params[\"session\"]))\n",
    "\n",
    "ols = smf.ols(\"logT ~ session\", data=df_trials).fit(\n",
    "    cov_type=\"cluster\", cov_kwds={\"groups\": df_trials[\"session\"]}\n",
    ")\n",
    "percent_change_10 = float((np.exp(10 * ols.params[\"session\"]) - 1) * 100)\n",
    "\n",
    "print({\n",
    "    \"rate_ratio_per_10_sessions\": RR10,\n",
    "    \"percent_change_duration_per_10_sessions\": percent_change_10\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessions[\"total_duration\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Poisson (captures per minute)\n",
    "po_ci = po.conf_int().loc[\"session\"]\n",
    "po_res = {\n",
    "    \"estimate\": float(po.params[\"session\"]),\n",
    "    \"rate_ratio_per_10_sessions\": float(np.exp(10 * po.params[\"session\"])),\n",
    "    \"95% CI for rate_ratio\": tuple(np.exp(10 * po_ci)),\n",
    "    \"p_value\": float(po.pvalues[\"session\"]),\n",
    "}\n",
    "\n",
    "# (B) OLS (log-duration per trial)\n",
    "ols_ci = ols.conf_int().loc[\"session\"]\n",
    "ols_res = {\n",
    "    \"estimate\": float(ols.params[\"session\"]),\n",
    "    \"percent_change_duration_per_10_sessions\":\n",
    "        float((np.exp(10 * ols.params[\"session\"]) - 1) * 100),\n",
    "    \"95% CI for percent_change\": tuple((np.exp(10 * ols_ci) - 1) * 100),\n",
    "    \"p_value\": float(ols.pvalues[\"session\"]),\n",
    "}\n",
    "\n",
    "print(\"Poisson (captures/min):\", po_res)\n",
    "print(\"OLS (log-duration):\", ols_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Poisson ---\n",
    "po_ci = po.conf_int().loc[\"session\"]\n",
    "po_dict = {\n",
    "    \"model\": \"Poisson (captures/min)\",\n",
    "    \"estimate_per_session\": float(po.params[\"session\"]),\n",
    "    \"effect_per_10_sessions\": float(np.exp(10 * po.params[\"session\"])),\n",
    "    \"95%_CI\": (np.exp(10 * po_ci[0]), np.exp(10 * po_ci[1])),\n",
    "    \"p_value\": float(po.pvalues[\"session\"]),\n",
    "    \"scale\": \"rate ratio\"\n",
    "}\n",
    "\n",
    "# --- OLS ---\n",
    "ols_ci = ols.conf_int().loc[\"session\"]\n",
    "ols_dict = {\n",
    "    \"model\": \"OLS (log-duration)\",\n",
    "    \"estimate_per_session\": float(ols.params[\"session\"]),\n",
    "    \"effect_per_10_sessions\": float((np.exp(10 * ols.params[\"session\"]) - 1) * 100),\n",
    "    \"95%_CI\": (\n",
    "        (np.exp(10 * ols_ci[0]) - 1) * 100,\n",
    "        (np.exp(10 * ols_ci[1]) - 1) * 100\n",
    "    ),\n",
    "    \"p_value\": float(ols.pvalues[\"session\"]),\n",
    "    \"scale\": \"% change\"\n",
    "}\n",
    "\n",
    "# Combine into one table\n",
    "summary_df = pd.DataFrame([po_dict, ols_dict])\n",
    "\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early vs late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_learning.plot_poisson_rate_fit(df_sessions, po)\n",
    "show_learning.plot_duration_fit(df_trials, ols)\n",
    "show_learning.plot_early_late_contrasts(df_sessions, df_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early/late tertiles\n",
    "sessions = np.sort(df_sessions[\"session\"].unique())\n",
    "n = len(sessions)\n",
    "early_cut = sessions[int(np.floor(n/3)) - 1]\n",
    "late_cut  = sessions[int(np.ceil(2*n/3)) - 1]\n",
    "\n",
    "df_sessions[\"phase\"] = np.where(df_sessions[\"session\"] <= early_cut, \"early\",\n",
    "                         np.where(df_sessions[\"session\"] >= late_cut, \"late\", \"mid\"))\n",
    "\n",
    "sub = df_sessions[df_sessions[\"phase\"].isin([\"early\",\"late\"])].copy()\n",
    "\n",
    "glm_phase = smf.glm(\"captures ~ C(phase)\", data=sub,\n",
    "                    family=sm.families.Poisson(),\n",
    "                    offset=np.log(sub[\"total_duration\"])).fit(cov_type=\"HC0\")\n",
    "\n",
    "coef = glm_phase.params[\"C(phase)[T.late]\"]\n",
    "RR = np.exp(coef)\n",
    "ci_low, ci_high = np.exp(glm_phase.conf_int().loc[\"C(phase)[T.late]\"])\n",
    "pval = glm_phase.pvalues[\"C(phase)[T.late]\"]\n",
    "\n",
    "print(f\"Captures/time, late vs early = RR {RR:.2f}, \"\n",
    "      f\"95% CI [{ci_low:.2f}, {ci_high:.2f}], p={pval:.3g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials[\"phase\"] = np.where(df_trials[\"session\"] <= early_cut, \"early\",\n",
    "                       np.where(df_trials[\"session\"] >= late_cut, \"late\", \"mid\"))\n",
    "sub2 = df_trials[df_trials[\"phase\"].isin([\"early\",\"late\"])].copy()\n",
    "\n",
    "ols_phase = smf.ols(\"logT ~ C(phase)\", data=sub2).fit(\n",
    "    cov_type=\"cluster\", cov_kwds={\"groups\": sub2[\"session\"]}\n",
    ")\n",
    "\n",
    "coef = ols_phase.params[\"C(phase)[T.late]\"]\n",
    "pct = (np.exp(coef) - 1) * 100\n",
    "ci_low, ci_high = ols_phase.conf_int().loc[\"C(phase)[T.late]\"]\n",
    "ci_pct = (np.exp(ci_low)-1)*100, (np.exp(ci_high)-1)*100\n",
    "pval = ols_phase.pvalues[\"C(phase)[T.late]\"]\n",
    "\n",
    "print(f\"Duration, late vs early = {pct:+.1f}% \"\n",
    "      f\"(95% CI {ci_pct[0]:+.1f}% to {ci_pct[1]:+.1f}%), p={pval:.3g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonparametric trend checks (and others)\n",
    "(a) Spearman: session vs geometric‑mean duration per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# geometric mean per session = exp(mean(log T))\n",
    "g = (\n",
    "    df_trials\n",
    "    .assign(logT=np.log(df_trials[\"duration_sec\"]))\n",
    "    .groupby(\"session\")[\"logT\"]\n",
    "    .mean()\n",
    "    .pipe(np.exp)\n",
    "    .rename(\"geom_mean_T\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rho, pval = spearmanr(g[\"session\"].values, g[\"geom_mean_T\"].values)\n",
    "print(f\"Spearman rho = {rho:.3f}, p = {pval:.3g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theil–Sen slope on log T (trial level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "X = df_trials[[\"session\"]].astype(float).values\n",
    "y = np.log(df_trials[\"duration_sec\"].values)\n",
    "\n",
    "ts = TheilSenRegressor(random_state=0)\n",
    "ts.fit(X, y)\n",
    "\n",
    "slope = ts.coef_[0]                    # per session (on log-scale)\n",
    "per10_pct = (np.exp(10*slope) - 1)*100 # % change per 10 sessions\n",
    "print(f\"Theil–Sen slope (logT vs session) = {slope:.5f} per session \"\n",
    "      f\"⇒ {per10_pct:+.1f}% per 10 sessions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plateau modeling with splines (joint Wald test & early‑vs‑late contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import bs   # not strictly needed, but good to import for clarity\n",
    "\n",
    "# df_trials must have: session (int), duration_sec (float)\n",
    "df_trials = df_trials.copy()\n",
    "df_trials[\"logT\"] = np.log(df_trials[\"duration_sec\"])\n",
    "\n",
    "# Fit spline directly in the formula; df=4 is a good default (cubic bspline)\n",
    "# Cluster-robust SEs by session to respect repeated trials within session\n",
    "ols_spline = smf.ols(\"logT ~ bs(session, df=4, degree=3)\", data=df_trials).fit(\n",
    "    cov_type=\"cluster\", cov_kwds={\"groups\": df_trials[\"session\"]}\n",
    ")\n",
    "print(ols_spline.summary())\n",
    "\n",
    "# Joint Wald test: all spline terms = 0 (i.e., no session effect at all)\n",
    "# Patsy names the columns like bs(session, df=4, degree=3)[0], [1], ...\n",
    "spline_terms = [c for c in ols_spline.params.index if c.startswith(\"bs(session\")]\n",
    "hypothesis = \" = 0, \".join(spline_terms) + \" = 0\"\n",
    "print(\"\\nJoint Wald F-test for spline terms:\")\n",
    "print(ols_spline.f_test(hypothesis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combine_patterns_and_features(self, exists_ok=True, save_data=True):\n",
    "    self.sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "        self.raw_data_dir_name, self.monkey_name)\n",
    "\n",
    "    self.combd_pattern_frequencies = pd.DataFrame()\n",
    "    self.combd_feature_statistics = pd.DataFrame()\n",
    "    self.combd_all_trial_features = pd.DataFrame()\n",
    "    self.combd_scatter_around_target_df = pd.DataFrame()\n",
    "\n",
    "    for index, row in self.sessions_df_for_one_monkey.iterrows():\n",
    "        if row['finished'] is True:\n",
    "            continue\n",
    "\n",
    "        data_name = row['data_name']\n",
    "        raw_data_folder_path = os.path.join(\n",
    "            self.raw_data_dir_name, row['monkey_name'], data_name)\n",
    "        print(raw_data_folder_path)\n",
    "        self.data_item = further_processing_class.FurtherProcessing(\n",
    "            raw_data_folder_path=raw_data_folder_path)\n",
    "        self.data_item.make_df_related_to_patterns_and_features(\n",
    "            exists_ok=exists_ok)\n",
    "        print('Successfully made df related to patterns and features for ', data_name)\n",
    "\n",
    "        self.data_item.pattern_frequencies['data_name'] = data_name\n",
    "        self.data_item.feature_statistics['data_name'] = data_name\n",
    "        self.data_item.all_trial_features['data_name'] = data_name\n",
    "        self.data_item.scatter_around_target_df['data_name'] = data_name\n",
    "\n",
    "        self.combd_pattern_frequencies = pd.concat(\n",
    "            [self.combd_pattern_frequencies, self.data_item.pattern_frequencies], axis=0).reset_index(drop=True)\n",
    "        self.combd_feature_statistics = pd.concat(\n",
    "            [self.combd_feature_statistics, self.data_item.feature_statistics], axis=0).reset_index(drop=True)\n",
    "        self.combd_all_trial_features = pd.concat(\n",
    "            [self.combd_all_trial_features, self.data_item.all_trial_features], axis=0).reset_index(drop=True)\n",
    "        self.combd_scatter_around_target_df = pd.concat(\n",
    "            [self.combd_scatter_around_target_df, self.data_item.scatter_around_target_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    organize_patterns_and_features.add_dates_and_sessions(\n",
    "        self.combd_pattern_frequencies)\n",
    "    organize_patterns_and_features.add_dates_and_sessions(\n",
    "        self.combd_feature_statistics)\n",
    "    organize_patterns_and_features.add_dates_and_sessions(\n",
    "        self.combd_all_trial_features)\n",
    "    organize_patterns_and_features.add_dates_and_sessions(\n",
    "        self.combd_scatter_around_target_df)\n",
    "\n",
    "    self.agg_pattern_frequencies = self._make_agg_pattern_frequency()\n",
    "    self.agg_feature_statistics = organize_patterns_and_features.make_feature_statistics(self.combd_all_trial_features.drop(\n",
    "        columns=['data_name', 'data', 'Date']), data_folder_name=None)\n",
    "\n",
    "    if save_data:\n",
    "        os.makedirs(\n",
    "            self.combd_patterns_and_features_folder_path, exist_ok=True)\n",
    "        self.combd_pattern_frequencies.to_csv(os.path.join(\n",
    "            self.combd_patterns_and_features_folder_path, 'combd_pattern_frequencies.csv'))\n",
    "        self.combd_feature_statistics.to_csv(os.path.join(\n",
    "            self.combd_patterns_and_features_folder_path, 'combd_feature_statistics.csv'))\n",
    "        self.combd_all_trial_features.to_csv(os.path.join(\n",
    "            self.combd_patterns_and_features_folder_path, 'combd_all_trial_features.csv'))\n",
    "        self.agg_pattern_frequencies.to_csv(os.path.join(\n",
    "            self.combd_patterns_and_features_folder_path, 'agg_pattern_frequencies.csv'))\n",
    "        self.agg_feature_statistics.to_csv(os.path.join(\n",
    "            self.combd_patterns_and_features_folder_path, 'agg_feature_statistics.csv'))\n",
    "        self.combd_scatter_around_target_df.to_csv(os.path.join(\n",
    "            self.combd_patterns_and_features_folder_path, 'combd_scatter_around_target_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success over stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Schro')\n",
    "\n",
    "all_trial_durations_df = pd.DataFrame()\n",
    "all_stop_df = pd.DataFrame()\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    if row['finished'] is True:\n",
    "        continue\n",
    "\n",
    "    data_name = row['data_name']\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], data_name)\n",
    "    print(raw_data_folder_path)\n",
    "    data_item = further_processing_class.FurtherProcessing(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "    \n",
    "    # disable printing\n",
    "    data_item.retrieve_or_make_monkey_data()\n",
    "    \n",
    "    # data_item.ff_caught_T_sorted, data_item.ff_index_sorted, data_item.ff_real_position_sorted, data_item.ff_believed_position_sorted, data_item.ff_life_sorted, \\\n",
    "    #     data_item.ff_flash_end_sorted = retrieve_raw_data.make_or_retrieve_ff_info_from_txt_data(\n",
    "    #         data_item.raw_data_folder_path)\n",
    "    # # data_item.make_or_retrieve_closest_stop_to_capture_df()\n",
    "    # # data_item.make_ff_caught_T_new()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trial_durations = np.diff(data_item.ff_caught_T_new)\n",
    "    trial_durations_df = pd.DataFrame(\n",
    "        {'duration_sec': trial_durations, 'trial_index': np.arange(len(trial_durations))})\n",
    "    trial_durations_df['data_name'] = data_name\n",
    "    all_trial_durations_df = pd.concat(\n",
    "        [all_trial_durations_df, trial_durations_df])\n",
    "    \n",
    "    stop_df = show_learning.get_stop_df(data_item.monkey_information, data_item.ff_caught_T_new)\n",
    "    stop_df['data_name'] = data_name\n",
    "    all_stop_df = pd.concat([all_stop_df, stop_df])\n",
    "    \n",
    "    \n",
    "all_trial_durations_df = make_variations_utils.assign_session_id(all_trial_durations_df, 'session')\n",
    "all_stop_df = make_variations_utils.assign_session_id(all_stop_df, 'session')\n",
    "\n",
    "all_stop_df_sessions = all_stop_df[['session', 'captures']].copy()\n",
    "all_stop_df_sessions['stops'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, statsmodels.formula.api as smf\n",
    "\n",
    "# Logistic regression (stop-level)\n",
    "glm_logit = smf.glm(\"captures ~ session\", data=all_stop_df_sessions,\n",
    "                    family=sm.families.Binomial()).fit(cov_type=\"HC0\")\n",
    "\n",
    "print(glm_logit.summary())\n",
    "\n",
    "# Effect size: odds ratio per 10 sessions\n",
    "b1 = glm_logit.params[\"session\"]\n",
    "OR10 = np.exp(10*b1)\n",
    "ci_low, ci_high = np.exp(10*glm_logit.conf_int().loc[\"session\"])\n",
    "pval = glm_logit.pvalues[\"session\"]\n",
    "\n",
    "print(f\"\\nOdds ratio per 10 sessions = {OR10:.2f}, \"\n",
    "      f\"95% CI [{ci_low:.2f}, {ci_high:.2f}], \"\n",
    "      f\"p = {pval:.3g}\")\n",
    "\n",
    "\n",
    "\n",
    "df_sessions = (\n",
    "    all_stop_df_sessions\n",
    "    .groupby(\"session\")\n",
    "    .agg(captures=(\"captures\", \"sum\"),\n",
    "         stops=(\"stops\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "glm_pois = smf.glm(\"captures ~ session\", data=df_sessions,\n",
    "                   family=sm.families.Poisson(),\n",
    "                   offset=np.log(df_sessions[\"stops\"].clip(lower=1))\n",
    "                  ).fit(cov_type=\"HC0\")\n",
    "\n",
    "print(glm_pois.summary())\n",
    "\n",
    "# Effect size: rate ratio per 10 sessions\n",
    "d1 = glm_pois.params[\"session\"]\n",
    "RR10 = np.exp(10*d1)\n",
    "ci_low, ci_high = np.exp(10*glm_pois.conf_int().loc[\"session\"])\n",
    "pval = glm_pois.pvalues[\"session\"]\n",
    "\n",
    "print(f\"\\nRate ratio per 10 sessions = {RR10:.2f}, \"\n",
    "      f\"95% CI [{ci_low:.2f}, {ci_high:.2f}], \"\n",
    "      f\"p = {pval:.3g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Logistic GLM plot (stop-level)\n",
    "show_learning.plot_logistic_stop_success_fit(all_stop_df_sessions, glm_logit)\n",
    "\n",
    "# 2) Poisson (captures per stop) plot (session-level)\n",
    "show_learning.plot_poisson_captures_per_stop_fit(df_sessions, glm_pois)\n",
    "\n",
    "# 3) Early vs Late bars\n",
    "show_learning.plot_early_late_success_bars(all_stop_df_sessions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early vs late contrast\n",
    "\n",
    "https://chatgpt.com/g/g-p-68a34392716c8191839b81db389836c7-multiff/c/68a7b6a5-04e0-8332-aa89-8c4235fe4c4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define early vs late phases\n",
    "\n",
    "Split by session index (first ~⅓ = early, last ~⅓ = late):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, statsmodels.formula.api as smf\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "df = all_stop_df_sessions.copy()\n",
    "\n",
    "# figure out session cutoffs\n",
    "sessions = np.sort(df[\"session\"].unique())\n",
    "n_sess = len(sessions)\n",
    "early_cut = sessions[int(np.floor(n_sess/3)) - 1]\n",
    "late_cut  = sessions[int(np.ceil(2*n_sess/3)) - 1]\n",
    "\n",
    "df[\"phase\"] = np.where(df[\"session\"] <= early_cut, \"early\",\n",
    "                np.where(df[\"session\"] >= late_cut, \"late\", \"mid\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# aggregate captures/stops\n",
    "agg = df.groupby(\"phase\")[\"captures\"].agg([\"sum\",\"count\"])\n",
    "early_succ, early_n = agg.loc[\"early\"]\n",
    "late_succ,  late_n  = agg.loc[\"late\"]\n",
    "\n",
    "count = np.array([late_succ, early_succ])\n",
    "nobs  = np.array([late_n, early_n])\n",
    "\n",
    "z, p = proportions_ztest(count, nobs, alternative=\"larger\")  # one-sided late>early\n",
    "print(f\"Two-proportion z-test: z = {z:.2f}, p = {p:.3g}\")\n",
    "print(f\"Success early = {early_succ}/{early_n} ({early_succ/early_n:.2%}), \"\n",
    "      f\"late = {late_succ}/{late_n} ({late_succ/late_n:.2%})\")\n",
    "\n",
    "\n",
    "\n",
    "sub = df[df[\"phase\"].isin([\"early\",\"late\"])].copy()\n",
    "glm_phase = smf.glm(\"captures ~ C(phase)\", data=sub,\n",
    "                    family=sm.families.Binomial()).fit(cov_type=\"HC0\")\n",
    "print(glm_phase.summary())\n",
    "\n",
    "coef = glm_phase.params[\"C(phase)[T.late]\"]\n",
    "OR = np.exp(coef)\n",
    "ci_low, ci_high = np.exp(glm_phase.conf_int().loc[\"C(phase)[T.late]\"])\n",
    "pval = glm_phase.pvalues[\"C(phase)[T.late]\"]\n",
    "\n",
    "print(f\"Odds ratio (late vs early) = {OR:.2f}, \"\n",
    "      f\"95% CI [{ci_low:.2f}, {ci_high:.2f}], \"\n",
    "      f\"p = {pval:.3g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bruno vs Schro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = compare_two_monkeys_class.CompareTwoMonkeys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.compare_monkeys(exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## success rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.prepare_to_compare_success_rates()\n",
    "tm.make_plot_to_compare_success_rates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single-ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ff = pd.DataFrame({'Bruno': [0.514, 0.570], 'Schro': [0.454, 0.475]}, index=['Mean', 'Median']).reset_index(drop=False).rename(columns={'index': 'statistic'})\n",
    "single_ff = single_ff.melt(id_vars=['statistic'], value_vars=['Bruno', 'Schro'])\n",
    "single_ff = single_ff.sort_values(by=['statistic']).reset_index(drop=True).rename(columns={'variable': 'monkey'})\n",
    "\n",
    "fig = px.bar(single_ff, x='statistic', y='value', color='monkey', barmode='group', text='value', title='Mean and Median Success Ratee', width=500)  # Adjust the width as needed\n",
    "fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "fig.update_yaxes(range=[0, max(single_ff['value']) + 0.1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.plot_pattern_frequencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.plot_feature_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.plot_the_changes_in_pattern_frequencies_over_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.plot_the_changes_in_feature_statistics_over_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.plot_the_changes_in_scatter_around_target_over_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num stops (in one session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = bruno # could be schro too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### during trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qLpdXSl9swu"
   },
   "outputs": [],
   "source": [
    "plot_statistics.plot_num_stops_in_histogram(monkey.combd_all_trial_features, 'num_stops')\n",
    "plt.title('Number of Stops During a Trial')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since target last visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFS7arf9966d"
   },
   "outputs": [],
   "source": [
    "plot_statistics.plot_num_stops_in_histogram(monkey.combd_all_trial_features, 'num_stops_since_last_vis')\n",
    "plt.title('Number of Stops Since Target Last Visible')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### near target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_num_stops_near_target_in_barplot(monkey.combd_all_trial_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scatterplots (vs. cum_distance/distance/time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cum_distance: Integrate the speed of the monkey during the trial. \n",
    "\n",
    "Distance: Find the absolute distance between two targets. Discard the trials where the monkey has gone to a border"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whole trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0314\"\n",
    "data_item = further_processing_class.FurtherProcessing(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.retrieve_or_make_monkey_data()\n",
    "data_item.make_or_retrieve_ff_dataframe(exists_ok=True)\n",
    "data_item.find_patterns()\n",
    "data_item.make_PlotTrials_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_distance_and_num_stops_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_column in ['distance', 'cum_distance']:\n",
    "    df_sub = data_item.num_stops_df[data_item.num_stops_df[x_column] < 2000]\n",
    "    plot_statistics.fit_and_plot_linear_regression(df_sub[x_column].values, df_sub['num_stops'].values, show_regression = True)\n",
    "    plt.title(f'Numbers of stops vs. {x_column}')\n",
    "    plt.xlabel(f'{x_column}', labelpad=15)\n",
    "    plt.ylabel('Numbers of Stops')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove num_stops=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_column in ['distance', 'cum_distance']:\n",
    "    df_sub = data_item.num_stops_df[data_item.num_stops_df[x_column] < 2000]\n",
    "    df_sub = df_sub[df_sub['num_stops'] > 0]\n",
    "    plot_statistics.fit_and_plot_linear_regression(df_sub[x_column].values, df_sub['num_stops'].values, show_regression = True)\n",
    "    plt.title(f'Numbers of stops vs. {x_column}')\n",
    "    plt.xlabel(f'{x_column}', labelpad=15)\n",
    "    plt.ylabel('Numbers of Stops')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since target last visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster_distance = 75\n",
    "exists_ok = True\n",
    "data_item.make_or_retrieve_target_clust_last_vis_df(exists_ok=exists_ok)\n",
    "data_item.make_or_retrieve_target_last_vis_df(exists_ok=exists_ok)\n",
    "target_clust_last_vis_df = data_item.target_clust_last_vis_df\n",
    "target_last_vis_df = data_item.target_last_vis_df\n",
    "\n",
    "num_stops = organize_patterns_and_features.get_num_stops_array(data_item.monkey_information, \n",
    "                                                                np.arange(len(data_item.ff_caught_T_new)))\n",
    "\n",
    "num_stops_since_clust_last_vis = organize_patterns_and_features._calculate_num_stops_since_last_vis(data_item.monkey_information, \n",
    "                                                                                                      len(data_item.ff_caught_T_new), \n",
    "                                                                                                      target_clust_last_vis_df['time_since_last_vis'].values)\n",
    "num_stops_since_last_vis = organize_patterns_and_features._calculate_num_stops_since_last_vis(data_item.monkey_information, \n",
    "                                                                                            len(data_item.ff_caught_T_new),\n",
    "                                                                                            target_last_vis_df['time_since_last_vis'].values)\n",
    "\n",
    "num_stops_near_target = organize_patterns_and_features._calculate_num_stops_near_target(data_item.monkey_information, data_item.ff_caught_T_new,\n",
    "                                                                                        data_item.ff_real_position_sorted, max_cluster_distance)\n",
    "\n",
    "target_clust_last_vis_df = prep_target_data.add_num_stops_to_target_last_vis_df(target_clust_last_vis_df, data_item.ff_caught_T_new, num_stops, num_stops_near_target, num_stops_since_clust_last_vis)\n",
    "target_last_vis_df = prep_target_data.add_num_stops_to_target_last_vis_df(target_last_vis_df, data_item.ff_caught_T_new, num_stops, num_stops_near_target, num_stops_since_last_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target last visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_last_seen_info_vs_stops(target_last_vis_df, filter_by_p_value=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target cluster last visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_clust_last_vis_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_last_seen_info_vs_stops(target_clust_last_vis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num ffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = bruno \n",
    "# monkey = schro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of visible ffs at any point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_number_of_visible_ff_per_point_in_histogram(data_item.ff_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of ffs in memory at any point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_number_of_ff_in_memory_per_point_in_histogram(data_item.ff_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alive ff around tagret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_num_ff_around_target_in_barplot(monkey.combd_all_trial_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_num_ff_caught_in_a_row_in_barplot(monkey.combd_pattern_frequencies, show_one_in_a_row = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_num_ff_caught_in_a_row_in_pie_chart(monkey.agg_pattern_frequencies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JwQP0QC6lZb9"
   },
   "source": [
    "# Compare target with non_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take out points where both targets and non-targets are either visible or in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7Ag4yknmH9l"
   },
   "outputs": [],
   "source": [
    "plot_statistics.compare_target_with_non_targets(data_item.ff_dataframe, var_of_interest='ff_angle')\n",
    "plot_statistics.compare_target_with_non_targets(data_item.ff_dataframe, var_of_interest='ff_angle_boundary')\n",
    "plot_statistics.compare_target_with_non_targets(data_item.ff_dataframe, var_of_interest='ff_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take out points where both targets and non-targets are visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywMclUhRmK_W"
   },
   "outputs": [],
   "source": [
    "ff_dataframe_subset = data_item.ff_dataframe[data_item.ff_dataframe['visible']==1]\n",
    "plot_statistics.compare_target_with_non_targets(ff_dataframe_subset, var_of_interest='ff_angle')\n",
    "plot_statistics.compare_target_with_non_targets(ff_dataframe_subset, var_of_interest='ff_angle_boundary')\n",
    "plot_statistics.compare_target_with_non_targets(ff_dataframe_subset, var_of_interest='ff_distance')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
