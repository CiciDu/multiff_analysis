{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, psth_postprocessing, psth_stats, compare_events, dpca_utils, prep_stop_psth_data\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import get_stops_utils\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0321\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0329\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0403\"\n",
    "#raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0413\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "# # raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\"\n",
    "# #raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\"\n",
    "\n",
    "# #raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0316\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "\n",
    "\n",
    "\n",
    "#raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0221\"\n",
    "#raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "# pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "#     pn.planning_data_by_point)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "if not hasattr(pn, 'spikes_df'):\n",
    "    pn.retrieve_or_make_monkey_data()\n",
    "    pn.spikes_df = neural_data_processing.make_spikes_df(pn.raw_data_folder_path, pn.ff_caught_T_sorted,\n",
    "                                                            sampling_rate=pn.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.spikes_df['cluster'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# NEXT: try stop end time instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Get captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(get_stops_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_or_retrieve_stop_category_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example wiring (mirrors your original usage)\n",
    "captures_df, valid_captures_df, filtered_no_capture_stops_df, stops_with_stats = get_stops_utils.prepare_no_capture_and_captures(\n",
    "    monkey_information=pn.monkey_information,\n",
    "    closest_stop_to_capture_df=pn.closest_stop_to_capture_df,\n",
    "    ff_caught_T_new=pn.ff_caught_T_new,\n",
    "    distance_col=\"distance_from_ff_to_stop\",\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Get misses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "##  one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = [\"stop_id\", \"stop_id_duration\", \"stop_id_start_time\", \"stop_id_end_time\"]\n",
    "\n",
    "pn.make_one_stop_w_ff_df()\n",
    "one_stop_miss_df = pn.one_stop_w_ff_df[['first_stop_point_index', 'first_stop_time', 'latest_visible_ff']].copy()\n",
    "one_stop_miss_df.rename(columns={'first_stop_point_index': 'stop_point_index', 'first_stop_time': 'stop_time'}, inplace=True)\n",
    "one_stop_miss_df[columns_to_add] = pn.monkey_information.loc[one_stop_miss_df['stop_point_index'], columns_to_add].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_or_retrieve_stop_category_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_try_a_few_times_info()\n",
    "pn.get_give_up_after_trying_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = [\"stop_id\", \"stop_id_duration\", \"stop_id_start_time\", \"stop_id_end_time\"]\n",
    "shared_columns = [\"stop_point_index\", \"stop_time\"] + columns_to_add\n",
    "\n",
    "# --- Build expanded + ordered tables for GUAT / TAFT ---\n",
    "GUAT_expanded = get_stops_utils._expand_trials(pn.GUAT_trials_df, pn.monkey_information)\n",
    "TAFT_expanded = get_stops_utils._expand_trials(pn.TAFT_trials_df, pn.monkey_information)\n",
    "\n",
    "# add stop_id to GUAT_trials_df and TAFT_trials_df\n",
    "GUAT_expanded[columns_to_add] = pn.monkey_information.loc[GUAT_expanded['stop_point_index'], columns_to_add].values\n",
    "TAFT_expanded[columns_to_add] = pn.monkey_information.loc[TAFT_expanded['stop_point_index'], columns_to_add].values\n",
    "\n",
    "\n",
    "GUAT = get_stops_utils._add_cluster_ordering(GUAT_expanded)\n",
    "TAFT = get_stops_utils._add_cluster_ordering(TAFT_expanded)\n",
    "\n",
    "# --- Per-cluster slices (consistent, vectorized) ---\n",
    "# First stop in each cluster\n",
    "GUAT_first = GUAT[GUAT[\"is_first\"]].reset_index(drop=True)\n",
    "TAFT_first = TAFT[TAFT[\"is_first\"]].reset_index(drop=True)\n",
    "\n",
    "# Last stop in each cluster\n",
    "GUAT_last = GUAT[GUAT[\"is_last\"]].reset_index(drop=True)\n",
    "capture_TAFT_last = TAFT[TAFT[\"is_last\"]].reset_index(drop=True)\n",
    "\n",
    "# Middle stops (exclude first and last)\n",
    "GUAT_middle = GUAT[GUAT[\"is_middle\"]].reset_index(drop=True)\n",
    "TAFT_middle = TAFT[TAFT[\"is_middle\"]].reset_index(drop=True)\n",
    "\n",
    "# “First several” = all but the last stop in each cluster\n",
    "GUAT_nonfinal = GUAT[GUAT[\"order_in_cluster\"] < GUAT[\"cluster_size\"] - 1].reset_index(drop=True)\n",
    "TAFT_nonfinal = TAFT[TAFT[\"order_in_cluster\"] < TAFT[\"cluster_size\"] - 1].reset_index(drop=True)\n",
    "\n",
    "# Combine the “first several” from both, keep only columns you care about, then sort by index\n",
    "both_nonfinal = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            GUAT_nonfinal[shared_columns],\n",
    "            TAFT_nonfinal[shared_columns],\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    .sort_values(\"stop_point_index\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "persist_both_first = pd.concat([GUAT_first[shared_columns], \n",
    "                         TAFT_first[shared_columns]])\n",
    "\n",
    "both_middle = pd.concat([GUAT_middle[shared_columns], \n",
    "                         TAFT_middle[shared_columns]])\n",
    "\n",
    "# Optional: if you also want “last several” (all but the first), it’s symmetrical:\n",
    "# GUAT_last_several = GUAT[GUAT[\"order_in_cluster\"] > 0].reset_index(drop=True)\n",
    "# capture_TAFT_last_several = TAFT[TAFT[\"order_in_cluster\"] > 0].reset_index(drop=True)\n",
    "\n",
    "GUAT_last_plus_single_miss = pd.concat([GUAT_last[shared_columns], \n",
    "                                         one_stop_miss_df[shared_columns]])\n",
    "\n",
    "all_misses = pd.concat([one_stop_miss_df[shared_columns], \n",
    "                                         GUAT_expanded[shared_columns],\n",
    "                                         TAFT_nonfinal[shared_columns]\n",
    "                                         ])\n",
    "\n",
    "all_first_misses = pd.concat(\n",
    "    [one_stop_miss_df[shared_columns], GUAT_first[shared_columns], TAFT_first[shared_columns]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "all_last_misses = pd.concat(\n",
    "    [one_stop_miss_df[shared_columns], GUAT_last[shared_columns]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# captures not in TAFT last (assuming capture_TAFT_last is a subset of captures)\n",
    "captures_minus_TAFT_last = compare_events.diff_by(valid_captures_df, capture_TAFT_last, key='stop_id')\n",
    "\n",
    "# non-captures excluding those flagged as 'all_misses'\n",
    "non_captures_minus_all_misses = compare_events.diff_by(filtered_no_capture_stops_df, all_misses, key='stop_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Dwell time on stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine all datasets, tagging each\n",
    "df_compare = pd.concat([\n",
    "    GUAT_last.assign(group='retry miss Last'),\n",
    "    capture_TAFT_last.assign(group='retry capture Last'),\n",
    "    valid_captures_df.assign(group='Capture'),\n",
    "    \n",
    "    GUAT_middle.assign(group='retry miss Middle'),\n",
    "    TAFT_middle.assign(group='retry capture Middle'),\n",
    "    \n",
    "    \n",
    "    # GUAT_nonfinal.assign(group='retry miss Non-final'),\n",
    "    # TAFT_nonfinal.assign(group='retry capture Non-final'),\n",
    "    \n",
    "    \n",
    "    GUAT_first.assign(group='retry miss First'),\n",
    "    TAFT_first.assign(group='retry capture First'),\n",
    "    \n",
    "    valid_captures_df.assign(group='Capture'),\n",
    "    #filtered_no_capture_stops_df.assign(group='Non-capture'),\n",
    "    one_stop_miss_df.assign(group='One-stop Miss'),\n",
    "    \n",
    "])\n",
    "\n",
    "# Plot all as separate violins\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(\n",
    "    data=df_compare,\n",
    "    x='group',\n",
    "    y='stop_id_duration',\n",
    "    inner='quartile',\n",
    "    cut=0,\n",
    "    scale='width'\n",
    ")\n",
    "\n",
    "# Customize\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Stop Duration (s)')\n",
    "#plt.ylim(0, 10)\n",
    "plt.title('Stop Duration Distribution Across Trial Types')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=df_compare,\n",
    "    x='group',\n",
    "    y='stop_id_duration',\n",
    "    showfliers=False,\n",
    "    width=0.6\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=df_compare,\n",
    "    x='group',\n",
    "    y='stop_id_duration',\n",
    "    color='black',\n",
    "    alpha=0.4,\n",
    "    jitter=0.3,\n",
    "    size=2\n",
    ")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.ylabel('Stop Duration (s)')\n",
    "plt.title('Stop Duration Distribution Across Trial Types')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --- Keep only retry-related rows ---\n",
    "retry_groups = [\n",
    "    'retry miss First', 'retry miss Middle', 'retry miss Last',\n",
    "    'retry capture First', 'retry capture Middle', 'retry capture Last'\n",
    "]\n",
    "df_retry = df_compare[df_compare['group'].isin(retry_groups)].copy()\n",
    "\n",
    "# --- Extract phase and outcome type ---\n",
    "df_retry['phase'] = df_retry['group'].str.extract(r'(First|Middle|Last)')\n",
    "df_retry['type'] = df_retry['group'].str.contains('capture', case=False).map({True: 'Capture', False: 'Miss'})\n",
    "\n",
    "# --- Define explicit phase order ---\n",
    "phase_order = ['First', 'Middle', 'Last']\n",
    "\n",
    "# --- Split violin plot ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(\n",
    "    data=df_retry,\n",
    "    x='phase',\n",
    "    y='stop_id_duration',\n",
    "    hue='type',\n",
    "    split=True,\n",
    "    inner='quartile',\n",
    "    cut=0,\n",
    "    order=phase_order\n",
    ")\n",
    "plt.ylabel('Stop Duration (s)')\n",
    "plt.xlabel('Retry Phase')\n",
    "plt.title('Retry Miss vs Capture Stop Durations by Phase')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Keep only retry groups explicitly ---\n",
    "retry_groups = [\n",
    "    'retry miss First', 'retry miss Middle', 'retry miss Last',\n",
    "    'retry capture First', 'retry capture Middle', 'retry capture Last'\n",
    "]\n",
    "df_retry = df_compare[df_compare['group'].isin(retry_groups)].copy()\n",
    "\n",
    "# --- Extract phase (First/Middle/Last) and outcome (Success/Fail) ---\n",
    "df_retry['phase'] = df_retry['group'].str.extract(r'(First|Middle|Last)')\n",
    "df_retry['outcome'] = np.where(df_retry['group'].str.contains('capture', case=False),\n",
    "                               'Success', 'Fail')\n",
    "\n",
    "# --- Add regular captures as a final category for successes ---\n",
    "df_capture = valid_captures_df.copy()\n",
    "df_capture['phase'] = 'Capture'\n",
    "df_capture['outcome'] = 'Success'\n",
    "\n",
    "# --- Add one-stop misses as a final category for fails ---\n",
    "df_first_miss = one_stop_miss_df.copy()\n",
    "df_first_miss['phase'] = 'First Miss'\n",
    "df_first_miss['outcome'] = 'Fail'\n",
    "\n",
    "# --- Combine everything ---\n",
    "df_all = pd.concat([df_retry, df_capture, df_first_miss], ignore_index=True)\n",
    "\n",
    "# --- Define phase orders (add 'Capture' or 'First Miss' at the end) ---\n",
    "phase_order_success = ['First', 'Middle', 'Last', 'Capture']\n",
    "phase_order_fail = ['First', 'Middle', 'Last', 'First Miss']\n",
    "\n",
    "# --- Separate success/fail for plotting ---\n",
    "df_success = df_all[df_all['outcome'] == 'Success']\n",
    "df_fail = df_all[df_all['outcome'] == 'Fail']\n",
    "\n",
    "# --- Retry + Success (includes Capture) ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(\n",
    "    data=df_success,\n",
    "    x='phase',\n",
    "    y='stop_id_duration',\n",
    "    order=phase_order_success,\n",
    "    inner='quartile',\n",
    "    scale='width',\n",
    "    cut=0,\n",
    "    color='skyblue'\n",
    ")\n",
    "plt.title('Retry + Success (Capture) Stop Durations by Phase')\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Stop Duration (s)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Retry + Fail (includes First Miss) ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(\n",
    "    data=df_fail,\n",
    "    x='phase',\n",
    "    y='stop_id_duration',\n",
    "    order=phase_order_fail,\n",
    "    inner='quartile',\n",
    "    scale='width',\n",
    "    cut=0,\n",
    "    color='salmon'\n",
    ")\n",
    "plt.title('Retry + Fail (Miss) Stop Durations by Phase')\n",
    "plt.xlabel('Phase')\n",
    "plt.ylabel('Stop Duration (s)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Compare 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine both dataframes and label them\n",
    "df_compare = pd.concat([\n",
    "    GUAT_last.assign(group='Give-up Last Stop'),\n",
    "    GUAT_nonfinal.assign(group='Non-final Stop')\n",
    "])\n",
    "\n",
    "# Add a common x category so both appear in the same violin\n",
    "df_compare['category'] = 'Stop Duration'\n",
    "\n",
    "# Create split violin plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.violinplot(\n",
    "    data=df_compare,\n",
    "    x='category',\n",
    "    y='stop_id_duration',\n",
    "    hue='group',\n",
    "    split=True,\n",
    "    inner='quartile',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# Customize\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Stop Duration (s)')\n",
    "plt.title('Stop Duration Distribution: Give-up Last vs Non-final Stops')\n",
    "plt.legend(title='')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "groups = df_compare['group'].unique()\n",
    "pairs = list(combinations(groups, 2))\n",
    "\n",
    "for g1, g2 in pairs:\n",
    "    x1 = df_compare.loc[df_compare['group'] == g1, 'stop_id_duration']\n",
    "    x2 = df_compare.loc[df_compare['group'] == g2, 'stop_id_duration']\n",
    "    stat, p = mannwhitneyu(x1, x2, alternative='two-sided')\n",
    "    print(f'{g1} vs {g2}: p = {p:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# omnibus across groups\n",
    "samples = [df_compare.loc[df_compare['group'] == g, 'stop_id_duration'] for g in df_compare['group'].unique()]\n",
    "kw_stat, kw_p = kruskal(*samples)\n",
    "print(f'Overall Kruskal–Wallis: H={kw_stat:.3f}, p={kw_p:.2e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare.groupby('group')['stop_id_duration'].median().sort_values()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
