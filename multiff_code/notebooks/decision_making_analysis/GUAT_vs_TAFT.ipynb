{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if using google drive\n",
    "# %cd /content/drive/MyDrive/ff_repo/Multifirefly-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from data_wrangling import specific_utils, combine_info_utils, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_collect_info_class, GUAT_combine_info_class, process_GUAT_trials_class\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_x_sessions_class, helper_GUAT_vs_TAFT_class\n",
    "from visualization.matplotlib_tools import plot_trials, plot_behaviors_utils\n",
    "from visualization.animation import animation_class\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from visualization.plotly_polar_tools import plotly_utils_polar, plotly_for_ff_polar, plotly_for_trajectory_polar\n",
    "from machine_learning.ml_methods import ml_methods_class\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import design_bases\n",
    "from machine_learning.ml_methods.advanced_ml_methods import advanced_regression_utils, advanced_classification_utils, reg_feat_importance\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from importlib import reload\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict TAFT vs GUAT\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    cgts = GUAT_vs_TAFT_x_sessions_class.GUATandTAFTacrossSessionsClass()\n",
    "    combd_GUAT_x_df, combd_TAFT_x_df = cgts.streamline_getting_combd_GUAT_or_TAFT_x_df(monkey_name=monkey_name)\n",
    "\n",
    "# Predict num_stops\n",
    "combined_info_exists_ok = True\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    gc_kwargs_temp = helper_GUAT_vs_TAFT_class.gc_kwargs.copy()\n",
    "    gc_kwargs_temp['num_old_ff_per_row'] = 2\n",
    "    gc_kwargs_temp['num_new_ff_per_row'] = 3\n",
    "\n",
    "    gas = GUAT_combine_info_class.GUATCombineInfoAcrossSessions(gc_kwargs_temp, monkey_name=monkey_name)\n",
    "    gas.retrieve_or_make_combined_info(gc_kwargs_temp, combined_info_exists_ok=combined_info_exists_ok, \n",
    "                                    traj_df_exist_in_GUAT_store_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_name = 'monkey_Bruno'\n",
    "cgts = GUAT_vs_TAFT_x_sessions_class.GUATandTAFTacrossSessionsClass()\n",
    "cgts.streamline_getting_combd_decision_making_basic_ff_info(monkey_name=monkey_name, exists_ok=True)\n",
    "decision_making_basic_ff_info_cleaned = general_utils.drop_rows_with_any_na(cgts.combd_decision_making_basic_ff_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data for one session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exists_ok = True\n",
    "cgt = GUAT_vs_TAFT_class.GUATvsTAFTclass(ref_point_mode='time', \n",
    "                                            raw_data_folder_path='all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330',\n",
    "                                            ref_point_value=-1.5,)\n",
    "cgt.make_decision_making_basic_ff_info()\n",
    "decision_making_basic_ff_info_cleaned = cgt.decision_making_basic_ff_info_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.decision_making_basic_ff_info['whether_switched'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_making_basic_ff_info_cleaned['whether_switched'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_making_basic_ff_info_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['ff_distance_ff_last_seen',\n",
    "            'ff_angle_ff_last_seen',\n",
    "            'ff_angle_boundary_ff_last_seen',\n",
    "            'time_since_ff_last_seen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['ff_distance', 'ff_angle','ff_angle_boundary', 'time_since_last_vis'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var_df = decision_making_basic_ff_info_cleaned[attributes]\n",
    "y_var_df = decision_making_basic_ff_info_cleaned[['whether_switched']].copy()\n",
    "\n",
    "ml_inst = ml_methods_class.MlMethods(x_var_df=x_var_df,\n",
    "                                     y_var_df=y_var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inst.use_ml_model_for_classification(ml_inst.x_var_df, ml_inst.y_var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X2 = sm.add_constant(ml_inst.x_var_df)  # add intercept\n",
    "logit_model = sm.Logit(ml_inst.y_var_df, X2)  # logistic regression\n",
    "result = logit_model.fit()\n",
    "\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model's feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "important = model.coef_[0] != 0\n",
    "X_new = ml_inst.x_var_df.loc[:, important]\n",
    "X_new.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction=\"forward\")\n",
    "X_new = sfs.fit_transform(ml_inst.x_var_df, ml_inst.y_var_df.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boolean mask of selected features\n",
    "mask = sfs.get_support()\n",
    "\n",
    "# Names of selected features\n",
    "selected_features = ml_inst.x_var_df.columns[mask]\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels, logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X2 = sm.add_constant(ml_inst.x_var_df)  # add intercept\n",
    "logit_model = sm.Logit(ml_inst.y_var_df, X2)  # logistic regression\n",
    "result = logit_model.fit()\n",
    "\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": ml_inst.x_var_df.columns,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad_boosting (so that we can see feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingClassifier(\n",
    "    learning_rate=0.05, max_depth=7, max_features='sqrt',\n",
    "    min_samples_leaf=2, min_samples_split=7,\n",
    "    n_estimators=500, subsample=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': ml_inst.x_var_df.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 18))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Determine significant features (e.g., importance > 0.01)\n",
    "significant_features = feature_importances_df[feature_importances_df['Importance'] > 0.01]\n",
    "print(\"Significant features:\")\n",
    "print(significant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune = False\n",
    "\n",
    "ml_inst.use_train_test_split(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "model, y_pred, model_comparison_df = advanced_classification_utils.use_advanced_model_for_classification(\n",
    "    ml_inst.X_train, ml_inst.y_train, ml_inst.X_test, ml_inst.y_test,\n",
    "    kfold_cv=5,\n",
    "    tune=tune,                # turn tuning on/off\n",
    "    n_iter=30,                # ~3k samples sweet spot\n",
    "    tune_scoring=\"balanced_accuracy\",\n",
    "    checkpoint_dir=\"pred_num_stops/cls_runs\",   # folder to save progress\n",
    "    resume=False,              # skip finished models on rerun\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btw, what's the reference point ??? (like at which point are we predicting TAFT vs GUAT?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # what might be interesting to add from TAFT_x_df:\n",
    " \n",
    " 'angle_from_stop_to_nxt_ff',\n",
    " 'angle_from_cur_ff_to_nxt_ff',\n",
    " \n",
    " \n",
    " # monkey's own curvature info?\n",
    " # so like find curv of traj based on [-25, 0] or something,\n",
    " \n",
    " \n",
    " # rather than ff last seen, what about at ref?\n",
    " # (btw, I'm not gonna use exactly the below...will tweak it)\n",
    " 'cur_ff_angle_diff_boundary_at_ref',\n",
    " 'cur_ff_flash_duration_at_ref',\n",
    " 'cur_ff_earliest_flash_rel_time_at_ref',\n",
    " 'cur_ff_latest_flash_rel_time_at_ref',\n",
    " \n",
    " \n",
    " # and also all the eye-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.TAFT_x_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare distributions of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['ff_distance_ff_last_seen', \n",
    "            'time_since_ff_last_seen',\n",
    "            'ff_angle_ff_last_seen', \n",
    "            'ff_angle_boundary_ff_last_seen',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['ff_distance', 'ff_angle', 'ff_angle_boundary', 'time_since_last_vis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature in significant_features, plot the histogram of the feature for each class\n",
    "for feature in attributes:\n",
    "    sns.histplot(x=feature, data=decision_making_basic_ff_info_cleaned, stat='probability', kde=False, hue='whether_switched', common_norm=False)\n",
    "    plt.title(f'{feature} histogram')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complex features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAFT = cgt.TAFT_x_df.copy()\n",
    "TAFT['whether_TAFT'] = 1\n",
    "GUAT = cgt.GUAT_x_df.copy()\n",
    "GUAT['whether_TAFT'] = 0\n",
    "both_df = pd.concat([TAFT, GUAT], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature in significant_features, plot the histogram of the feature for each class\n",
    "for feature in ['cur_ff_distance_at_ref', 'cur_ff_angle_at_ref']:\n",
    "    sns.histplot(x=feature, data=both_df, stat='probability', kde=False, hue='whether_TAFT', common_norm=False)\n",
    "    plt.title(f'{feature} histogram')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature in significant_features, plot the histogram of the feature for each class\n",
    "max_features_to_plot = 3\n",
    "count = 0\n",
    "for feature in significant_features['Feature']:\n",
    "    sns.histplot(x=feature, data=both_df, stat='probability', kde=False, hue='whether_TAFT', common_norm=False)\n",
    "    plt.title(f'{feature} histogram')\n",
    "    plt.show()\n",
    "    count += 1\n",
    "    if count >= max_features_to_plot:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can skip this if only wanting ML results\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# ml_inst.use_vif(ml_inst.x_var_df)\n",
    "# features_w_big_vif = ml_inst.vif_df[ml_inst.vif_df['vif'] > 100].feature.values\n",
    "# #ml_inst.x_var_df = ml_inst.x_var_df.drop(columns=features_w_big_vif)\n",
    "# ml_inst.vif_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_columns = ml_inst.vif_df[ml_inst.vif_df[\"VIF\"] > 2000].feature.values\n",
    "# ml_inst.show_correlation_heatmap(specific_columns=specific_columns)\n",
    "# ml_inst.show_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test and see what is taking so long in running a function\n",
    "# import cProfile\n",
    "\n",
    "# cProfile.run(\"cgt.streamline_getting_GUAT_or_TAFT_x_df(GUAT_or_TAFT='GUAT', exists_ok=False)\", sort='cumtime')\n",
    "\n",
    "# #ncalls  tottime  percall  cumtime  percall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
