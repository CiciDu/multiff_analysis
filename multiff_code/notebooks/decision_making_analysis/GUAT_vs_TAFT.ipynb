{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if using google drive\n",
    "# %cd /content/drive/MyDrive/ff_repo/Multifirefly-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from data_wrangling import specific_utils, combine_info_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils\n",
    "from decision_making_analysis.decision_making import decision_making_class, decision_making_utils, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_collect_info_class, GUAT_combine_info_class, process_GUAT_trials_class\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import GUAT_vs_TAFT_class, GUAT_vs_TAFT_x_sessions_class, helper_GUAT_vs_TAFT_class\n",
    "from visualization.matplotlib_tools import plot_trials, plot_behaviors_utils\n",
    "from visualization.animation import animation_class\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curv_of_traj_utils\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from visualization.plotly_polar_tools import plotly_utils_polar, plotly_for_ff_polar, plotly_for_trajectory_polar\n",
    "from machine_learning.ml_methods import ml_methods_class\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os, sys, sys\n",
    "from importlib import reload\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict TAFT vs GUAT\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    cgtxs = GUAT_vs_TAFT_x_sessions_class.GUATandTAFTacrossSessionsClass()\n",
    "    combd_GUAT_x_df, combd_TAFT_x_df = cgtxs.streamline_getting_combd_GUAT_or_TAFT_x_df(monkey_name=monkey_name)\n",
    "\n",
    "# Predict num_stops\n",
    "combined_info_exists_ok = True\n",
    "for monkey_name in ['monkey_Bruno', 'monkey_Schro']:\n",
    "    gc_kwargs_temp = helper_GUAT_vs_TAFT_class.gc_kwargs.copy()\n",
    "    gc_kwargs_temp['num_old_ff_per_row'] = 2\n",
    "    gc_kwargs_temp['num_new_ff_per_row'] = 3\n",
    "\n",
    "    gas = GUAT_combine_info_class.GUATCombineInfoAcrossSessions(gc_kwargs_temp, monkey_name=monkey_name)\n",
    "    gas.retrieve_or_make_combined_info(gc_kwargs_temp, combined_info_exists_ok=combined_info_exists_ok, \n",
    "                                    traj_df_exist_in_GUAT_store_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict TAFT vs GUAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_name='monkey_Bruno'\n",
    "cgtxs = GUAT_vs_TAFT_x_sessions_class.GUATandTAFTacrossSessionsClass()\n",
    "# now, put the data into a GUATvsTAFTclass instance for downstream analysis\n",
    "cgt = GUAT_vs_TAFT_class.GUATvsTAFTclass()\n",
    "cgt.GUAT_x_df, cgt.TAFT_x_df = cgtxs.streamline_getting_combd_GUAT_or_TAFT_x_df(monkey_name=monkey_name)\n",
    "\n",
    "cgt.get_x_and_y_var_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data for one session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exists_ok = True\n",
    "cgt = GUAT_vs_TAFT_class.GUATvsTAFTclass(ref_point_mode='time', \n",
    "                                            raw_data_folder_path='all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330',\n",
    "                                            ref_point_value=-1.5,)\n",
    "cgt.streamline_getting_GUAT_or_TAFT_x_df(GUAT_or_TAFT='TAFT', exists_ok=exists_ok)\n",
    "cgt.streamline_getting_GUAT_or_TAFT_x_df(GUAT_or_TAFT='GUAT', exists_ok=exists_ok)\n",
    "\n",
    "cgt.get_x_and_y_var_df() # it used concatenated self.TAFT_x_df and self.GUAT_x_df to make self.x_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test and see what is taking so long in running a function\n",
    "# import cProfile\n",
    "\n",
    "# cProfile.run(\"cgt.streamline_getting_GUAT_or_TAFT_x_df(GUAT_or_TAFT='GUAT', exists_ok=False)\", sort='cumtime')\n",
    "\n",
    "# #ncalls  tottime  percall  cumtime  percall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.TAFT_trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.TAFT_x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btw, what's the reference point ??? (like at which point are we predicting TAFT vs GUAT?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # what might be interesting to add:\n",
    " \n",
    "  'angle_from_stop_to_nxt_ff',\n",
    " 'angle_from_cur_ff_to_nxt_ff',\n",
    " \n",
    " \n",
    " # monkey's own curvature info?\n",
    " # so like find curv of traj based on [-25, 0] or something,\n",
    " \n",
    " \n",
    " \n",
    " # rather than ff last seen, what about at ref?\n",
    " # (btw, I'm not gonna use exactly the below...will tweak it)\n",
    " 'cur_ff_angle_diff_boundary_at_ref',\n",
    " 'cur_ff_flash_duration_at_ref',\n",
    " 'cur_ff_earliest_flash_rel_time_at_ref',\n",
    " 'cur_ff_latest_flash_rel_time_at_ref',\n",
    " \n",
    " \n",
    " # and also all the eye-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.TAFT_x_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use simple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eh....can only do on one session for right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.streamline_getting_GUAT_or_TAFT_df(GUAT_or_TAFT='TAFT')\n",
    "cgt.streamline_getting_GUAT_or_TAFT_df(GUAT_or_TAFT='GUAT')\n",
    "cgt.make_or_retrieve_target_clust_last_vis_df(exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.TAFT_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_var_df = pd.concat([cgt.TAFT_df, cgt.GUAT_df], axis=0).reset_index(\n",
    "    drop=True).drop(columns=['stop_point_index'])\n",
    "self.x_var_df['dir_from_cur_ff_same_side'] = self.x_var_df['dir_from_cur_ff_same_side'].astype(\n",
    "    int)\n",
    "self.y_var_df = pd.DataFrame(np.array(\n",
    "    [1]*len(self.TAFT_x_df) + [0]*len(self.GUAT_x_df)).reshape(-1, 1), columns=['y_var'])\n",
    "\n",
    "columns_with_na = self.x_var_df.columns[self.x_var_df.isna(\n",
    ").any()].tolist()\n",
    "print(\n",
    "    f'There are {len(columns_with_na)} columns with NA that are dropped. {self.x_var_df.shape[1]} columns are left. The dropped columns with number of NA are:')\n",
    "print(self.x_var_df[columns_with_na].isna().sum())\n",
    "self.x_var_df = self.x_var_df.drop(columns=columns_with_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import design_bases\n",
    "x_var_df_reduced = design_bases.reduce_df(cgt.x_var_df)\n",
    "\n",
    "ml_inst = ml_methods_class.MlMethods(x_var_df=x_var_df_reduced,\n",
    "                                     y_var_df=cgt.y_var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_keywords = (\"eye\", \"gaze\", \"LD\", \"RD\")\n",
    "eye_cols = [col for col in cgt.x_var_df.columns if any(k in col for k in eye_keywords)]\n",
    "cgt.x_var_df = cgt.x_var_df.drop(columns=eye_cols)\n",
    "\n",
    "\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import design_bases\n",
    "x_var_df_reduced = design_bases.reduce_df(cgt.x_var_df)\n",
    "\n",
    "ml_inst = ml_methods_class.MlMethods(x_var_df=x_var_df_reduced,\n",
    "                                     y_var_df=cgt.y_var_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use more complex features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgt.get_x_and_y_var_df()\n",
    "\n",
    "eye_keywords = (\"eye\", \"gaze\", \"LD\", \"RD\")\n",
    "eye_cols = [col for col in cgt.x_var_df.columns if any(k in col for k in eye_keywords)]\n",
    "cgt.x_var_df = cgt.x_var_df.drop(columns=eye_cols)\n",
    "\n",
    "\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import design_bases\n",
    "x_var_df_reduced = design_bases.reduce_df(cgt.x_var_df)\n",
    "\n",
    "ml_inst = ml_methods_class.MlMethods(x_var_df=x_var_df_reduced,\n",
    "                                     y_var_df=cgt.y_var_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can skip this if only wanting ML results\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# ml_inst.use_vif(ml_inst.x_var_df)\n",
    "# features_w_big_vif = ml_inst.vif_df[ml_inst.vif_df['vif'] > 100].feature.values\n",
    "# #ml_inst.x_var_df = ml_inst.x_var_df.drop(columns=features_w_big_vif)\n",
    "# ml_inst.vif_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific_columns = ml_inst.vif_df[ml_inst.vif_df[\"VIF\"] > 2000].feature.values\n",
    "# ml_inst.show_correlation_heatmap(specific_columns=specific_columns)\n",
    "# ml_inst.show_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inst.x_var_df.columns[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inst.use_ml_model_for_classification(ml_inst.x_var_df, ml_inst.y_var_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model's feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(ml_inst.x_var_df, ml_inst.y_var_df)\n",
    "important = model.coef_[0] != 0\n",
    "X_new = ml_inst.x_var_df.loc[:, important]\n",
    "X_new.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction=\"forward\")\n",
    "X_new = sfs.fit_transform(ml_inst.x_var_df, ml_inst.y_var_df.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boolean mask of selected features\n",
    "mask = sfs.get_support()\n",
    "\n",
    "# Names of selected features\n",
    "selected_features = ml_inst.x_var_df.columns[mask]\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad_boosting (so that we can see feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(ml_inst.x_var_df, ml_inst.y_var_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingClassifier(\n",
    "    learning_rate=0.05, max_depth=7, max_features='sqrt',\n",
    "    min_samples_leaf=2, min_samples_split=7,\n",
    "    n_estimators=500, subsample=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': ml_inst.x_var_df.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 18))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Determine significant features (e.g., importance > 0.01)\n",
    "significant_features = feature_importances_df[feature_importances_df['Importance'] > 0.01]\n",
    "print(\"Significant features:\")\n",
    "print(significant_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare distributions of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAFT = cgt.TAFT_x_df.copy()\n",
    "TAFT['whether_TAFT'] = 1\n",
    "GUAT = cgt.GUAT_x_df.copy()\n",
    "GUAT['whether_TAFT'] = 0\n",
    "both_df = pd.concat([TAFT, GUAT], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature in significant_features, plot the histogram of the feature for each class\n",
    "for feature in ['cur_ff_distance_at_ref', 'cur_ff_angle_at_ref']:\n",
    "    sns.histplot(x=feature, data=both_df, stat='probability', kde=False, hue='whether_TAFT', common_norm=False)\n",
    "    plt.title(f'{feature} histogram')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature in significant_features, plot the histogram of the feature for each class\n",
    "max_features_to_plot = 3\n",
    "count = 0\n",
    "for feature in significant_features['Feature']:\n",
    "    sns.histplot(x=feature, data=both_df, stat='probability', kde=False, hue='whether_TAFT', common_norm=False)\n",
    "    plt.title(f'{feature} histogram')\n",
    "    plt.show()\n",
    "    count += 1\n",
    "    if count >= max_features_to_plot:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
