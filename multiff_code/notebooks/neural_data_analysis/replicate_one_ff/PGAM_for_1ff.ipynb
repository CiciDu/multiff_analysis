{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# PGAM Tutorial\n",
    "\n",
    "## Introduction\n",
    "This tutorial is aimed to introduce the user to some key concepts of Generalized Additive Model (GAMs), how these concepts are implemented in this specific PGAM library, and describe in detail how to estimate tuning functions with the PGAM library on an example syntetic dataset.\n",
    "\n",
    "\n",
    "## Why GAMs?\n",
    "<!--\n",
    "Estimating tuning functions entails finding maps that characterize how a set of task variables affects the firing rate of a recorded neuron. Some of the challenges that comes with this estimation problem are: (i) the experimenter has no direct access to the firing rate, but can only measure spikes; (ii) there may be no a priori hypothesis on the shape of the tuning functions (excluding some well characterized special case, e.g. the Gabor filter-like responses in V1); (iii) correlations between task variables may be a confunding factor (e.g. if eye movemet and hand movement are correlated during a reaching movement, it becomes hard to discriminate if the hand, the eye or a combination of both is driving a neuron).  \n",
    "\n",
    "During naturalistic experiments, where there is a lack of identical trial repeats and the behavior is less constrained, those challenges become even more prominent: (i) the firing rate cannot be easily estimated by trial averaging over \"identical\" experimental conditions; (ii) cortical neurons manifest a strong mixed selectivity to a multitude of behavioral covariates and stimuli features that may not be apparent in trial-based experiments due to the simpler, usually multi-alternative stimulus and response space. Mixed selective reponses are not yet fully characterized for most brain areas; (iii) finally, no/weaker control over the animal behavior and sensory experience may introduce additional correlations (e.g. a correlation between eye position and visual stimuli will arise if no eye-fixation is imposed).\n",
    "-->\n",
    "\n",
    "Generalized Linear Models (GLMs) have been succesful in characterizing mixed selective responses by capturing well the statistics of spike trains (which can be modelled as Poisson distributed observations, no averaging needed) and by jointly estimating the contribution of a (potentially) large number of task variables. \n",
    "\n",
    "However, GLMs comes with their own limitations. In particular, one needs to carefully choose how to represent tuning functions (in the case of GLMs this translates into chosing an appropriate basis of functions, e.g. Gaussian-shaped, Fourrier, cosine raised... and the number of basis element to be used). More importantly, defining the minimal subset of variable which drive the neural activity becomes cumbersome for a naive implementations. Variables are usually selected through model comparison, an approach that becomes quickly unfeasible when the number of task variable increases (combinatorial explosion of candidate models). Additionally, classical stepwise methods comes with well known theoretical and practical flaws (e.g., Frank Harrell (2001)).\n",
    "\n",
    "Our solution takles those limitation by taking advantage of GAM theory. GAMs are non-linear extensions of GLMs that retain the advantages mentioned above (model counts directly and jointly infers responses), but additoinally learns from the data the type of non-linearities that are suited to represent each individual response function. As we will see in the tutorial, this will translate into learning the proper prior distribution over a set of possible response functions. The appproach comes with the additional benefit of deriving confidence intervals over the model parameters that can be used to select the minimal subset of variables driving neural acticity. Since selection is based on statistical testing, we completely circumvented costly model comparison. \n",
    "\n",
    "Overall, our approach is both user-friendly, requiring less choices for the user, and computatoinally advantageous when variable selection is required.\n",
    "\n",
    "\n",
    "\n",
    "## What is covered in the tutorial?\n",
    "\n",
    "The tutorial will cover the main components of the model, in particular:\n",
    "\n",
    "1. <a href=\"#repr-nl\">**Representing non-linearities**: </a>\n",
    "\n",
    "    1.1 <a href=\"#b-spline\">**B-spline definition and properties**</a> To familiarize ourselves with the concept of B-splines, we will plot b-splines for different type of response functions: 1D and 2D responses, cyclic or not, and temporal kernels.\n",
    "    \n",
    "    1.2 <a href=\"#sm-prior\">**Smoothing prior**</a> We will introduce the concept of smoothing prior and clarify the role of the smoothing prior in GAM fitting. We will provide intuitive insight into the role of the prior by drawing and plottnig functions sampled from different levels of smoothing. \n",
    "\n",
    "\n",
    "2. <a href=\"#pgam-lib\">**Introduction to the PGAM library**</a> We will illustrate how the concepts introduced in the tutorial are implemented in the PGAM library and applied to the problem of tuning function estimation.\n",
    "\n",
    "    2.1 <a href=\"#sm-handl\">**Define the B-spline via the *smooths_handler* class**</a>: We will use a particular class (smooths_handler) to appropriately format the model covariates, as well as design the B-spline and the corresponding smoothing pealization.\n",
    "\n",
    "    2.2. <a href=\"#model-fit\">**Model fit**</a>: Fit the model by jointly learning the smoothing levels and the B-spline parametters.\n",
    "    \n",
    "    2.3. <a href=\"#post-proc\">**Post processing**</a>: Post-process model outputs, plot and explore the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# import sys\n",
    "# ## if working outside the docker container, uncomment the line below and add the path to [YOUR PATH TO PGAM FOLDER]/src/\n",
    "# ## sys.path.append('[YOUR PATH TO PGAM FOLDER]/src/')\n",
    "# sys.path.append('src/')\n",
    "\n",
    "pgam_path = '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/external/pgam/src/'\n",
    "import sys\n",
    "if not pgam_path in sys.path: \n",
    "    sys.path.append(pgam_path)\n",
    "    \n",
    "import numpy as np\n",
    "import sys\n",
    "from PGAM.GAM_library import *\n",
    "import PGAM.gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 2 Introduction to the PGAM library<a name=\"pgam-lib\">\n",
    "\n",
    "The PGAM library simplifies the process of constructing and fitting GAM models for tuning function estimation. The key classes of the library are:\n",
    "    \n",
    "* **smooths_handler**: constructs B-spline and the penalty matrix for each variable and allows to concatenate multiple B-spline constructing a global model matrix.\n",
    "    \n",
    "* **general_additive_model**: a class that contains methods for fitting GAMs by means of dGCV optimization\n",
    "\n",
    "## 2.1 Define the B-spline via the *smooths_handler* class <a name=\"sm-handl\">\n",
    "\n",
    "\n",
    "The *smooths_handler* class will construct the appropriate B-spline for all the covariates of interest. \n",
    "\n",
    "Each task variable needs to be inputed to the *smooths_handler* class one at the time via the method\n",
    "    \n",
    "        smooths_handler.add_smooth\n",
    "\n",
    "The the method requires the following inputs:\n",
    "* **name**: string, the label of the task variable\n",
    "\n",
    "* **x_cov**: list containing the input variable (the list will contain 1 vector per dimension of the variable)\n",
    "    \n",
    "* **is_temporal_kernel**: boolean, True if the variable is \"temporal\", False if \"spatial\" (see <a href=\"spatial-temporal\">below </a> for the definitioin of temporal and spatial variables)\n",
    "\n",
    "* **kernel_direction**: int or None, None when \"is_temporal_kernel == False\". When \"True\", 0 = acausal (bidirectional), '1' = causal (i.e., firing change after the event happens), '-1' = anticipatory (i.e., firing change before event happens). See the <a href=\"tempcov\"> temporal covariate</a> session for examples.\n",
    "\n",
    "* **kernel_length**: int or None. None when \"is_temporal_kernel == False\". When \"True the number of time points used for the kernel. Suggested to use an odd number of samples.\n",
    "\n",
    "* **ord**: integer, the order of the B-spline\n",
    "\n",
    "* **knots**: list or None. None when \"is_temporal_kernel == True\". If list, each element of the list is a vector of knots locations for a specific dimension of the variable. \n",
    "\n",
    "* **knots_num**: integer or None. If integer, the number of equispaced knots over the x_cov range (for spatial variables) or the temporal kernel range (for temporal variable); knots_num must be smaller then the number of time points that for the filter.\n",
    "\n",
    "* **penalty_type**: 'der' for derivative based penalty matrix, or 'diff' for a difference based penalty matrix. see <a href='der-diff-pen'> above </a>.\n",
    "\n",
    "* **der**: int or None. None if 'diff' penalty is used. The order of the derivative used for the penalizatoin. Default is 2 for a smoother penalty.\n",
    "\n",
    "* **is_cyclic**: list of bool, \"is_cyclic$[i]$ = True\" if the i-th dimension of the task variable is cyclic\n",
    "\n",
    "* **lam**: float, initial smoothing controlling parameter $\\lambda$.\n",
    "\n",
    "* **samp_period**: float, the sampling period in seconds.\n",
    "\n",
    "* **trial_idx**: vector of length the number of samples containing the trial ids of each sample\n",
    "\n",
    "### 2.1.1 Spatial vs. temporal covariates <a name=\"spatial-temporal\">\n",
    "We label the covariates as \"spatial\" or \"temporal\" in order to specify two different type of response functions. In a neuroscience application, a \"spatial\" variable would be a traditional tuning function, where the X-axis defines a range of stimuli, for example position of an animal in an arena, or orientation of gratings. While a \"temporal\" variable would describe response to events, such as stimulus onset.  \n",
    "\n",
    "1. Response to **spatial variable** are instantaneous non-linear effects (the task variable $x_t$ immediately affects the rate in a non-linear way),\n",
    "    \\begin{align}\n",
    "    f(x_t) = \\sum_j\\beta_j b_j(x_t).\n",
    "    \\end{align}\n",
    "\n",
    "2. The response to a **temporal variable** is assumed to be the convolution of a kernel function (described in terms of B-spline) and the variable:\n",
    "\n",
    "    \\begin{align}\n",
    "    f(x_t) &= \\int_{-\\infty}^{\\infty} x(\\tau) h(t-\\tau) d \\tau \\\\\n",
    "    h(t) &= \\sum_j \\beta_j b_j(t)\n",
    "    \\end{align}\n",
    "\n",
    "    where $b_j$ are spline basis elements. This means that past and/or future values of the variable $x_t$ will affect the current firing rate with a linear contribution weighted by $h$.\n",
    "\n",
    "\n",
    "\n",
    "<!--Fitting a PGAM will entail learning the appropriate $\\mathbf{\\beta}$ coefficients characterizing the response function.\n",
    "\n",
    "Below we will create an example of three syntetic covariates (an event indicator, a 1D continous variable and a 2D continuous variable) for an experiment with 2 trials of 500 time points per trial.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### 2.1.2 Temporal covariates <a name=\"tempcov\">\n",
    "\n",
    "Below we define and plot an acausal and the two directional filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_ev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a series of event marker\n",
    "tot_tp = 10**3\n",
    "\n",
    "# set some trial ids\n",
    "trial_ids = np.zeros(tot_tp)\n",
    "trial_ids[400:] = 1\n",
    "\n",
    "# sample some event marker at random\n",
    "event = np.zeros(tot_tp)\n",
    "event[[100, 200, 600, 900]] = 1\n",
    "\n",
    "# define the b-spline params\n",
    "kernel_h_legnth = 121 # duration of the kernel h(t) in time points \n",
    "num_int_knots = 12 # number of internal knots used to represent h\n",
    "order = 4\n",
    "dict_kernel = {0:'Acausal',1:'Direction %d'%1, -1:'Direction %d'%(-1)}\n",
    "\n",
    "\n",
    "for kernel_direction in [0,1,-1]:\n",
    "    # define the \"smooths_handler\" container\n",
    "    sm_handler = gdh.smooths_handler()\n",
    "    \n",
    "    # add the covariate & evaluate the convolution\n",
    "    sm_handler.add_smooth('this_event', \n",
    "                          [event], \n",
    "                          is_temporal_kernel=True, \n",
    "                          ord=order, \n",
    "                          knots_num=num_int_knots,\n",
    "                          trial_idx=trial_ids,\n",
    "                          kernel_length=kernel_h_legnth,\n",
    "                          kernel_direction=kernel_direction)\n",
    "\n",
    "    # sm_handler['varname'] process and stores the B-spline for the variable\n",
    "    # below we retrive the B-spline convolved with the \"event\" variable\n",
    "    convolved_ev = sm_handler['this_event'].X.toarray()\n",
    "    \n",
    "    # retrive the B-spline used for the convolution\n",
    "    basis = sm_handler['this_event'].basis_kernel.toarray()\n",
    "\n",
    "    # plot the basis & the convolved events\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.suptitle('%s Filter'%dict_kernel[kernel_direction])\n",
    "    \n",
    "    # basis for the kenel h\n",
    "    plt.subplot(121)\n",
    "    plt.title('kernel basis')\n",
    "    tps = np.repeat(np.arange(kernel_h_legnth)-kernel_h_legnth//2, basis.shape[1]).reshape(basis.shape)\n",
    "    plt.plot(tps, basis)\n",
    "    plt.xlabel('time points')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title('convolved events')\n",
    "\n",
    "    # select a time point interval containing an event\n",
    "    idx0, idx1 = np.where(event == 1)[0][2] - 100, np.where(event == 1)[0][2] + 400\n",
    "\n",
    "    # extract the events convolved with each of the B-spline elements\n",
    "    conv = convolved_ev[idx0:idx1,:]\n",
    "\n",
    "    tps = np.arange(0,idx1-idx0) - 100\n",
    "    tps = np.repeat(tps,conv.shape[1]).reshape(conv.shape)\n",
    "    plt.plot(tps, conv)\n",
    "    plt.vlines(tps[0,0] + np.where(event[idx0:idx1])[0],0,1.5,'k',ls='--',label='event')\n",
    "    plt.xlabel('time points')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2.1.3 Spatial variable 1D and 2D\n",
    "\n",
    "Example \"spatial\" variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate three covariate\n",
    "x = np.random.normal(size=tot_tp)\n",
    "y = np.random.normal(size=tot_tp)\n",
    "z = np.random.normal(size=tot_tp)\n",
    "\n",
    "# add the 1d spatial variable\n",
    "int_knots = np.linspace(-2,2,10)\n",
    "order = 4\n",
    "knots = np.hstack(([int_knots[0]]*(order-1), int_knots, [int_knots[-1]]*(order-1)))\n",
    "\n",
    "# remove out of range values\n",
    "x[np.abs(x)>2] = np.nan\n",
    "y[np.abs(y)>2] = np.nan\n",
    "z[np.abs(z)>2] = np.nan\n",
    "\n",
    "# add the variable\n",
    "if 'spatial_1D' in sm_handler.smooths_var:\n",
    "    sm_handler.smooths_var.remove('spatial_1D')\n",
    "    sm_handler.smooths_dict.pop('spatial_1D')\n",
    "    \n",
    "sm_handler.add_smooth('spatial_1D', [x], \n",
    "                      knots=[knots], \n",
    "                      ord=order, \n",
    "                      is_temporal_kernel=False,\n",
    "                      trial_idx=trial_ids, \n",
    "                      is_cyclic=[False])\n",
    "\n",
    "\n",
    "# retrive the b-spline evaluated at x.\n",
    "X_1D = sm_handler['spatial_1D'].X.toarray()\n",
    "\n",
    "\n",
    "# sort for plotting\n",
    "plt.figure()\n",
    "plt.title('1D spatial response')\n",
    "idx_srt = np.argsort(x)\n",
    "X_srt = X_1D[idx_srt]\n",
    "p = plt.plot(X_srt)\n",
    "\n",
    "# add a 2D response with one cyclic variable and one acyclic\n",
    "if 'spatial_2D' in sm_handler.smooths_var:\n",
    "    sm_handler.smooths_var.remove('spatial_2D')\n",
    "    sm_handler.smooths_dict.pop('spatial_2D')\n",
    "\n",
    "sm_handler.add_smooth('spatial_2D', [y,z], knots=[knots, knots], ord=order, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False,True])\n",
    "X_2D = sm_handler['spatial_2D'].X.toarray()\n",
    "\n",
    "\n",
    "# the size of basis set grows as n^m where n is the basis in the 1D case, and m is the number of dimensions\n",
    "print('Size of X_1D',X_1D.shape)\n",
    "print('Size of X_2D',X_2D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.2 Model fit <a name=\"model-fit\">\n",
    "Putting all the pieces together, here we will create a *smooths_handler* object containing the appropriate covariates, fit the model, evaluate the fit quality, and save the post-processed outputs in a standard numpy structured array or as a MATLAB structure.\n",
    "    \n",
    "The class **general_additive_model** is used for defining the GAM. It requires the following inputs:\n",
    "    \n",
    "* **sm_handler**: the smooths_handler object\n",
    "* **var_list**: list of variable names\n",
    "* **y**: the array with the spike counts (all trials must be stacked in a 1D array)\n",
    "* **link**: statsmoldels.genmod.families.links.link class which describe the link function (the library allows to fit any exponential family observation noise)\n",
    "\n",
    "\n",
    "You can fit the GAM with the method **general_additive_model.fit_full_and_reduced**, which fits a model with all the variables in **var_list**, then selects a minimal subset of variables that drive the neural activity by statistical testing, and re-fits the model with the significant variables only.\n",
    "    \n",
    "The inputs for **fit_full_and_reduced** requires are the following:\n",
    "\n",
    "* **var_list**: list with the subset of variables to be used for model fitting \n",
    "* **th_pval**: float between 0 and 1,the significance level for task variable inclusion\n",
    "* **max_iter**: int, max number of iteration of the optization routine\n",
    "* **use_dgcv**: True for learning the smoothing constants via dgcv\n",
    "* **trial_idx**: vector of length the number of samples containing the trial ids of each sample\n",
    "* **filter_trials**: vector of boolean, of the same length of *trial_idx*, indicates which time points should be used for training the model\n",
    "\n",
    "    \n",
    "In the following subsection we will provide an example where spike counts are generated according to the PGAM assumptions, and estimate the response function within the GAM framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 2.2.1 Generate synthetic data\n",
    "Below we generate a syntetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## inputs parameters\n",
    "num_events = 6000\n",
    "time_points = 3 * 10 ** 5  # 30 mins at 0.006 ms resolution\n",
    "rate = 5. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # spatial input and nuisance variance\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(200),time_points//200)\n",
    "\n",
    "## create temporal input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov= variance * np.eye(2))\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var = samp[:, 0]\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate X to avoid jumps in the resp function\n",
    "sele_idx = np.abs(spatial_var) < 5\n",
    "spatial_var = spatial_var[sele_idx]\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while spatial_var.shape[0] < time_points:\n",
    "    tmpX = rv.rvs(10 ** 4)\n",
    "    sele_idx = np.abs(tmpX[:, 0]) < 5\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "\n",
    "    spatial_var = np.hstack((spatial_var, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "spatial_var = spatial_var[:time_points]\n",
    "nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a resp function\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,8),[5]*3))\n",
    "beta = np.arange(10)\n",
    "beta = beta / np.linalg.norm(beta)\n",
    "beta = np.hstack((beta[5:], beta[:5][::-1]))\n",
    "resp_func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=0),beta)\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0,20,100),a=2) - sts.gamma.pdf(np.linspace(0,20,100),a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101),filter_used_conv))*2\n",
    "# mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr], filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "const = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - const\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the firing rate and the spike counts generated\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1000) * 0.006, np.exp(log_mu0)[:1000]/0.006)\n",
    "plt.title('firing rate [Hz]', fontsize=16)\n",
    "plt.xlabel('time[sec]', fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(1000) * 0.006, spk_counts[:1000])\n",
    "plt.title('6ms binned spike counts', fontsize=16)\n",
    "plt.xlabel('time[sec]', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### 2.2.2 Create the *smooths_handler* object and fit the model\n",
    "Below we create the smooths_handler object and run a fit. We include a \"nuisance\" spatial variable, that is not driving the neuron, the fit will learn to discard the variable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Creating the class\n",
    "sm_handler = smooths_handler()\n",
    "# Creating the knots (notice the over-representation of edge knots)\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,15),[5]*3))\n",
    "# Using smooths_handler class to add variables \n",
    "sm_handler.add_smooth('spatial', [spatial_var], knots=[knots], ord=4, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2)\n",
    "\n",
    "sm_handler.add_smooth('nuisance', [nuisance_var], knots=[knots], ord=4, is_temporal_kernel=False,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2)\n",
    "\n",
    "sm_handler.add_smooth('temporal', [events], knots=None, ord=4, is_temporal_kernel=True,\n",
    "                     trial_idx=trial_ids, is_cyclic=[False],penalty_type='der', der=2,\n",
    "                     knots_num=10, kernel_length=500, kernel_direction=1)\n",
    "\n",
    "\n",
    "# split trial in train and eval\n",
    "train_trials = trial_ids % 10 != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "\n",
    "link = sm.genmod.families.links.log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "# create the pgam model\n",
    "pgam = general_additive_model(sm_handler,\n",
    "                              sm_handler.smooths_var, # list of covariate we want to include in the model\n",
    "                              spk_counts, # vector of spike counts\n",
    "                              poissFam # poisson family with exponential link from statsmodels.api\n",
    "                             )\n",
    "\n",
    "# with with all covariate, remove according to stat testing, and then refit\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var, \n",
    "                                          th_pval=0.001,# pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2, # max number of iteration\n",
    "                                          use_dgcv=True, # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials)\n",
    "\n",
    "print('Minimal subset of variables driving the activity:')\n",
    "print(reduced.var_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2.3 Post processing<a name=\"post-proc\">\n",
    "After a fit, it is possible to post-process the model fit output to obtain an easy to parse result in the form of a numpy.structarray. \n",
    "\n",
    "Each row will represent results for a specific input variable, additional information about the neuron (e.g. channel ID, electrode ID, or anything else) can be provided in the form of a dictionary, each dictionary value will be stored in the structured array with type \"object\".\n",
    "\n",
    "The output structure can be saved either as a \".npy\" via *numpy.save(\\<filename\\>)* or as a .mat (for MATLAB) via *scipy.io.savemat(\\<filename*\\>)*.\n",
    "\n",
    "Below is an example of the post-processing applied to the fit just obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string with the neuron identifier\n",
    "neuron_id = 'neuron_000_session_1_monkey_001'\n",
    "# dictionary containing some information about the neuron, keys must be strings and values can be anything\n",
    "# since are stored with type object.\n",
    "info_save = {'x':100,\n",
    "             'y':801.2,\n",
    "             'z':301,\n",
    "             'brain_region': 'V1',\n",
    "             'subject':'monkey_001'\n",
    "            }\n",
    "\n",
    "# assume that we used 90% of the trials for training, 10% for evaluation\n",
    "res = postprocess_results(neuron_id, spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids, var_zscore_par=None,info_save=info_save,bins=100)\n",
    "\n",
    "# each row of res contains the info about a variable\n",
    "# some info are shared for all the variables (p-rsquared for example is a goodness of fit measure for the model\n",
    "# it is shared, not a property of the variable), while other, like the parameters of the b-splines, \n",
    "# are variable specific\n",
    "print('\\n\\n')\n",
    "print('Result structarray types\\n========================\\n')\n",
    "for name in res.dtype.names:\n",
    "    print('%s: \\t %s'%(name, type(res[name][0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot tuning functions\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for k in range(3):\n",
    "    plt.subplot(2,3,k+1)\n",
    "    plt.title('log-space %s'%res['variable'][k])\n",
    "    x_kernel = res['x_kernel'][k]\n",
    "    \n",
    "    # changed from the original tutorial\n",
    "    x_kernel = x_kernel.reshape(-1)  # reshape for plotting\n",
    "    \n",
    "    \n",
    "    y_kernel = res['y_kernel'][k]\n",
    "    ypCI_kernel = res['y_kernel_pCI'][k]\n",
    "    ymCI_kernel = res['y_kernel_mCI'][k]\n",
    "    \n",
    "    plt.plot(x_kernel, y_kernel, color='r')\n",
    "    plt.fill_between(x_kernel, ymCI_kernel, ypCI_kernel, color='r', alpha=0.3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_firing = res['x_rate_Hz'][k]\n",
    "    y_firing_model = res['model_rate_Hz'][k]\n",
    "    y_firing_raw = res['raw_rate_Hz'][k]\n",
    "    \n",
    "    plt.subplot(2,3,k+4)\n",
    "    plt.title('rate-space %s'%res['variable'][k])\n",
    "    \n",
    "    plt.plot(x_firing, y_firing_raw, color='k',label='raw')\n",
    "    plt.plot(x_firing, y_firing_model, color='r',label='model')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the output for further analysis\n",
    "#np.save('/notebooks/result_pgam.npy', res)\n",
    "#savemat('/notebooks/result_pgam.mat', mdict = {'result_pgam':res})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# References <a name=\"refs\"></a>\n",
    "<a id=\"1\">[1]</a> \n",
    "<a href=\"https://proceedings.neurips.cc/paper/2020/hash/94d2a3c6dd19337f2511cdf8b4bf907e-Abstract.html\">\n",
    "Balzani, Edoardo , et al., \n",
    "\"Efficient estimation of neural tuning during naturalistic behavior.\"\n",
    "Advances in Neural Information Processing Systems 33 (2020): 12604-12614.<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bibtex_bibfiles": "references.bib",
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "pgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "references.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
