{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "import json\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat('all_monkey_data/one_ff_data/sessions_python.mat',\n",
    "               squeeze_me=True,\n",
    "               struct_as_record=False)\n",
    "\n",
    "sessions = data['sessions_out']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_num = 30\n",
    "trial_data = sessions[0].behaviour_trials[trial_num].continuous\n",
    "print(trial_data._fieldnames)\n",
    "\n",
    "x = trial_data.xmp\n",
    "y = trial_data.ymp\n",
    "v = trial_data.v\n",
    "w = trial_data.w\n",
    "t = trial_data.ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_events = sessions[0].behaviour_trials[trial_num].events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_events.t_flyON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.average(np.diff(t))\n",
    "dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, y, 'k-')\n",
    "plt.xlabel('x (forward)')\n",
    "plt.ylabel('y (lateral)')\n",
    "plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unit0's trial_0 spike times\n",
    "unit0 = sessions[0].units[0]\n",
    "trial0 = unit0.trials[0]\n",
    "trial0._fieldnames\n",
    "trial0.tspk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# concat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sessions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = session.behaviour_trials\n",
    "units = session.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prs.dt\n",
    "full_time_window(tr)\n",
    "bin_spikes(...)\n",
    "concatenate_trials(...)\n",
    "trial_ids = np.arange(n_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_trials_with_trial_id(trials, trial_ids, signal_fn, time_window_fn):\n",
    "    X = []\n",
    "    trial_id_vec = []\n",
    "\n",
    "    for tid in trial_ids:\n",
    "        tr = trials[tid]\n",
    "        mask = time_window_fn(tr)\n",
    "\n",
    "        sig = signal_fn(tr, tid)[mask]\n",
    "        X.append(sig)\n",
    "\n",
    "        trial_id_vec.append(np.full(len(sig), tid, dtype=int))\n",
    "\n",
    "    return np.concatenate(X), np.concatenate(trial_id_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs.posttrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_concat, trial_id_vec = concatenate_trials_with_trial_id(\n",
    "    trials,\n",
    "    trial_ids,\n",
    "    lambda tr, tid: tr.continuous.v,\n",
    "    full_time_window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0  # neuron to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_counts, trial_id_vec_spk = concatenate_trials_with_trial_id(\n",
    "    trials,\n",
    "    trial_ids,\n",
    "    lambda tr, tid: bin_spikes(\n",
    "        units[k].trials[tid].tspk,\n",
    "        tr.continuous.ts\n",
    "    ),\n",
    "    full_time_window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_id_vec_spk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_impulse(tr, tid, event_name):\n",
    "    ts = tr.continuous.ts\n",
    "    evt_t = getattr(tr.events, event_name)\n",
    "\n",
    "    ev = np.zeros_like(ts)\n",
    "    idx = np.searchsorted(ts, evt_t)\n",
    "    if 0 <= idx < len(ts):\n",
    "        ev[idx] = 1.0\n",
    "    return ev\n",
    "\n",
    "events_concat, trial_id_vec_evt = concatenate_trials_with_trial_id(\n",
    "    trials,\n",
    "    trial_ids,\n",
    "    lambda tr, tid: event_impulse(tr, tid, 't_targ'),\n",
    "    full_time_window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for events in ['t_targ', 't_move', 't_rew']:\n",
    "    events_concat, trial_id_vec_evt = concatenate_trials_with_trial_id(\n",
    "        trials,\n",
    "        trial_ids,\n",
    "        lambda tr, tid: event_impulse(tr, tid, 't_targ'),\n",
    "        full_time_window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Replicate (chatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "\n",
    "data = loadmat(\n",
    "    'all_monkey_data/one_ff_data/sessions_python.mat',\n",
    "    squeeze_me=True,\n",
    "    struct_as_record=False\n",
    ")\n",
    "\n",
    "sessions = data['sessions_out']\n",
    "session = sessions[0]\n",
    "\n",
    "trials = session.behaviour_trials\n",
    "units = session.units\n",
    "\n",
    "n_trials = len(trials)\n",
    "n_units = len(units)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build trial index (all trials)\n",
    "# =========================\n",
    "\n",
    "trial_ids = np.arange(n_trials)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Population analysis pipeline\n",
    "Python replication of AnalysePopulation.m (core functionality)\n",
    "\n",
    "Author: you + ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal.windows import gaussian\n",
    "from scipy.linalg import lstsq\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Parameters (prs struct)\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    dt: float = 0.01\n",
    "    neural_filtwidth: int = 5\n",
    "    pretrial: float = 0\n",
    "    posttrial: float = 0\n",
    "    cca_vars: List[str] = None\n",
    "    decode_vars: List[str] = None\n",
    "\n",
    "\n",
    "prs = Params(\n",
    "    dt=0.01,\n",
    "    neural_filtwidth=5,\n",
    "    cca_vars=['v', 'w'],\n",
    "    decode_vars=['v', 'w']\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "\n",
    "def gaussian_kernel(width):\n",
    "    t = np.arange(-2 * width, 2 * width + 1)\n",
    "    h = np.exp(-t ** 2 / (2 * width ** 2))\n",
    "    return h / h.sum()\n",
    "\n",
    "\n",
    "def smooth_signal(x, width):\n",
    "    if width <= 0:\n",
    "        return x\n",
    "    h = gaussian_kernel(width)\n",
    "    return np.apply_along_axis(lambda m: np.convolve(m, h, mode='same'), 0, x)\n",
    "\n",
    "\n",
    "def bin_spikes(spike_times, ts):\n",
    "    counts = np.zeros(len(ts))\n",
    "    idx = np.searchsorted(ts, spike_times)\n",
    "    idx = idx[(idx >= 0) & (idx < len(ts))]\n",
    "    np.add.at(counts, idx, 1)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def concatenate_trials(trials, trial_ids, signal_fn, time_window_fn):\n",
    "    X = []\n",
    "    trial_lengths = []\n",
    "\n",
    "    for tid in trial_ids:\n",
    "        tr = trials[tid]\n",
    "        mask = time_window_fn(tr)\n",
    "        sig = signal_fn(tr, tid)[mask]   # <-- pass tid\n",
    "        X.append(sig)\n",
    "        trial_lengths.append(len(sig))\n",
    "\n",
    "    return np.concatenate(X), trial_lengths\n",
    "\n",
    "\n",
    "def deconcatenate(x, trial_lengths):\n",
    "    out = []\n",
    "    idx = 0\n",
    "    for L in trial_lengths:\n",
    "        out.append(x[idx:idx+L])\n",
    "        idx += L\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_d(v, ts, dt):\n",
    "    d = np.zeros_like(v)\n",
    "    valid = ts > 0\n",
    "    d[valid] = np.cumsum(v[valid]) * dt\n",
    "    return d\n",
    "\n",
    "\n",
    "def compute_phi(w, ts, dt):\n",
    "    phi = np.zeros_like(w)\n",
    "    valid = ts > 0\n",
    "    phi[valid] = np.cumsum(w[valid]) * dt\n",
    "    return phi\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Time window helper\n",
    "# =========================\n",
    "\n",
    "def full_time_window(tr):\n",
    "    t0 = min(tr.events.t_move, tr.events.t_targ) - prs.pretrial\n",
    "    t1 = tr.events.t_end + prs.posttrial\n",
    "    return (tr.continuous.ts >= t0) & (tr.continuous.ts <= t1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build stimulus matrix X\n",
    "# =========================\n",
    "\n",
    "def get_var(tr, name):\n",
    "    if name == 'v':\n",
    "        return tr.continuous.v\n",
    "    if name == 'w':\n",
    "        return tr.continuous.w\n",
    "    if name == 'd':\n",
    "        return compute_d(tr.continuous.v, tr.continuous.ts, prs.dt)\n",
    "    if name == 'phi':\n",
    "        return compute_phi(tr.continuous.w, tr.continuous.ts, prs.dt)\n",
    "    raise ValueError(name)\n",
    "\n",
    "\n",
    "def gen_traj(w, v, ts):\n",
    "    x = np.zeros(len(ts))\n",
    "    y = np.zeros(len(ts))\n",
    "    for i in range(1, len(ts)):\n",
    "        x[i] = x[i-1] + v[i] * np.cos(w[i]) * prs.dt\n",
    "        y[i] = y[i-1] + v[i] * np.sin(w[i]) * prs.dt\n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "trial_lengths = None\n",
    "total_len_ref = None\n",
    "\n",
    "for var in prs.cca_vars:\n",
    "    x, tl = concatenate_trials(\n",
    "        trials,\n",
    "        trial_ids,\n",
    "        lambda tr, tid, v=var: get_var(tr, v),\n",
    "        full_time_window\n",
    "    )\n",
    "\n",
    "    if trial_lengths is None:\n",
    "        trial_lengths = tl\n",
    "        total_len_ref = x.shape[0]\n",
    "    else:\n",
    "        if x.shape[0] != total_len_ref:\n",
    "            raise RuntimeError(f'Concatenated length mismatch for var {var}: {x.shape[0]} vs {total_len_ref}')\n",
    "\n",
    "    X_list.append(x)\n",
    "\n",
    "X = np.column_stack(X_list)\n",
    "X[np.isnan(X)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # =========================\n",
    "# # Build population activity Y\n",
    "# # =========================\n",
    "\n",
    "# Y = np.zeros((X.shape[0], n_units))\n",
    "\n",
    "# for k in range(n_units):\n",
    "#     yk, _ = concatenate_trials(\n",
    "#         trials,\n",
    "#         trial_ids,\n",
    "#         lambda tr, tid, k=k: bin_spikes(\n",
    "#             units[k].trials[tid].tspk,\n",
    "#             tr.continuous.ts\n",
    "#         ),\n",
    "#         full_time_window\n",
    "#     )\n",
    "#     Y[:, k] = yk\n",
    "\n",
    "# Y_smooth = smooth_signal(Y, prs.neural_filtwidth) / prs.dt\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Canonical Correlation Analysis\n",
    "# # =========================\n",
    "\n",
    "# cca = CCA(n_components=min(X.shape[1], Y_smooth.shape[1]))\n",
    "# Xc, Yc = cca.fit_transform(X, Y_smooth)\n",
    "\n",
    "# cca_corrs = [np.corrcoef(Xc[:, i], Yc[:, i])[0, 1] for i in range(Xc.shape[1])]\n",
    "\n",
    "# print('CCA correlations:', cca_corrs)\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Linear population decoding\n",
    "# # =========================\n",
    "\n",
    "# decode_results = {}\n",
    "\n",
    "# for var in prs.decode_vars:\n",
    "#     xt, _ = concatenate_trials(\n",
    "#         trials,\n",
    "#         trial_ids,\n",
    "#         lambda tr, tid, v=var: get_var(tr, v),\n",
    "#         full_time_window\n",
    "#     )\n",
    "#     xt[np.isnan(xt)] = 0\n",
    "\n",
    "#     Yd = smooth_signal(Y, prs.neural_filtwidth)\n",
    "\n",
    "#     wts, _, _, _ = lstsq(Yd, xt)\n",
    "#     pred = Yd @ wts\n",
    "\n",
    "#     corr = np.corrcoef(xt, pred)[0, 1]\n",
    "\n",
    "#     decode_results[var] = dict(\n",
    "#         weights=wts,\n",
    "#         corr=corr,\n",
    "#         true=deconcatenate(xt, trial_lengths),\n",
    "#         pred=deconcatenate(pred, trial_lengths)\n",
    "#     )\n",
    "\n",
    "#     print(f'Decode {var}: r = {corr:.3f}')\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Trajectory reconstruction\n",
    "# # =========================\n",
    "\n",
    "# traj_pred = []\n",
    "\n",
    "# for tr_v, tr_w in zip(decode_results['v']['pred'], decode_results['w']['pred']):\n",
    "#     x, y = gen_traj(tr_w, tr_v, np.arange(len(tr_v)) * prs.dt)\n",
    "#     traj_pred.append((x, y))\n",
    "\n",
    "\n",
    "# print('Pipeline complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# PGAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# import sys\n",
    "# ## if working outside the docker container, uncomment the line below and add the path to [YOUR PATH TO PGAM FOLDER]/src/\n",
    "# ## sys.path.append('[YOUR PATH TO PGAM FOLDER]/src/')\n",
    "# sys.path.append('src/')\n",
    "\n",
    "pgam_path = '/Users/dusiyi/Documents/Multifirefly-Project/multiff_analysis/external/pgam/src/'\n",
    "import sys\n",
    "if not pgam_path in sys.path: \n",
    "    sys.path.append(pgam_path)\n",
    "    \n",
    "import numpy as np\n",
    "import sys\n",
    "from PGAM.GAM_library import *\n",
    "import PGAM.gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## temporal covariance\n",
    "\n",
    "Temporal filters g were parameterized using a basis of ten raised cosine filters spanning a range of 600 milliseconds. The filter associated with target-onset was causal ([0, 600] ms), while the remaining filters were non-causal ([-300, 300] ms). Both spike-history filter h and coupling filter p were expressed using a basis of ten causal raised cosine filters in logarithmic time scale. Spike-history filters spanned 350 ms, while coupling filters spanned 1.375 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "knots_num = 10 - order\n",
    "dt_ms = 1.0          # e.g. 1 ms bins (1 kHz sampling)\n",
    "kernel_ms = 600\n",
    "kernel_h_length = int(kernel_ms / dt_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_tp = 10**3\n",
    "\n",
    "# # trial ids\n",
    "# trial_ids = np.zeros(tot_tp)\n",
    "# trial_ids[400:] = 1\n",
    "\n",
    "# # event markers\n",
    "# event = np.zeros(tot_tp)\n",
    "# event[[100, 200, 600, 900]] = 1\n",
    "\n",
    "# kernel parameters\n",
    "dt_ms = 1.0                 # ms per time bin\n",
    "kernel_ms = 600             # total temporal span\n",
    "kernel_h_length = int(kernel_ms / dt_ms)\n",
    "\n",
    "order = 4                   # cubic B-splines\n",
    "num_filters = 10\n",
    "num_int_knots = num_filters - order\n",
    "\n",
    "dict_kernel = {\n",
    "    0: 'Acausal',\n",
    "    1: 'Direction %d' % 1,\n",
    "    -1: 'Direction %d' % (-1)\n",
    "}\n",
    "\n",
    "for kernel_direction in [0, 1, -1]:\n",
    "    sm_handler = gdh.smooths_handler()\n",
    "\n",
    "    sm_handler.add_smooth(\n",
    "        'this_event',\n",
    "        [event],\n",
    "        is_temporal_kernel=True,\n",
    "        ord=order,\n",
    "        knots_num=num_int_knots,\n",
    "        trial_idx=trial_ids,\n",
    "        kernel_length=kernel_h_length,\n",
    "        kernel_direction=kernel_direction\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
