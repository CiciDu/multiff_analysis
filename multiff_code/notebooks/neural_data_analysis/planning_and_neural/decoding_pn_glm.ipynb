{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, ml_methods_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils\n",
    "from neural_data_analysis.design_kits.design_by_segment import create_design_df, predictor_utils, other_feats\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event, pn_glm_utils\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.tpg import glm_bases, glm_plotting, glm_plotting2, glm_fit\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, get_stops_utils, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_fit import stop_glm_fit, cv_stop_glm, glm_fit_utils, variance_explained\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_plotting import plot_spikes, plot_glm_fit, plot_tuning_func\n",
    "from neural_data_analysis.design_kits.design_around_event import event_binning, stop_design, cluster_design, design_checks\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_hyperparams import compare_glm_configs, glm_hyperparams_class\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_decoding_tools import glm_decoding_llr, glm_decoding\n",
    "from planning_analysis.show_planning.cur_vs_nxt_ff import cvn_from_ref_class\n",
    "from planning_analysis.plan_factors import build_factor_comp\n",
    "\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "from numpy import pi\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'time_since_target_last_seen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.dec.behav_data_by_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.planning_data_by_bin.loc[pn.planning_data_by_bin['time_since_target_last_seen'].isna(), ['time', 'stop_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.planning_data_by_bin.loc[pn.planning_data_by_bin['time_since_target_last_seen'].isna(), 'cur_in_memory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.planning_data_by_bin['cur_in_memory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.planning_data_by_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0222\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvn = cvn_from_ref_class.CurVsNxtFfFromRefClass(raw_data_folder_path=raw_data_folder_path)\n",
    "# Quick method - tries to retrieve first, creates if needed\n",
    "cvn.make_heading_info_df_without_long_process(\n",
    "    test_or_control='test',  # or 'control'\n",
    "    ref_point_mode='distance',  # or 'time after cur ff visible'\n",
    "    ref_point_value=-100,  # or 0.0 for time mode\n",
    "    heading_info_df_exists_ok=True,  # Set to False to force recreation\n",
    "    stops_near_ff_df_exists_ok=True,\n",
    "    save_data=True\n",
    ")\n",
    "\n",
    "# Access the result\n",
    "heading_info_df = cvn.heading_info_df\n",
    "heading_df = heading_info_df[['cur_ff_index', 'diff_in_abs_angle_to_nxt_ff']].copy()\n",
    "heading_df = heading_df.sort_values(by='diff_in_abs_angle_to_nxt_ff', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# based on same side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(pn.planning_data_by_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(pn.planning_data_by_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_y_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_y_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_y_var['time_since_target_last_seen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import combine_info_utils, specific_utils\n",
    "\n",
    "# Get all sessions for a specific monkey\n",
    "monkey_name = \"monkey_Bruno\"  # or \"monkey_Schro\"\n",
    "sessions_df = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name='all_monkey_data/raw_monkey_data',\n",
    "    monkey_name=monkey_name\n",
    ")\n",
    "\n",
    "# Iterate through each session\n",
    "for index, row in sessions_df.iterrows():\n",
    "    if row['finished']:\n",
    "        continue  # Skip already processed sessions\n",
    "    \n",
    "    # Construct the raw_data_folder_path\n",
    "    raw_data_folder_path = f\"all_monkey_data/raw_monkey_data/{row['monkey_name']}/{row['data_name']}\"\n",
    "    \n",
    "    print(f\"Processing: {raw_data_folder_path}\")\n",
    "    \n",
    "\n",
    "    pn = glm_decoding.init_decoding_data(raw_data_folder_path)\n",
    "\n",
    "    heading_info_df, heading_df = pn_glm_utils.get_test_heading_df(raw_data_folder_path)\n",
    "\n",
    "    build_factor_comp.add_dir_from_cur_ff_same_side(heading_info_df)\n",
    "    heading_info_df['dir_from_cur_ff_same_side'].mean()\n",
    "\n",
    "\n",
    "    for same_side in [True, False]:\n",
    "        print('-'*100)\n",
    "        print('-'*100)\n",
    "        if same_side:\n",
    "            str = \"=========Same Side=========\"\n",
    "        else:\n",
    "            str = \"=========Opposite Side=========\"\n",
    "            \n",
    "        rebinned_x_var, rebinned_y_var = pn_glm_utils.select_ff_subset_by_dir_from_cur_ff_same_side(heading_info_df, pn.rebinned_x_var, pn.rebinned_y_var,\n",
    "                                                                                                    same_side=same_side)\n",
    "        \n",
    "        rebinned_x_var = pn_glm_utils.drop_constant_columns(rebinned_x_var)\n",
    "        data = rebinned_y_var.copy()\n",
    "\n",
    "\n",
    "        df_X, df_Y = glm_decoding.get_data_for_decoding_vis(rebinned_x_var, rebinned_y_var, pn.bin_width)\n",
    "\n",
    "        exposure = np.ones(len(df_Y)) * pn.bin_width\n",
    "        offset_log = np.log(exposure)\n",
    "\n",
    "        report = stop_glm_fit.glm_mini_report(\n",
    "            df_X=df_X, df_Y=df_Y, offset_log=offset_log,\n",
    "            cov_type='HC1',            # or 'nonrobust' for even faster\n",
    "            fast_mle=True,             # << use the ultra-fast path\n",
    "            do_inference=False,        # skip FDR/ratios/pop-tests\n",
    "            make_plots=False,          # skip figure creation\n",
    "            show_plots=True,          # nothing to display\n",
    "        )\n",
    "        \n",
    "        #cols_to_decode = ['nxt_vis', 'random_0_or_1', 'cur_vis']\n",
    "        cols_to_decode = ['nxt_vis']\n",
    "        groups = np.array(data['new_segment'])\n",
    "\n",
    "        # # Decoding from fit\n",
    "        # print(f\"{str}\")\n",
    "        # glm_decoding.glm_decoding_from_fit(cols_to_decode, df_X, df_Y, offset_log, report)\n",
    "\n",
    "        # CV\n",
    "        print(f\"{str}\")\n",
    "        glm_decoding.glm_decoding_cv(cols_to_decode, df_X, df_Y, groups, offset_log)\n",
    "\n",
    "        # # permutations\n",
    "        # print(f\"{str}\")\n",
    "        # glm_decoding.glm_decoding_permutation_test(cols_to_decode, df_X, df_Y,\n",
    "        #                         groups, offset_log, report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top vs bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import combine_info_utils, specific_utils\n",
    "\n",
    "# Get all sessions for a specific monkey\n",
    "monkey_name = \"monkey_Bruno\"  # or \"monkey_Schro\"\n",
    "sessions_df = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name='all_monkey_data/raw_monkey_data',\n",
    "    monkey_name=monkey_name\n",
    ")\n",
    "\n",
    "# Iterate through each session\n",
    "for index, row in sessions_df.iterrows():\n",
    "    if row['finished']:\n",
    "        continue  # Skip already processed sessions\n",
    "    \n",
    "    # Construct the raw_data_folder_path\n",
    "    raw_data_folder_path = f\"all_monkey_data/raw_monkey_data/{row['monkey_name']}/{row['data_name']}\"\n",
    "    \n",
    "    print(f\"Processing: {raw_data_folder_path}\")\n",
    "    \n",
    "\n",
    "    pn = glm_decoding.init_decoding_data(raw_data_folder_path)\n",
    "\n",
    "    heading_info_df, heading_df = pn_glm_utils.get_test_heading_df(raw_data_folder_path)\n",
    "\n",
    "    build_factor_comp.add_dir_from_cur_ff_same_side(heading_info_df)\n",
    "    heading_info_df['dir_from_cur_ff_same_side'].mean()\n",
    "\n",
    "\n",
    "    for top in [True, False]:\n",
    "        print('-'*100)\n",
    "        print('-'*100)\n",
    "        if top:\n",
    "            str = \"=========TOP TOP TOP TOP TOP=========\"\n",
    "        else:\n",
    "            str = \"=========BOTTOM BOTTOM BOTTOM BOTTOM BOTTOM=========\"\n",
    "        rebinned_x_var, rebinned_y_var = pn_glm_utils.select_ff_subset(heading_df, pn.rebinned_x_var, pn.rebinned_y_var, \n",
    "                                                                    top=False, pct=0.5)\n",
    "\n",
    "        rebinned_x_var = pn_glm_utils.drop_constant_columns(rebinned_x_var)\n",
    "        data = rebinned_y_var.copy()\n",
    "\n",
    "\n",
    "        df_X, df_Y = glm_decoding.get_data_for_decoding_vis(rebinned_x_var, rebinned_y_var, pn.bin_width)\n",
    "\n",
    "        exposure = np.ones(len(df_Y)) * pn.bin_width\n",
    "        offset_log = np.log(exposure)\n",
    "\n",
    "        report = stop_glm_fit.glm_mini_report(\n",
    "            df_X=df_X, df_Y=df_Y, offset_log=offset_log,\n",
    "            cov_type='HC1',            # or 'nonrobust' for even faster\n",
    "            fast_mle=True,             # << use the ultra-fast path\n",
    "            do_inference=False,        # skip FDR/ratios/pop-tests\n",
    "            make_plots=False,          # skip figure creation\n",
    "            show_plots=True,          # nothing to display\n",
    "        )\n",
    "        \n",
    "        #cols_to_decode = ['nxt_vis', 'random_0_or_1', 'cur_vis']\n",
    "        cols_to_decode = ['nxt_vis']\n",
    "        groups = np.array(data['new_segment'])\n",
    "\n",
    "        # Decoding from fit\n",
    "        print(f\"{str}\")\n",
    "        glm_decoding.glm_decoding_from_fit(cols_to_decode, df_X, df_Y, offset_log, report)\n",
    "\n",
    "        # CV\n",
    "        print(f\"{str}\")\n",
    "        glm_decoding.glm_decoding_cv(cols_to_decode, df_X, df_Y, groups, offset_log)\n",
    "\n",
    "        # permutations\n",
    "        print(f\"{str}\")\n",
    "        glm_decoding.glm_decoding_permutation_test(cols_to_decode, df_X, df_Y,\n",
    "                                groups, offset_log, report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0222\"\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebin_data_in_new_segments(cur_or_nxt='cur', first_or_last='first', time_limit_to_count_sighting=2,\n",
    "                                pre_event_window=0, post_event_window=1.5, rebinned_max_x_lag_number=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var = pn_utils.rebin_segment_data(\n",
    "    pn.planning_data_by_point, pn.new_seg_info, bin_width=pn.bin_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var[['time_since_target_last_seen']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var['time_since_target_last_seen'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.dec.behav_data_by_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.dec.get_basic_data()\n",
    "pn.dec._make_or_retrieve_target_df(exists_ok=False)\n",
    "pn.dec.make_or_retrieve_target_cluster_df()\n",
    "pn.dec.target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var.loc[pn.rebinned_y_var['time_since_target_last_seen'].isna(), ['cur_ff_index', 'cur_in_memory', 'time']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
