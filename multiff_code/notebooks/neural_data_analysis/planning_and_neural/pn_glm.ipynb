{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -E 's/(=[^=]+)=.*$/\\1/' multiff_analysis/environment.yml > multiff_analysis/environment_nobuild.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -E 's/(=[^=]+)=.*$/\\1/' multiff_analysis/pgam_environment.yml > multiff_analysis/pgam_environment_nobuild.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, ml_methods_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools import glm_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.tpg import glm_bases, plot_glm_fit, glm_on_multiff\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "from numpy import pi\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pn_helper_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = False\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_seg.PlanningAndNeuralSegmentAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "    pn.planning_data_by_point)\n",
    "# pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "pn.rebin_data_in_new_segments(segment_duration=2, rebinned_max_x_lag_number=2)\n",
    "\n",
    "for col in ['cur_vis', 'nxt_vis', 'cur_in_memory', 'nxt_in_memory']:\n",
    "    pn.rebinned_y_var[col] = (pn.rebinned_y_var[col] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.monkey_information['dt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(process_monkey_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = pn.monkey_information.copy()\n",
    "out = process_monkey_information.compute_kinematics_loclin(m_df, time_col='time', x_col='monkey_x', y_col='monkey_y', theta_col='monkey_angle',\n",
    "                              window_s=0.040, use_theta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.monkey_information[['monkey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out['dt'] < 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1945\n",
    "out.loc[i:i+30,['dt', 'monkey_speed', 'speed', 'monkey_ddv', 'accel', 'monkey_angle', 'monkey_dw', 'ang_speed', 'monkey_ddw', 'ang_accel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def _tricube(u):\n",
    "#     u = np.clip(np.abs(u), 0, 1)\n",
    "#     return (1 - u**3)**3\n",
    "\n",
    "# def _local_linear_slope(time, z, window_s=0.040, min_pts=5, ridge=1e-8):\n",
    "#     \"\"\"\n",
    "#     Locally weighted linear regression of z(t) per sample.\n",
    "#     Returns dz/dt at each t_k. Handles irregular sampling.\n",
    "#     \"\"\"\n",
    "#     t = np.asarray(time, float)\n",
    "#     z = np.asarray(z, float)\n",
    "#     n = t.size\n",
    "#     half = 0.5 * window_s\n",
    "#     dzdt = np.zeros(n, float)\n",
    "\n",
    "#     for k in range(n):\n",
    "#         t0, t1 = t[k] - half, t[k] + half\n",
    "#         idx = np.flatnonzero((t >= t0) & (t <= t1))\n",
    "#         widen = 1.0\n",
    "#         while idx.size < min_pts and widen <= 10:\n",
    "#             widen *= 1.5\n",
    "#             idx = np.flatnonzero((t >= t[k] - half*widen) & (t <= t[k] + half*widen))\n",
    "#         tk = t[idx]\n",
    "#         tc = tk - t[k]\n",
    "#         w = _tricube(np.abs(tc) / max(half, 1e-9))\n",
    "#         X = np.column_stack([np.ones_like(tc), tc])\n",
    "#         XtW = X.T * w\n",
    "#         G = XtW @ X + ridge * np.eye(2)\n",
    "#         b  = XtW @ z[idx]\n",
    "#         coeff = np.linalg.solve(G, b)  # [intercept, slope]\n",
    "#         dzdt[k] = coeff[1]\n",
    "#     return dzdt\n",
    "\n",
    "# def local_linear_velocity(time, x, y, window_s=0.040, min_pts=5, ridge=1e-8):\n",
    "#     \"\"\"\n",
    "#     Locally weighted linear regression of x(t), y(t) per sample.\n",
    "#     Returns vx, vy, speed.\n",
    "#     \"\"\"\n",
    "#     vx = _local_linear_slope(time, x, window_s=window_s, min_pts=min_pts, ridge=ridge)\n",
    "#     vy = _local_linear_slope(time, y, window_s=window_s, min_pts=min_pts, ridge=ridge)\n",
    "#     speed = np.hypot(vx, vy)\n",
    "#     return vx, vy, speed\n",
    "\n",
    "# def compute_kinematics_loclin(df, time_col='time', x_col='x', y_col='y', theta_col='theta',\n",
    "#                               window_s=0.040, use_theta='auto'):\n",
    "#     \"\"\"\n",
    "#     Adds: vx, vy, speed, accel (tangential), ang_speed, ang_accel.\n",
    "#     - use_theta: 'auto' | 'measured' | 'velocity'\n",
    "#         'auto'      -> use measured theta if available & finite; else velocity-derived\n",
    "#         'measured'  -> force use theta_col (will fallback if missing)\n",
    "#         'velocity'  -> ignore theta_col, derive heading from vx,vy\n",
    "#     \"\"\"\n",
    "#     out = df.copy()\n",
    "#     t = out[time_col].to_numpy()\n",
    "#     x = out[x_col].to_numpy()\n",
    "#     y = out[y_col].to_numpy()\n",
    "\n",
    "#     # linear velocity via local-linear slopes\n",
    "#     vx, vy, speed = local_linear_velocity(t, x, y, window_s=window_s)\n",
    "#     out['vx'] = vx\n",
    "#     out['vy'] = vy\n",
    "#     out['speed'] = speed\n",
    "\n",
    "#     # tangential acceleration (project a onto v)\n",
    "#     ax = _local_linear_slope(t, vx, window_s=window_s)\n",
    "#     ay = _local_linear_slope(t, vy, window_s=window_s)\n",
    "#     a_t = (vx*ax + vy*ay) / np.maximum(speed, 1e-9)\n",
    "#     out['accel'] = a_t\n",
    "\n",
    "#     # choose heading source\n",
    "#     heading_from_v = np.arctan2(vy, vx)\n",
    "#     heading_src = 'velocity'\n",
    "#     if use_theta in ('auto', 'measured') and theta_col in out.columns:\n",
    "#         theta = out[theta_col].to_numpy().astype(float)\n",
    "#         if np.isfinite(theta).any():\n",
    "#             heading = np.unwrap(theta)\n",
    "#             heading_src = 'measured'\n",
    "#         else:\n",
    "#             heading = np.unwrap(heading_from_v)\n",
    "#     elif use_theta == 'velocity':\n",
    "#         heading = np.unwrap(heading_from_v)\n",
    "#     else:\n",
    "#         heading = np.unwrap(heading_from_v)\n",
    "\n",
    "#     # angular kinematics via local-linear slopes for robustness\n",
    "#     ang_speed = _local_linear_slope(t, heading, window_s=window_s)\n",
    "#     ang_accel = _local_linear_slope(t, ang_speed, window_s=window_s)\n",
    "\n",
    "#     out['ang_speed'] = ang_speed\n",
    "#     out['ang_accel'] = ang_accel\n",
    "#     out['heading_source'] = heading_src  # handy for QA\n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _interpolate_to_times(time, value, target_times):\n",
    "    t = np.asarray(time, float)\n",
    "    v = np.asarray(value, float)\n",
    "    tau = np.asarray(target_times, float)\n",
    "    return np.interp(tau, t, v)\n",
    "\n",
    "def _bin_timeseries_weighted(time, values, bin_edges, how='mean'):\n",
    "    t = np.asarray(time, float)\n",
    "    V = np.asarray(values, float)\n",
    "    if V.ndim == 1:\n",
    "        V = V[:, None]\n",
    "\n",
    "    n = t.size\n",
    "    assert n == V.shape[0] and n >= 2, 'need ≥2 samples with matching values'\n",
    "\n",
    "    dt = np.diff(t, prepend=t[0])\n",
    "    if n > 1:\n",
    "        dt[0] = dt[1]\n",
    "\n",
    "    bin_idx = np.searchsorted(bin_edges, t, side='right') - 1\n",
    "    n_bins = len(bin_edges) - 1\n",
    "\n",
    "    out_sum = np.zeros((n_bins, V.shape[1]), float)  # ∑ v·Δt\n",
    "    out_w   = np.zeros(n_bins, float)                # ∑ Δt\n",
    "\n",
    "    valid = (bin_idx >= 0) & (bin_idx < n_bins) & np.isfinite(dt)\n",
    "    np.add.at(out_sum, bin_idx[valid], (V[valid] * dt[valid, None]))\n",
    "    np.add.at(out_w,   bin_idx[valid], dt[valid])\n",
    "\n",
    "    if how == 'mean':\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            out = out_sum / out_w[:, None]\n",
    "        out[~np.isfinite(out)] = np.nan\n",
    "        return out.squeeze()\n",
    "    elif how == 'sum':\n",
    "        return out_sum.squeeze()\n",
    "    else:\n",
    "        raise ValueError('how must be \"mean\" or \"sum\"')\n",
    "\n",
    "def _bin_positions(df, bin_edges, time_col='time', x_col='x', y_col='y', method='center'):\n",
    "    \"\"\"\n",
    "    method:\n",
    "      'center' -> interpolate x,y to bin centers (good for GLM covariates)\n",
    "      'start'  -> interpolate to left edges\n",
    "      'end'    -> interpolate to right edges\n",
    "      'mean'   -> time-weighted mean position within bin (occupancy centroid)\n",
    "    \"\"\"\n",
    "    t = df[time_col].to_numpy()\n",
    "    x = df[x_col].to_numpy()\n",
    "    y = df[y_col].to_numpy()\n",
    "\n",
    "    n_bins = len(bin_edges) - 1\n",
    "    centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    if method in ('center', 'start', 'end'):\n",
    "        if method == 'center':\n",
    "            query_t = centers\n",
    "        elif method == 'start':\n",
    "            query_t = bin_edges[:-1]\n",
    "        else:\n",
    "            query_t = bin_edges[1:]\n",
    "        xb = _interpolate_to_times(t, x, query_t)\n",
    "        yb = _interpolate_to_times(t, y, query_t)\n",
    "        return pd.DataFrame({'t_bin': query_t, 'x_bin': xb, 'y_bin': yb})\n",
    "\n",
    "    if method == 'mean':\n",
    "        dt = np.diff(t, prepend=t[0])\n",
    "        if t.size > 1:\n",
    "            dt[0] = dt[1]\n",
    "        bin_idx = np.searchsorted(bin_edges, t, side='right') - 1\n",
    "        valid = (bin_idx >= 0) & (bin_idx < n_bins) & np.isfinite(dt)\n",
    "\n",
    "        x_sum = np.zeros(n_bins, float)\n",
    "        y_sum = np.zeros(n_bins, float)\n",
    "        w_sum = np.zeros(n_bins, float)\n",
    "\n",
    "        np.add.at(x_sum, bin_idx[valid], x[valid] * dt[valid])\n",
    "        np.add.at(y_sum, bin_idx[valid], y[valid] * dt[valid])\n",
    "        np.add.at(w_sum, bin_idx[valid], dt[valid])\n",
    "\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            x_mean = x_sum / w_sum\n",
    "            y_mean = y_sum / w_sum\n",
    "        x_mean[~np.isfinite(x_mean)] = np.nan\n",
    "        y_mean[~np.isfinite(y_mean)] = np.nan\n",
    "\n",
    "        return pd.DataFrame({'t_bin': centers, 'x_bin': x_mean, 'y_bin': y_mean})\n",
    "\n",
    "    raise ValueError(\"method must be 'center', 'start', 'end', or 'mean'\")\n",
    "\n",
    "def make_binned_features(df,\n",
    "                         bin_width=0.2,\n",
    "                         start=None,\n",
    "                         stop=None,\n",
    "                         # required columns\n",
    "                         time_col='time',\n",
    "                         x_col='x',\n",
    "                         y_col='y',\n",
    "                         vx_col='vx',\n",
    "                         vy_col='vy',\n",
    "                         speed_col='speed',\n",
    "                         ang_speed_col='ang_speed',\n",
    "                         spike_col='spike_count',\n",
    "                         # position binning choice\n",
    "                         pos_method='center'):\n",
    "    \"\"\"\n",
    "    Build fixed-width bin edges and return one row per bin with:\n",
    "      - position per bin (interpolated or time-weighted mean)\n",
    "      - time-weighted mean kinematics (vx, vy, speed, ang_speed)\n",
    "      - summed spikes\n",
    "      - offset_log_bin for Poisson GLM\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Contains continuous-time samples with columns listed below.\n",
    "        Kinematics should already be computed at native resolution\n",
    "        (e.g., via local-linear velocity).\n",
    "    bin_width : float\n",
    "        Bin size in seconds (e.g., 0.2 for 200 ms).\n",
    "    start, stop : float, optional\n",
    "        Start/stop times for binning. Defaults to data min/max.\n",
    "    time_col, x_col, y_col, vx_col, vy_col, speed_col, ang_speed_col, spike_col : str\n",
    "        Column names in `df`.\n",
    "    pos_method : {'center','start','end','mean'}\n",
    "        How to define per-bin position:\n",
    "          - 'center'/'start'/'end': interpolate x,y to those times\n",
    "          - 'mean': time-weighted mean position within the bin\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned : DataFrame with columns\n",
    "        t_bin, x_bin, y_bin,\n",
    "        vx_bin, vy_bin, speed_bin, ang_speed_bin,\n",
    "        spike_count_bin, offset_log_bin\n",
    "    \"\"\"\n",
    "    t = df[time_col].to_numpy()\n",
    "    if start is None:\n",
    "        start = float(t[0])\n",
    "    if stop is None:\n",
    "        stop = float(t[-1])\n",
    "\n",
    "    # Build edges (include small epsilon to catch the last bin)\n",
    "    edges = np.arange(start, stop + bin_width*0.5, bin_width)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    # Positions per bin (interpolated or time-weighted mean)\n",
    "    pos_binned = _bin_positions(\n",
    "        df, edges,\n",
    "        time_col=time_col, x_col=x_col, y_col=y_col,\n",
    "        method=pos_method\n",
    "    )\n",
    "\n",
    "    # Kinematics: time-weighted mean within each bin\n",
    "    vx_bin = _bin_timeseries_weighted(t, df[vx_col].to_numpy(), edges, how='mean')\n",
    "    vy_bin = _bin_timeseries_weighted(t, df[vy_col].to_numpy(), edges, how='mean')\n",
    "    speed_bin = _bin_timeseries_weighted(t, df[speed_col].to_numpy(), edges, how='mean')\n",
    "    ang_speed_bin = _bin_timeseries_weighted(t, df[ang_speed_col].to_numpy(), edges, how='mean')\n",
    "\n",
    "    # Spikes: sum counts in bin\n",
    "    spike_count_bin = _bin_timeseries_weighted(t, df[spike_col].to_numpy(), edges, how='sum')\n",
    "\n",
    "    # Assemble one row per bin; ensure t_bin matches positions' time axis\n",
    "    # (pos_binned['t_bin'] is centers for 'center'/'mean', left/right edges for others)\n",
    "    binned = pd.DataFrame({\n",
    "        't_bin': pos_binned['t_bin'].to_numpy(),\n",
    "        'x_bin': pos_binned['x_bin'].to_numpy(),\n",
    "        'y_bin': pos_binned['y_bin'].to_numpy(),\n",
    "        'vx_bin': vx_bin,\n",
    "        'vy_bin': vy_bin,\n",
    "        'speed_bin': speed_bin,\n",
    "        'ang_speed_bin': ang_speed_bin,\n",
    "        'spike_count_bin': spike_count_bin,\n",
    "        'offset_log_bin': np.log(bin_width),\n",
    "    })\n",
    "\n",
    "    # If you want all features aligned strictly at bin centers, you can\n",
    "    # enforce t_bin = centers here (uncomment next line):\n",
    "    # binned['t_bin'] = centers\n",
    "\n",
    "    return binned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_native has: time,x,y,vx,vy,speed,ang_speed,spike_count at native resolution\n",
    "b200 = make_binned_features(\n",
    "    df_native,\n",
    "    bin_width=0.2,\n",
    "    time_col='time',\n",
    "    x_col='x', y_col='y',\n",
    "    vx_col='vx', vy_col='vy',\n",
    "    speed_col='speed',\n",
    "    ang_speed_col='ang_speed',\n",
    "    spike_col='spike_count',\n",
    "    pos_method='center'   # or 'mean' for occupancy centroid\n",
    ")\n",
    "\n",
    "# then build your GLM design by z-scoring b200[['speed_bin', ...]]\n",
    "# and using b200['offset_log_bin'] for the Poisson offset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bin_timeseries_weighted(time, values, bin_edges, how='mean'):\n",
    "    \"\"\"\n",
    "    Bin irregularly-sampled time series into fixed time bins,\n",
    "    using **time-weighted integration**.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : array-like, shape (n_samples,)\n",
    "        Sample timestamps (must be strictly increasing).\n",
    "    values : array-like, shape (n_samples,) or (n_samples, n_features)\n",
    "        Values sampled at each timestamp. Can be scalar or vector.\n",
    "    bin_edges : array-like, shape (n_bins+1,)\n",
    "        Monotonic array of bin edges in seconds. \n",
    "        Bins are [edge[i], edge[i+1]).\n",
    "    how : {'mean', 'sum'}\n",
    "        'mean' → returns the **time-weighted average** of the values\n",
    "                 in each bin (∫v dt / Δt).\n",
    "        'sum'  → returns the **time integral** (∫v dt) in each bin.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned_values : ndarray, shape (n_bins,) or (n_bins, n_features)\n",
    "        The aggregated values per bin.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Weights each sample by how long it \"lasted\" (Δt since previous sample).\n",
    "    - This prevents bias from irregular sampling (e.g., 3 ms bursts vs 16 ms gaps).\n",
    "    - Example: with 'mean', speed_bin = distance traveled / bin_duration.\n",
    "    \"\"\"\n",
    "    t = np.asarray(time, float)\n",
    "    V = np.asarray(values, float)\n",
    "    if V.ndim == 1:\n",
    "        V = V[:, None]\n",
    "\n",
    "    n = t.size\n",
    "    assert n == V.shape[0] and n >= 2, 'need ≥2 samples with matching values'\n",
    "\n",
    "    # Duration that each sample value was held (since last sample).\n",
    "    # This is the \"weight\" for time averaging.\n",
    "    dt = np.diff(t, prepend=t[0])\n",
    "    if n > 1:\n",
    "        dt[0] = dt[1]   # make the first weight not zero\n",
    "\n",
    "    # Assign each sample to a bin by its timestamp\n",
    "    bin_idx = np.searchsorted(bin_edges, t, side='right') - 1\n",
    "    n_bins = len(bin_edges) - 1\n",
    "\n",
    "    out_sum = np.zeros((n_bins, V.shape[1]), float)  # ∑(v * Δt) per bin\n",
    "    out_w   = np.zeros(n_bins, float)                # ∑Δt per bin (denominator for mean)\n",
    "\n",
    "    # Only keep valid indices (within bin range)\n",
    "    valid = (bin_idx >= 0) & (bin_idx < n_bins) & np.isfinite(dt)\n",
    "\n",
    "    # Weighted accumulation: add v*dt to sum, and dt to weight\n",
    "    np.add.at(out_sum, bin_idx[valid], (V[valid] * dt[valid, None]))\n",
    "    np.add.at(out_w,   bin_idx[valid], dt[valid])\n",
    "\n",
    "    if how == 'mean':\n",
    "        # Divide weighted sum by total time in bin\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            out = out_sum / out_w[:, None]\n",
    "        out[~np.isfinite(out)] = np.nan\n",
    "        return out.squeeze()\n",
    "    elif how == 'sum':\n",
    "        # Return ∑(v * Δt) (time integral) — useful for spike counts\n",
    "        return out_sum.squeeze()\n",
    "    else:\n",
    "        raise ValueError('how must be \"mean\" or \"sum\"')\n",
    "\n",
    "\n",
    "def bin_kinematics_for_glm(df,\n",
    "                           bin_width=0.2,\n",
    "                           start=None,\n",
    "                           stop=None,\n",
    "                           time_col='time',\n",
    "                           vx_col='vx',\n",
    "                           vy_col='vy',\n",
    "                           speed_col='speed',\n",
    "                           ang_vel_col='ang_speed',\n",
    "                           spike_col='spike_count'):\n",
    "    \"\"\"\n",
    "    Bin continuous kinematic features and discrete spike counts\n",
    "    into fixed-width bins (e.g., 200 ms) for GLM analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Must contain time series columns for kinematics (vx, vy, speed,\n",
    "        ang_speed) and spike counts at native resolution.\n",
    "    bin_width : float\n",
    "        Desired bin size in seconds (e.g., 0.2 for 200 ms).\n",
    "    start, stop : float, optional\n",
    "        Start/stop times for binning. Defaults to df[time_col].min/max.\n",
    "    time_col : str\n",
    "        Column with timestamps in seconds.\n",
    "    vx_col, vy_col : str\n",
    "        Column names for velocity components.\n",
    "    speed_col : str\n",
    "        Column for instantaneous speed (|v|).\n",
    "    ang_vel_col : str\n",
    "        Column for angular velocity (heading rate).\n",
    "    spike_col : str\n",
    "        Column for spike counts (already per-sample counts).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned_df : DataFrame, shape (n_bins, n_features)\n",
    "        Contains:\n",
    "          - t_bin: bin centers\n",
    "          - vx_bin, vy_bin: time-weighted mean velocity components\n",
    "          - speed_bin: distance traveled / Δt (time-weighted mean of speed)\n",
    "          - ang_speed_bin: time-weighted mean angular velocity\n",
    "          - spike_count_bin: summed spike counts in bin\n",
    "          - offset_log_bin: log(bin_width), for use as Poisson offset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Kinematics: use **time-weighted mean** to respect irregular dt.\n",
    "    - Speed: time-weighted mean(|v|) is equivalent to distance traveled / Δt.\n",
    "    - Spikes: summed, since spike_count is per-sample.\n",
    "    - offset_log_bin ensures GLM models rates, not raw counts.\n",
    "    \"\"\"\n",
    "    t = df[time_col].to_numpy()\n",
    "    if start is None: start = float(t[0])\n",
    "    if stop  is None: stop  = float(t[-1])\n",
    "    edges = np.arange(start, stop + bin_width*0.5, bin_width)\n",
    "\n",
    "    # Velocity components: time-weighted average in each bin\n",
    "    vx_bin = bin_timeseries_weighted(t, df[vx_col].to_numpy(), edges, how='mean')\n",
    "    vy_bin = bin_timeseries_weighted(t, df[vy_col].to_numpy(), edges, how='mean')\n",
    "\n",
    "    # Speed: mean(|v|) = (∫|v| dt) / Δt = distance / Δt\n",
    "    speed_mean = bin_timeseries_weighted(t, df[speed_col].to_numpy(), edges, how='mean')\n",
    "    speed_bin  = speed_mean\n",
    "\n",
    "    # Angular velocity: time-weighted mean (signed); use abs(...) if you want magnitude\n",
    "    ang_vel_bin = bin_timeseries_weighted(t, df[ang_vel_col].to_numpy(), edges, how='mean')\n",
    "\n",
    "    # Spikes: sum across samples (∑ spikes in bin)\n",
    "    spike_sum = bin_timeseries_weighted(t, df[spike_col].to_numpy(), edges, how='sum')\n",
    "\n",
    "    # Bin centers for plotting / alignment\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        't_bin': centers,\n",
    "        'vx_bin': vx_bin,\n",
    "        'vy_bin': vy_bin,\n",
    "        'speed_bin': speed_bin,\n",
    "        'ang_speed_bin': ang_vel_bin,\n",
    "        'spike_count_bin': spike_sum,\n",
    "        'offset_log_bin': np.log(bin_width),\n",
    "    })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binned_features_with_stops(\n",
    "    df,\n",
    "    bin_width=0.2,\n",
    "    start=None,\n",
    "    stop=None,\n",
    "    # column names\n",
    "    time_col='time',\n",
    "    x_col='x',\n",
    "    y_col='y',\n",
    "    vx_col='vx',\n",
    "    vy_col='vy',\n",
    "    speed_col='speed',\n",
    "    ang_speed_col='ang_speed',\n",
    "    spike_col='spike_count',\n",
    "    stop_id_col='stop_id',\n",
    "    stop_onset_col='stop_id_start_time',\n",
    "    # how to bin position\n",
    "    pos_method='center'\n",
    "):\n",
    "    \"\"\"\n",
    "    Bin position, kinematics, and spikes into fixed-width bins,\n",
    "    and also attach stop-aligned labels to each bin.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Input samples at irregular times (native resolution). Must contain:\n",
    "          - time, x, y\n",
    "          - kinematics: vx, vy, speed, ang_speed\n",
    "          - spike_count\n",
    "          - stop_id, stop_id_start_time (for stop-aligned labeling)\n",
    "    bin_width : float\n",
    "        Bin width in seconds (e.g., 0.2 for 200 ms).\n",
    "    start, stop : float, optional\n",
    "        Start/stop times. Defaults to min/max of df[time_col].\n",
    "    pos_method : {'center','start','end','mean'}\n",
    "        How to compute per-bin position:\n",
    "          - 'center'/'start'/'end' : interpolate position to that time\n",
    "          - 'mean' : time-weighted mean x,y within the bin\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binned : DataFrame, one row per bin, with columns:\n",
    "        t_bin           : bin center (or query time used for position)\n",
    "        x_bin, y_bin    : position per bin\n",
    "        vx_bin, vy_bin  : time-weighted mean velocity components\n",
    "        speed_bin       : time-weighted mean speed\n",
    "        ang_speed_bin   : time-weighted mean angular velocity\n",
    "        spike_count_bin : summed spikes per bin\n",
    "        offset_log_bin  : log(bin_width), for Poisson GLM offset\n",
    "        stop_id         : ID of stop active at this bin (NaN if none)\n",
    "        t_rel_to_stop_bin : bin_time - stop_onset_time (NaN if no stop)\n",
    "        prepost_bin     : 0 = pre-stop, 1 = post-stop (relative to onset)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- Build bin edges -----\n",
    "    # If start/stop not given, use data min/max.\n",
    "    t = df[time_col].to_numpy()\n",
    "    if start is None: start = float(t[0])\n",
    "    if stop  is None: stop  = float(t[-1])\n",
    "    edges = np.arange(start, stop + bin_width*0.5, bin_width)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])  # bin centers for convenience\n",
    "\n",
    "    # ----- Bin positions -----\n",
    "    # Use helper _bin_positions, which interpolates (center/start/end)\n",
    "    # or computes time-weighted mean (method='mean').\n",
    "    pos_binned = _bin_positions(\n",
    "        df, edges,\n",
    "        time_col=time_col, x_col=x_col, y_col=y_col,\n",
    "        method=pos_method\n",
    "    )\n",
    "\n",
    "    # ----- Bin kinematics (time-weighted means) -----\n",
    "    vx_bin = _bin_timeseries_weighted(t, df[vx_col].to_numpy(), edges, how='mean')\n",
    "    vy_bin = _bin_timeseries_weighted(t, df[vy_col].to_numpy(), edges, how='mean')\n",
    "    speed_bin = _bin_timeseries_weighted(t, df[speed_col].to_numpy(), edges, how='mean')\n",
    "    ang_speed_bin = _bin_timeseries_weighted(t, df[ang_speed_col].to_numpy(), edges, how='mean')\n",
    "\n",
    "    # ----- Bin spikes (sums) -----\n",
    "    # Here spike_col should be \"count in that native bin\",\n",
    "    # so summing across native samples gives total spikes per 200 ms.\n",
    "    spike_count_bin = _bin_timeseries_weighted(t, df[spike_col].to_numpy(), edges, how='sum')\n",
    "\n",
    "    # ----- Assemble the main binned DataFrame -----\n",
    "    binned = pd.DataFrame({\n",
    "        't_bin': pos_binned['t_bin'].to_numpy(),  # time anchor for each bin\n",
    "        'x_bin': pos_binned['x_bin'].to_numpy(),\n",
    "        'y_bin': pos_binned['y_bin'].to_numpy(),\n",
    "        'vx_bin': vx_bin,\n",
    "        'vy_bin': vy_bin,\n",
    "        'speed_bin': speed_bin,\n",
    "        'ang_speed_bin': ang_speed_bin,\n",
    "        'spike_count_bin': spike_count_bin,\n",
    "        'offset_log_bin': np.log(bin_width),  # log-binwidth for Poisson offset\n",
    "    })\n",
    "\n",
    "    # ----- Add stop-aligned labels (optional) -----\n",
    "    # This requires that df has stop_id and stop_onset_col columns.\n",
    "    # For each bin, we assign the most recent stop (if any) whose onset\n",
    "    # is <= bin time.\n",
    "    if stop_id_col in df.columns and stop_onset_col in df.columns:\n",
    "        df_sorted = df[[time_col, stop_id_col, stop_onset_col]].sort_values(time_col)\n",
    "\n",
    "        stop_ids = []\n",
    "        t_rel = []\n",
    "        prepost = []\n",
    "\n",
    "        for tb in binned['t_bin']:\n",
    "            # find last sample <= tb\n",
    "            idx = np.searchsorted(df_sorted[time_col].to_numpy(), tb, side='right') - 1\n",
    "            if idx < 0:\n",
    "                # before first sample → no stop\n",
    "                stop_ids.append(np.nan)\n",
    "                t_rel.append(np.nan)\n",
    "                prepost.append(0)\n",
    "            else:\n",
    "                sid = df_sorted.iloc[idx][stop_id_col]\n",
    "                onset = df_sorted.iloc[idx][stop_onset_col]\n",
    "                if pd.isna(sid):\n",
    "                    # no stop active at this time\n",
    "                    stop_ids.append(np.nan)\n",
    "                    t_rel.append(np.nan)\n",
    "                    prepost.append(0)\n",
    "                else:\n",
    "                    # inside a stop → assign labels\n",
    "                    stop_ids.append(sid)\n",
    "                    t_rel.append(tb - onset)      # time since stop onset\n",
    "                    prepost.append(int(tb >= onset))  # 0=pre, 1=post\n",
    "\n",
    "        binned['stop_id'] = stop_ids\n",
    "        binned['t_rel_to_stop_bin'] = t_rel\n",
    "        binned['prepost_bin'] = prepost\n",
    "\n",
    "    return binned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in design_df.columns if c.startswith('cur_angle_cos_rc')]\n",
    "design_df[cols].var().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) How much signal is actually in the cos block?\n",
    "\n",
    "\n",
    "# B) Are those params actually ~0 and SE huge?\n",
    "res.params.loc[['const'] + cols]           # param values\n",
    "res.cov_params().loc[cols, cols].abs().max().max()  # largest abs entry in cov submatrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# design_df.dtypes[design_df.dtypes == \"object\"]\n",
    "# design_df.loc[:, design_df.dtypes == \"object\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pn.rebinned_y_var.copy()\n",
    "spike_data = pn.rebinned_x_var['cluster_0']\n",
    "dt = pn.bin_width\n",
    "\n",
    "res, design_df, metrics, meta = glm_on_multiff.fit_multiff_glm(\n",
    "    dt=dt, trial_ids=data[\"new_segment\"],\n",
    "    cur_vis=data[\"cur_in_memory\"], nxt_vis=data[\"nxt_in_memory\"],\n",
    "    cur_dist=data[\"cur_ff_distance\"], nxt_dist=data[\"nxt_ff_distance\"],\n",
    "    cur_angle=data[\"cur_ff_angle\"], nxt_angle=data[\"nxt_ff_angle\"],\n",
    "    heading=None, speed=data[\"monkey_speed\"], curvature=data[\"curv_of_traj\"],\n",
    "    spike_counts=spike_data, l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")\n",
    "print(\"Metrics:\\n\", {k: (v if k != 'per_trial_deviance' else '... DataFrame ...') for k, v in metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_nr = sm.GLM(spike_data, sm.add_constant(design_df),\n",
    "#                 family=sm.families.Poisson(),\n",
    "#                 exposure=np.full(len(design_df), float(pn.bin_width))\n",
    "#                ).fit()  # non-robust (model-based) cov\n",
    "# plot_glm_fit.plot_angle_kernels_with_ci(res_nr, design_df, meta, float(pn.bin_width),\n",
    "#                                         base_prefix='cur_angle', show_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_glm_fit.plot_fitted_kernels(res, design_df, meta, data[\"dt\"])\n",
    "# plot_glm_fit.plot_angle_tuning(res, design_df, meta, data[\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_prefix in ['cur_angle', 'nxt_angle', 'cur_dist', 'nxt_dist', 'cur_vis', 'nxt_vis', 'speed', 'curvature']:\n",
    "    plot_glm_fit.plot_angle_kernels_with_ci(res, design_df, meta, dt, base_prefix=base_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## across neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = [col for col in pn.rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "cluster_nums = [int(col.split('_')[1]) for col in cluster_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, design_df, metrics, meta = glm_on_multiff.fit_multiff_glm(\n",
    "    dt=pn.bin_width, trial_ids=data[\"new_segment\"],\n",
    "    cur_vis=data[\"cur_in_memory\"], nxt_vis=data[\"nxt_in_memory\"],\n",
    "    cur_dist=data[\"cur_ff_distance\"], nxt_dist=data[\"nxt_ff_distance\"],\n",
    "    cur_angle=data[\"cur_ff_angle\"], nxt_angle=data[\"nxt_ff_angle\"],\n",
    "    heading=None, speed=data[\"monkey_speed\"], curvature=data[\"curv_of_traj\"],\n",
    "    spike_counts=spike_data, l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "designs = []\n",
    "B_hist_ref = None\n",
    "\n",
    "for cluster in cluster_nums:\n",
    "    # Simulate one \"neuron\" worth of data (independent draws)\n",
    "    trial_ids, y, stim_dict, stim_basis_dict, B_hist = simulate_spikes_with_trials(\n",
    "        n_trials=12, trial_len=300, dt=dt, seed=seed + i\n",
    "    )\n",
    "    if B_hist_ref is None:\n",
    "        B_hist_ref = B_hist\n",
    "\n",
    "    # Build design + history for this neuron\n",
    "    design_df, y_fit = build_glm_design_with_trials(\n",
    "        dt=dt,\n",
    "        trial_ids=trial_ids,\n",
    "        stimulus_dict=stim_dict,\n",
    "        stimulus_basis_dict=stim_basis_dict,\n",
    "        spike_counts=y,\n",
    "        history_basis=B_hist,\n",
    "        extra_covariates=None,\n",
    "        use_trial_FE=True,\n",
    "    )\n",
    "\n",
    "    # Fit GLM (cluster-robust by trial)\n",
    "    res = fit_poisson_glm_trials(design_df, y_fit, dt, trial_ids, add_const=True, l2=0.0, cluster_se=False)\n",
    "    results.append(res)\n",
    "    designs.append(design_df)\n",
    "\n",
    "meta = {\"B_hist\": B_hist_ref}\n",
    "\n",
    "# Collect population history kernels\n",
    "hist_df = collect_history_kernels_across_neurons(results, designs, meta, dt)\n",
    "\n",
    "# Plot overlays + heatmap\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=True, max_overlays=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you looped over N neurons and saved:\n",
    "results = [res_n0, res_n1, ..., res_nN]         # statsmodels results\n",
    "designs = [X_n0_df, X_n1_df, ..., X_nN_df]      # matching design DataFrames\n",
    "meta    = meta_from_any_single_fit              # must contain 'B_hist'\n",
    "dt      = 0.01                                  # your bin size (s)\n",
    "\n",
    "hist_df = collect_history_kernels_across_neurons(\n",
    "    results, designs, meta, dt, neuron_ids=None  # or e.g. list of unit IDs\n",
    ")\n",
    "\n",
    "\n",
    "# Overlay individual kernels (up to max_overlays) + population mean ± 95% CI\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=False, max_overlays=60)\n",
    "\n",
    "# Heatmap only (neuron × lag)\n",
    "plot_history_kernels_population(hist_df, overlay_mean=False, heatmap=True)\n",
    "\n",
    "# Both\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glm_on_multiff.simulate_multiff_trials(n_trials=8, trial_len=350, dt=0.01, seed=3)\n",
    "res, design_df, metrics, meta = glm_on_multiff.fit_multiff_glm(\n",
    "    dt=data[\"dt\"], trial_ids=data[\"trial_ids\"],\n",
    "    cur_vis=data[\"cur_vis\"], nxt_vis=data[\"nxt_vis\"],\n",
    "    cur_dist=data[\"cur_dist\"], nxt_dist=data[\"nxt_dist\"],\n",
    "    cur_angle=data[\"cur_angle\"], nxt_angle=data[\"nxt_angle\"],\n",
    "    heading=data[\"heading\"], speed=data[\"speed\"], curvature=data[\"curvature\"],\n",
    "    spike_counts=data[\"spike_counts\"], l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")\n",
    "print(\"Metrics:\\n\", {k: (v if k != 'per_trial_deviance' else '... DataFrame ...') for k, v in metrics.items()})\n",
    "# plot_glm_fit.plot_fitted_kernels(res, design_df, meta, data[\"dt\"])\n",
    "# plot_glm_fit.plot_angle_tuning(res, design_df, meta, data[\"dt\"])\n",
    "plot_glm_fit.plot_angle_kernels_with_ci(res, design_df, meta, data[\"dt\"], base_prefix='cur_angle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in data.items():\n",
    "    try:\n",
    "        print(key, item.shape)\n",
    "    except:\n",
    "        print(key, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGAM functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(pn.x_var, pn.y_var_reduced, pn.bin_width, pn.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.prepare_for_pgam(num_total_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal kernel\n",
    "\n",
    "modified from PGAM_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_temporal_features_to_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gdh.smooths_handler.add_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_spatial_features_to_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.run_pgam(neural_cluster_number=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.post_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "# Degree of spline\n",
    "k = 3\n",
    "\n",
    "# Example 1: Not enough start knots\n",
    "knots1 = [0, 2, 4, 6, 8, 10, 10, 10, 10]\n",
    "coeffs = np.ones(len(knots1) - k - 1)\n",
    "spline1 = BSpline(knots1, coeffs, k)\n",
    "\n",
    "# Example 2: Proper repeated start knots\n",
    "knots2 = [0, 0, 0, 0, 2, 4, 6, 8, 10, 10, 10, 10]\n",
    "coeffs2 = np.ones(len(knots2) - k - 1)\n",
    "spline2 = BSpline(knots2, coeffs2, k)\n",
    "\n",
    "x = np.linspace(-1, 11, 400)\n",
    "\n",
    "plt.plot(x, spline1(x), label=\"Start knot once\")\n",
    "plt.plot(x, spline2(x), label=\"Start knot repeated\")\n",
    "plt.axvline(0, color='gray', linestyle='--', label=\"x = 0\")\n",
    "plt.legend()\n",
    "plt.title(\"Effect of Repeating Start Knot\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Spline Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate through all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(pn.x_var, pn.y_var, pn.bin_width, pn.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(pn.x_var.shape[1]):\n",
    "    print(f'neural_cluster_number: {i} out of {pn.x_var.shape[1]}')\n",
    "    pgam_inst.streamline_pgam(neural_cluster_number=i, num_total_trials=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
