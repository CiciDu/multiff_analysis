{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, ml_methods_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils\n",
    "from neural_data_analysis.design_kits.design_by_segment import create_design_df, predictor_utils, other_feats\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event, pn_glm_utils\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.tpg import glm_bases, glm_plotting, glm_plotting2, glm_fit\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, get_stops_utils, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_fit import stop_glm_fit, cv_stop_glm, glm_fit_utils, variance_explained\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_plotting import plot_spikes, plot_glm_fit, plot_tuning_func\n",
    "from neural_data_analysis.design_kits.design_around_event import event_binning, stop_design, cluster_design, design_checks\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_hyperparams import compare_glm_configs, glm_hyperparams_class\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_decoding_tools import glm_decoding_llr, glm_decoding\n",
    "\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "from numpy import pi\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "pn.rebin_data_in_new_segments(cur_or_nxt='cur', first_or_last='first', time_limit_to_count_sighting=2,\n",
    "                                 pre_event_window=0, post_event_window=1.5, rebinned_max_x_lag_number=2)\n",
    "\n",
    "for col in ['cur_vis', 'nxt_vis', 'cur_in_memory', 'nxt_in_memory']:\n",
    "    pn.rebinned_y_var[col] = (pn.rebinned_y_var[col] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_x_var = pn.rebinned_x_var.copy()\n",
    "rebinned_y_var = pn.rebinned_y_var.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get heading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_info_df, heading_df = pn_glm_utils.get_test_heading_df(raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebinned_x_var, rebinned_y_var = pn_glm_utils.select_ff_subset(heading_df, pn.rebinned_x_var, pn.rebinned_y_var, \n",
    "                                                               top=False, pct=0.5)\n",
    "rebinned_x_var = pn_glm_utils.drop_constant_columns(rebinned_x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(create_design_df)\n",
    "reload(other_feats)\n",
    "\n",
    "data = rebinned_y_var.copy()\n",
    "trial_ids = data['new_segment']\n",
    "dt = pn.bin_width\n",
    "\n",
    "design_df, meta0, meta = create_design_df.get_initial_design_df(data, dt, trial_ids)\n",
    "\n",
    "cluster_num = 3\n",
    "y = rebinned_x_var[f'cluster_{cluster_num}']\n",
    "\n",
    "# design_df, meta = create_design_df.add_spike_history(\n",
    "#     design_df, y, meta0['trial_ids'], dt,\n",
    "#     n_basis=4, t_max=0.20, edge='zero',\n",
    "#     prefix='spk_hist', style='bjk',\n",
    "#     meta=meta\n",
    "# )\n",
    "\n",
    "chk = predictor_utils.check_design_vs_bases(design_df, meta, strict=True)\n",
    "assert chk['ok'], chk['problems']\n",
    "\n",
    "\n",
    "df_X = design_df[['speed_z', 'time_since_last_capture',\n",
    "       'ang_accel_mag_spline:s0', 'ang_accel_mag_spline:s1',\n",
    "       'ang_accel_mag_spline:s2', 'ang_accel_mag_spline:s3', 'cur_vis', 'nxt_vis']].copy()\n",
    "\n",
    "print('df_X.shape:', df_X.shape)\n",
    "cluster_cols = [col for col in rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "df_Y = rebinned_x_var[cluster_cols]\n",
    "df_Y.columns = df_Y.columns.str.replace('cluster_', '').astype(int)\n",
    "\n",
    "\n",
    "\n",
    "df_X['random_0_or_1'] = np.random.randint(0, 2, len(df_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other ways of taking out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = design_df[['speed_z', 'time_since_last_capture',\n",
    "       'ang_accel_mag_spline:s0', 'ang_accel_mag_spline:s1',\n",
    "       'ang_accel_mag_spline:s2', 'ang_accel_mag_spline:s3'] + cur_ff_cols + nxt_ff_cols].copy()\n",
    "\n",
    "print('df_X.shape:', df_X.shape)\n",
    "cluster_cols = [col for col in rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "df_Y = rebinned_x_var[cluster_cols]\n",
    "df_Y.columns = df_Y.columns.str.replace('cluster_', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_X = design_df\n",
    "\n",
    "# # cur_ff_distance_cols = [col for col in design_df.columns if 'cur_ff_distance' in col]\n",
    "# # df_X = design_df[cur_ff_distance_cols + ['const']]\n",
    "\n",
    "cur_ff_cols = [col for col in design_df.columns if ('cur_' in col) and ('cur_in_memory' not in col)]\n",
    "nxt_ff_cols = [col for col in design_df.columns if 'nxt_' in col]\n",
    "\n",
    "# # df_X = design_selected\n",
    "\n",
    "# # df_X = design_selected.copy()\n",
    "\n",
    "# df_X = design_df[['speed_z', 'time_since_last_capture',\n",
    "#        'ang_accel_mag_spline:s0', 'ang_accel_mag_spline:s1',\n",
    "#        'ang_accel_mag_spline:s2', 'ang_accel_mag_spline:s3']].copy()\n",
    "# df_X[cur_ff_distance_cols] = design_df[cur_ff_distance_cols].copy()\n",
    "\n",
    "# print('df_X.shape:', df_X.shape)\n",
    "# cluster_cols = [col for col in rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "# df_Y = rebinned_x_var[cluster_cols]\n",
    "# df_Y.columns = df_Y.columns.str.replace('cluster_', '').astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = np.full_like(y, fill_value=pn.bin_width, dtype=float)\n",
    "offset_log = np.log(exposure)\n",
    "\n",
    "report = stop_glm_fit.glm_mini_report(\n",
    "    df_X=df_X, df_Y=df_Y, offset_log=offset_log,\n",
    "    cov_type='HC1',            # or 'nonrobust' for even faster\n",
    "    fast_mle=True,             # << use the ultra-fast path\n",
    "    do_inference=False,        # skip FDR/ratios/pop-tests\n",
    "    make_plots=True,          # skip figure creation\n",
    "    show_plots=True,          # nothing to display\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = np.full_like(y, fill_value=pn.bin_width, dtype=float)\n",
    "offset_log = np.log(exposure)\n",
    "cols_to_decode = ['nxt_vis', 'random_0_or_1', 'cur_vis']\n",
    "glm_decoding.glm_decoding_from_fit(cols_to_decode, df_X, df_Y, offset_log, report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_decode = ['nxt_vis']\n",
    "groups = np.array(data['new_segment'])\n",
    "glm_decoding.glm_decoding_cv(cols_to_decode, df_X, df_Y, groups, offset_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_decode = ['random_0_or_1', 'nxt_vis', 'cur_vis']\n",
    "groups = np.array(data['new_segment'])\n",
    "glm_decoding.glm_decoding_permutation_test(cols_to_decode, df_X, df_Y,\n",
    "                          groups, offset_log, report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM (from stop glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df    = report['coefs_df']\n",
    "metrics_df  = report['metrics_df']\n",
    "pop_tests   = report['population_tests_df']\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(variance_explained)\n",
    "df_Y_pred = variance_explained.build_df_Y_pred_from_results(\n",
    "    results=report['results'],\n",
    "    df_X=df_X,\n",
    "    offset_log=offset_log,\n",
    "    df_Y=df_Y\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "assert df_Y_pred.shape == df_Y.shape\n",
    "assert np.isfinite(df_Y_pred.values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert observed/predicted DataFrames to arrays\n",
    "X = df_Y.to_numpy()      # observed counts\n",
    "X_hat = df_Y_pred.to_numpy() # predicted expected counts\n",
    "#event_ids = meta_used['event_id'].to_numpy()\n",
    "event_ids = rebinned_x_var['new_segment'].to_numpy()\n",
    "\n",
    "ve_pop, k_eff = variance_explained.population_VE_in_PCspace(X, X_hat, k=10, center='neuron')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_per_neuron, ve_mean = variance_explained.single_neuron_temporal_VE(X, X_hat, aggregate='mean')\n",
    "ve_median = float(np.median(ve_per_neuron))\n",
    "\n",
    "print(f'Mean single-neuron VE:   {ve_mean:.3f}')\n",
    "print(f'Median single-neuron VE: {ve_median:.3f}')\n",
    "\n",
    "variance_explained.plot_single_neuron_VE_hist(ve_per_neuron)\n",
    "\n",
    "ve_pop, k_eff = variance_explained.population_VE_in_PCspace(X, X_hat, k=10, center='neuron')\n",
    "print(f'Population VE in {k_eff} PCs: {ve_pop:.3f}')\n",
    "\n",
    "variance_explained.plot_population_VE_bar(ve_pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(ve_per_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def population_ve_cca(X, Y, n_components=10):\n",
    "    \"\"\"\n",
    "    Canonical correlation analysis between observed (X) and predicted (Y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (samples, neurons)\n",
    "    Y : array, shape (samples, neurons) -- same shape as X\n",
    "    n_components : number of canonical pairs to compute\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrs : array of canonical correlations (length n_components)\n",
    "    mean_corr : average canonical correlation across components\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, 'X and Y must have same shape'\n",
    "\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    Xc, Yc = cca.fit_transform(X, Y)\n",
    "    corrs = [np.corrcoef(Xc[:,i], Yc[:,i])[0,1] for i in range(Xc.shape[1])]\n",
    "    return np.array(corrs), np.mean(corrs)\n",
    "\n",
    "corrs, mean_corr = population_ve_cca(X, X_hat, n_components=10)\n",
    "print('Canonical correlations:', corrs)\n",
    "print('Mean correlation:', mean_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_rank_regression(X, Y, rank=10):\n",
    "    \"\"\"\n",
    "    Reduced-rank regression: regress Y onto X with low-rank constraint.\n",
    "\n",
    "    Returns regression coefficients B of shape (X_dim, Y_dim).\n",
    "    \"\"\"\n",
    "    # Center\n",
    "    Xc = X - X.mean(0, keepdims=True)\n",
    "    Yc = Y - Y.mean(0, keepdims=True)\n",
    "\n",
    "    # Solve full regression\n",
    "    B_ols, _, _, _ = np.linalg.lstsq(Xc, Yc, rcond=None)\n",
    "\n",
    "    # Project coefficient matrix onto low-rank space\n",
    "    U, s, Vt = np.linalg.svd(B_ols, full_matrices=False)\n",
    "    B_rr = U[:, :rank] @ np.diag(s[:rank]) @ Vt[:rank, :]\n",
    "    return B_rr\n",
    "\n",
    "B_rr = reduced_rank_regression(X, X_hat, rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-stop breakdown (good for figures)\n",
    "per_stop_df = variance_explained.per_event_breakdown(X, X_hat, event_ids=event_ids, k=10)\n",
    "print(per_stop_df.head())\n",
    "\n",
    "summary_metrics = {\n",
    "    'VE_population_PC': ve_pop,\n",
    "    'VE_single_unit_mean': ve_mean,\n",
    "    'VE_single_unit_median': ve_median,\n",
    "    'PCs_used': k_eff\n",
    "}\n",
    "print(pd.Series(summary_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def _zscore(X, eps=1e-12):\n",
    "    mu = np.nanmean(X, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(X, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (X - mu) / sd, mu, sd\n",
    "\n",
    "def _nanmask_pair(X, Y):\n",
    "    m = np.isfinite(X).all(1) & np.isfinite(Y).all(1)\n",
    "    return X[m], Y[m], m\n",
    "\n",
    "def cca_pop_metric_train_test(\n",
    "    X_tr, Y_tr, X_te, Y_te, *,\n",
    "    n_components=10,\n",
    "    standardize=True,\n",
    "    map_type='diag',          # 'diag' (per-dim gain) or 'full' (least-squares)\n",
    "    max_iter=1000,\n",
    "    eps=1e-12\n",
    "):\n",
    "    \"\"\"\n",
    "    Population comparison in CCA subspace (train/test safe).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr, Y_tr : (n_train, n_neurons)\n",
    "    X_te, Y_te : (n_test,  n_neurons)\n",
    "    n_components : number of canonical pairs to keep\n",
    "    standardize : z-score each view before CCA (recommended)\n",
    "    map_type : 'diag' maps each canonical dim with a scalar gain learned on train;\n",
    "               'full' learns an unconstrained linear map between canonical spaces on train.\n",
    "    Returns\n",
    "    -------\n",
    "    out : dict\n",
    "        {\n",
    "          've_test': float in [0,1],\n",
    "          'corrs_train': (k,), canonical correlations on train,\n",
    "          'corrs_test':  (k,), canonical correlations on test (using train-fitted CCA),\n",
    "          'k_eff': int, number of dims used,\n",
    "          'A': mapping matrix in canonical space (k x k),\n",
    "          'cca': fitted CCA object,\n",
    "          'mu_sigmas': {(view): (mu, sd)} if standardized\n",
    "        }\n",
    "    \"\"\"\n",
    "    X_tr, Y_tr, _ = _nanmask_pair(np.asarray(X_tr, float), np.asarray(Y_tr, float))\n",
    "    X_te, Y_te, _ = _nanmask_pair(np.asarray(X_te, float), np.asarray(Y_te, float))\n",
    "\n",
    "    # standardize each view (prevents scale dominating CCA)\n",
    "    mu_sigmas = {}\n",
    "    if standardize:\n",
    "        X_tr, mu_x, sd_x = _zscore(X_tr, eps)\n",
    "        Y_tr, mu_y, sd_y = _zscore(Y_tr, eps)\n",
    "        X_te = (X_te - mu_x) / sd_x\n",
    "        Y_te = (Y_te - mu_y) / sd_y\n",
    "        mu_sigmas = {'X': (mu_x, sd_x), 'Y': (mu_y, sd_y)}\n",
    "\n",
    "    # fit CCA on TRAIN only\n",
    "    k = int(n_components)\n",
    "    cca = CCA(n_components=k, max_iter=max_iter)\n",
    "    Xc_tr, Yc_tr = cca.fit_transform(X_tr, Y_tr)         # (n_train, k), (n_train, k)\n",
    "\n",
    "    # canonical correlations on TRAIN\n",
    "    corrs_tr = np.array([np.corrcoef(Xc_tr[:, i], Yc_tr[:, i])[0, 1] for i in range(Xc_tr.shape[1])])\n",
    "    # transform TEST into the SAME canonical axes\n",
    "    Xc_te, Yc_te = cca.transform(X_te, Y_te)\n",
    "\n",
    "    # canonical correlations on TEST (no re-fit)\n",
    "    corrs_te = np.array([np.corrcoef(Xc_te[:, i], Yc_te[:, i])[0, 1] for i in range(Xc_te.shape[1])])\n",
    "\n",
    "    # learn mapping in canonical space on TRAIN\n",
    "    if map_type == 'diag':\n",
    "        # per-dim least-squares gains: minimize ||Xc - A_diag Yc||^2\n",
    "        gains = []\n",
    "        for i in range(Xc_tr.shape[1]):\n",
    "            y = Yc_tr[:, i]\n",
    "            x = Xc_tr[:, i]\n",
    "            num = float(np.dot(y, x))\n",
    "            den = float(np.dot(y, y)) + eps\n",
    "            gains.append(num / den)\n",
    "        A = np.diag(gains)\n",
    "    elif map_type == 'full':\n",
    "        # unconstrained least squares in canonical space\n",
    "        # solve Yc_tr @ A ≈ Xc_tr  =>  A = argmin ||Yc_tr A - Xc_tr||\n",
    "        A, _, _, _ = np.linalg.lstsq(Yc_tr, Xc_tr, rcond=None)\n",
    "    else:\n",
    "        raise ValueError('map_type must be \"diag\" or \"full\"')\n",
    "\n",
    "    # VE-style score on TEST in the learned canonical subspace\n",
    "    # 1 - ||Xc_te - A Yc_te||_F^2 / ||Xc_te||_F^2\n",
    "    X_hat_can_te = Yc_te @ A\n",
    "    den = float(np.sum(Xc_te**2))\n",
    "    if not np.isfinite(den) or den <= eps:\n",
    "        ve_test = 0.0\n",
    "    else:\n",
    "        num = float(np.sum((Xc_te - X_hat_can_te)**2))\n",
    "        ve_test = 1.0 - (num / den)\n",
    "        ve_test = float(np.clip(ve_test, 0.0, 1.0))\n",
    "\n",
    "    return {\n",
    "        've_test': ve_test,\n",
    "        'corrs_train': corrs_tr,\n",
    "        'corrs_test': corrs_te,\n",
    "        'k_eff': int(Xc_tr.shape[1]),\n",
    "        'A': A,\n",
    "        'cca': cca,\n",
    "        'mu_sigmas': mu_sigmas\n",
    "    }\n",
    "\n",
    "def kfold_cca_pop_metric(\n",
    "    X, Y, *, n_components=10, n_splits=5, standardize=True, map_type='diag', rng=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience K-fold wrapper returning fold-wise VE and canonical correlation spectra.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=rng)\n",
    "    ve_list, corrs_tr_list, corrs_te_list = [], [], []\n",
    "    for tr, te in kf.split(X):\n",
    "        out = cca_pop_metric_train_test(\n",
    "            X[tr], Y[tr], X[te], Y[te],\n",
    "            n_components=n_components, standardize=standardize, map_type=map_type\n",
    "        )\n",
    "        ve_list.append(out['ve_test'])\n",
    "        corrs_tr_list.append(out['corrs_train'])\n",
    "        corrs_te_list.append(out['corrs_test'])\n",
    "    # pad corr arrays if n_components varies (it shouldn’t with sklearn CCA)\n",
    "    corrs_tr = np.vstack(corrs_tr_list)\n",
    "    corrs_te = np.vstack(corrs_te_list)\n",
    "    return {\n",
    "        've_mean': float(np.mean(ve_list)),\n",
    "        've_std':  float(np.std(ve_list)),\n",
    "        'corrs_train_mean': np.nanmean(corrs_tr, axis=0),\n",
    "        'corrs_test_mean':  np.nanmean(corrs_te, axis=0),\n",
    "        'corrs_train_all': corrs_tr,\n",
    "        'corrs_test_all':  corrs_te\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, X_hat: (samples, neurons) aligned in time; split by trial/stop for CV!\n",
    "cv = kfold_cca_pop_metric(\n",
    "    X, X_hat,\n",
    "    n_components=10,\n",
    "    n_splits=5,\n",
    "    standardize=True,\n",
    "    map_type='diag'   # try 'full' as a sensitivity check\n",
    ")\n",
    "\n",
    "print('CCA-subspace VE (mean±sd):', cv['ve_mean'], cv['ve_std'])\n",
    "print('Mean canonical correlations (train):', cv['corrs_train_mean'])\n",
    "print('Mean canonical correlations (test): ', cv['corrs_test_mean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import lstsq, svd\n",
    "\n",
    "def rrr_fit(X_tr, Y_tr, rank, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Fit reduced-rank regression: X ≈ Y @ B,  rank(B) ≤ rank.\n",
    "    Returns B (features_Y × features_X) and centering stats.\n",
    "    \"\"\"\n",
    "    X_tr = np.asarray(X_tr, float); Y_tr = np.asarray(Y_tr, float)\n",
    "    mx = X_tr.mean(0, keepdims=True); my = Y_tr.mean(0, keepdims=True)\n",
    "    Xc = X_tr - mx; Yc = Y_tr - my\n",
    "\n",
    "    # Full OLS then truncate via SVD (optimal rank-r in Frobenius norm)\n",
    "    B_ols, *_ = lstsq(Yc, Xc, rcond=None)            # (pY × pX)\n",
    "    U, s, Vt = svd(B_ols, full_matrices=False)\n",
    "    r = int(max(1, min(rank, np.sum(s > eps))))\n",
    "    B_rr = (U[:, :r] @ np.diag(s[:r]) @ Vt[:r, :])    # rank-r projection of B_ols\n",
    "    return {'B': B_rr, 'mx': mx, 'my': my, 'rank': r}\n",
    "\n",
    "def rrr_predict(fit, Y):\n",
    "    Y = np.asarray(Y, float)\n",
    "    return (Y - fit['my']) @ fit['B'] + fit['mx']\n",
    "\n",
    "def ve_fro(X, Xhat, eps=1e-12):\n",
    "    X = np.asarray(X, float); Xhat = np.asarray(Xhat, float)\n",
    "    num = np.sum((X - Xhat)**2)\n",
    "    den = np.sum((X - X.mean(0, keepdims=True))**2)\n",
    "    if not np.isfinite(den) or den <= eps: return 0.0\n",
    "    ve = 1.0 - num / den\n",
    "    return float(np.clip(ve, 0.0, 1.0))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def rrr_cv(X, Y, ranks=(1,2,3,5,8,12,16), n_splits=5, rng=0):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=rng)\n",
    "    scores = {r: [] for r in ranks}\n",
    "    for tr, te in kf.split(X):\n",
    "        for r in ranks:\n",
    "            fit = rrr_fit(X[tr], Y[tr], rank=r)\n",
    "            Xhat_te = rrr_predict(fit, Y[te])\n",
    "            scores[r].append(ve_fro(X[te], Xhat_te))\n",
    "    means = {r: float(np.mean(v)) for r, v in scores.items()}\n",
    "    best_rank = max(means, key=means.get)\n",
    "    return best_rank, means, scores\n",
    "\n",
    "# Split by trials/stops, not random bins (to avoid leakage).\n",
    "best_r, mean_by_r, cv_scores = rrr_cv(X, X_hat, ranks=(1,2,3,5,8,12), n_splits=5, rng=0)\n",
    "print('Best rank:', best_r, 'CV VE:', mean_by_r[best_r])\n",
    "\n",
    "# Train final model at best rank on all data, or retrain on a train split and evaluate on held-out:\n",
    "fit = rrr_fit(X, X_hat, rank=best_r)\n",
    "Xhat_rr = rrr_predict(fit, X_hat)\n",
    "ve_total = ve_fro(X, Xhat_rr)\n",
    "print('RRR VE (all data, optimistic):', ve_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCA shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def cca_shuffle_test(X, Y, n_components=10, n_splits=100, shuffle_axis=0, random_state=0):\n",
    "    \"\"\"\n",
    "    Permutation test for canonical correlations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : arrays (samples × neurons), same shape\n",
    "    n_components : # of CCA dimensions to compute\n",
    "    n_splits : number of permutations\n",
    "    shuffle_axis : 0 = shuffle rows (samples/trials), 1 = shuffle cols (neurons)\n",
    "    random_state : reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrs_real : array, canonical correlations from true data\n",
    "    thresh : array, 95th percentile across shuffles for each dim\n",
    "    sig_dims : list, indices of significant canonical dims\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # fit on true data\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    Xc, Yc = cca.fit_transform(X, Y)\n",
    "    corrs_real = np.array([np.corrcoef(Xc[:, i], Yc[:, i])[0, 1] for i in range(n_components)])\n",
    "\n",
    "    # shuffle null\n",
    "    corrs_shuff = []\n",
    "    for _ in range(n_splits):\n",
    "        if shuffle_axis == 0:\n",
    "            Y_perm = rng.permutation(Y)      # shuffle rows\n",
    "        else:\n",
    "            Y_perm = Y.copy()\n",
    "            rng.shuffle(Y_perm.T)           # shuffle columns independently\n",
    "        Xs, Ys = cca.fit_transform(X, Y_perm)\n",
    "        corrs = [np.corrcoef(Xs[:, i], Ys[:, i])[0, 1] for i in range(n_components)]\n",
    "        corrs_shuff.append(corrs)\n",
    "    corrs_shuff = np.vstack(corrs_shuff)\n",
    "\n",
    "    # significance threshold per dim\n",
    "    thresh = np.percentile(corrs_shuff, 95, axis=0)\n",
    "    sig_dims = [i for i, c in enumerate(corrs_real) if c > thresh[i]]\n",
    "\n",
    "    return corrs_real, thresh, sig_dims\n",
    "\n",
    "corrs_real, thresh, sig_dims = cca_shuffle_test(X, X_hat, n_components=10, n_splits=100)\n",
    "\n",
    "print(\"Canonical correlations:\", corrs_real)\n",
    "print(\"95% shuffle thresholds:\", thresh)\n",
    "print(\"Significant dims:\", sig_dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population_latent_benchmark.py\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Dict, Any, Tuple, List\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from numpy.linalg import lstsq, svd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- small utils ----------\n",
    "def _zscore(A: np.ndarray, eps: float = 1e-12) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    mu = np.nanmean(A, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(A, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (A - mu) / sd, mu, sd\n",
    "\n",
    "def _fro_ve(X: np.ndarray, Xhat: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    num = np.sum((X - Xhat) ** 2)\n",
    "    den = np.sum((X - np.mean(X, axis=0, keepdims=True)) ** 2)\n",
    "    if not np.isfinite(den) or den <= eps:\n",
    "        return 0.0\n",
    "    ve = 1.0 - (num / den)\n",
    "    return float(np.clip(ve, 0.0, 1.0))\n",
    "\n",
    "def _safe_mask_pair(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    m = np.isfinite(X).all(1) & np.isfinite(Y).all(1)\n",
    "    return X[m], Y[m], m\n",
    "\n",
    "# ---------- RRR ----------\n",
    "def rrr_fit(X_tr: np.ndarray, Y_tr: np.ndarray, rank: int, eps: float = 1e-12) -> Dict[str, Any]:\n",
    "    X_tr = np.asarray(X_tr, float); Y_tr = np.asarray(Y_tr, float)\n",
    "    mx = X_tr.mean(0, keepdims=True); my = Y_tr.mean(0, keepdims=True)\n",
    "    Xc = X_tr - mx; Yc = Y_tr - my\n",
    "    B_ols, *_ = lstsq(Yc, Xc, rcond=None)               # (pY × pX)\n",
    "    U, s, Vt = svd(B_ols, full_matrices=False)\n",
    "    r = int(max(1, min(rank, np.sum(s > eps))))\n",
    "    B_rr = U[:, :r] @ np.diag(s[:r]) @ Vt[:r, :]\n",
    "    return {'B': B_rr, 'mx': mx, 'my': my, 'rank': r}\n",
    "\n",
    "def rrr_predict(fit: Dict[str, Any], Y: np.ndarray) -> np.ndarray:\n",
    "    return (np.asarray(Y, float) - fit['my']) @ fit['B'] + fit['mx']\n",
    "\n",
    "def rrr_cv(X: np.ndarray, Y: np.ndarray, groups: np.ndarray,\n",
    "           ranks: Iterable[int] = (1, 2, 3, 5, 8, 12, 16),\n",
    "           n_splits: int = 5, rng: int = 0) -> Dict[str, Any]:\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    ranks = list(ranks)\n",
    "    ve_mat = {r: [] for r in ranks}\n",
    "    for tr_idx, te_idx in gkf.split(X, groups=groups):\n",
    "        for r in ranks:\n",
    "            fit = rrr_fit(X[tr_idx], Y[tr_idx], rank=r)\n",
    "            Xhat_te = rrr_predict(fit, Y[te_idx])\n",
    "            ve_mat[r].append(_fro_ve(X[te_idx], Xhat_te))\n",
    "    ve_mean = {r: float(np.mean(v)) for r, v in ve_mat.items()}\n",
    "    best_rank = max(ve_mean, key=ve_mean.get)\n",
    "    return {'ve_by_rank': ve_mean, 'best_rank': best_rank, 'cv_scores': ve_mat}\n",
    "\n",
    "# ---------- CCA (train/test safe) ----------\n",
    "def cca_train_test(X_tr: np.ndarray, Y_tr: np.ndarray,\n",
    "                   X_te: np.ndarray, Y_te: np.ndarray,\n",
    "                   n_components: int = 10, standardize: bool = True,\n",
    "                   map_type: str = 'diag', max_iter: int = 1000,\n",
    "                   eps: float = 1e-12) -> Dict[str, Any]:\n",
    "    if standardize:\n",
    "        X_tr, mux, sdx = _zscore(X_tr); X_te = (X_te - mux) / sdx\n",
    "        Y_tr, muy, sdy = _zscore(Y_tr); Y_te = (Y_te - muy) / sdy\n",
    "    cca = CCA(n_components=int(n_components), max_iter=max_iter)\n",
    "    Xc_tr, Yc_tr = cca.fit_transform(X_tr, Y_tr)\n",
    "    Xc_te, Yc_te = cca.transform(X_te, Y_te)\n",
    "\n",
    "    corrs_tr = np.array([np.corrcoef(Xc_tr[:, i], Yc_tr[:, i])[0, 1] for i in range(Xc_tr.shape[1])])\n",
    "    corrs_te = np.array([np.corrcoef(Xc_te[:, i], Yc_te[:, i])[0, 1] for i in range(Xc_te.shape[1])])\n",
    "\n",
    "    if map_type == 'diag':\n",
    "        gains = []\n",
    "        for i in range(Xc_tr.shape[1]):\n",
    "            num = float(np.dot(Yc_tr[:, i], Xc_tr[:, i]))\n",
    "            den = float(np.dot(Yc_tr[:, i], Yc_tr[:, i])) + eps\n",
    "            gains.append(num / den)\n",
    "        A = np.diag(gains)\n",
    "    elif map_type == 'full':\n",
    "        A, *_ = lstsq(Yc_tr, Xc_tr, rcond=None)\n",
    "    else:\n",
    "        raise ValueError('map_type must be \"diag\" or \"full\"')\n",
    "\n",
    "    Xc_hat_te = Yc_te @ A\n",
    "    den = float(np.sum(Xc_te ** 2))\n",
    "    ve_te = 0.0 if den <= eps else float(np.clip(1.0 - np.sum((Xc_te - Xc_hat_te) ** 2) / den, 0.0, 1.0))\n",
    "    return {'corrs_train': corrs_tr, 'corrs_test': corrs_te, 've_test': ve_te, 'A': A, 'cca': cca}\n",
    "\n",
    "def cca_shuffle_test_on_test(X_te: np.ndarray, Y_te: np.ndarray, cca_obj: CCA,\n",
    "                             n_components: int, n_shuffles: int = 100,\n",
    "                             rng: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Use fixed CCA axes (trained on train) to avoid leakage\n",
    "    rng = np.random.default_rng(rng)\n",
    "    Xc_te, Yc_te = cca_obj.transform(X_te, Y_te)\n",
    "    k = int(n_components)\n",
    "    corrs_real = np.array([np.corrcoef(Xc_te[:, i], Yc_te[:, i])[0, 1] for i in range(k)])\n",
    "    null = []\n",
    "    for _ in range(n_shuffles):\n",
    "        Yp = rng.permutation(Y_te)  # row-shuffle on test\n",
    "        _, Yc_perm = cca_obj.transform(X_te, Yp)\n",
    "        null.append([np.corrcoef(Xc_te[:, i], Yc_perm[:, i])[0, 1] for i in range(k)])\n",
    "    null = np.vstack(null)\n",
    "    thresh = np.percentile(null, 95, axis=0)\n",
    "    return corrs_real, thresh\n",
    "\n",
    "# ---------- FA latent metric ----------\n",
    "def fa_train_test(X_tr: np.ndarray, Y_tr: np.ndarray,\n",
    "                  X_te: np.ndarray, Y_te: np.ndarray,\n",
    "                  n_factors: int = 8, standardize: bool = True,\n",
    "                  map_type: str = 'full', eps: float = 1e-12) -> Dict[str, Any]:\n",
    "    if standardize:\n",
    "        X_tr, mux, sdx = _zscore(X_tr); X_te = (X_te - mux) / sdx\n",
    "        Y_tr, muy, sdy = _zscore(Y_tr); Y_te = (Y_te - muy) / sdy\n",
    "    fa = FactorAnalysis(n_components=int(n_factors), rotation=None)\n",
    "    Zx_tr = fa.fit_transform(X_tr); Zx_te = fa.transform(X_te)\n",
    "    Zy_tr = fa.transform(Y_tr);    Zy_te = fa.transform(Y_te)\n",
    "    if map_type == 'diag':\n",
    "        gains = []\n",
    "        for i in range(Zx_tr.shape[1]):\n",
    "            den = float(np.dot(Zy_tr[:, i], Zy_tr[:, i])) + eps\n",
    "            gains.append(float(np.dot(Zy_tr[:, i], Zx_tr[:, i])) / den)\n",
    "        A = np.diag(gains)\n",
    "    elif map_type == 'full':\n",
    "        A, *_ = lstsq(Zy_tr, Zx_tr, rcond=None)\n",
    "    else:\n",
    "        raise ValueError('map_type must be \"diag\" or \"full\"')\n",
    "    Zx_hat_te = Zy_te @ A\n",
    "    den = float(np.sum((Zx_te - Zx_te.mean(0)) ** 2))\n",
    "    ve_lat = 0.0 if den <= eps else float(np.clip(1.0 - np.sum((Zx_te - Zx_hat_te) ** 2) / den, 0.0, 1.0))\n",
    "    # per-factor test correlations\n",
    "    corrs = []\n",
    "    for i in range(Zx_te.shape[1]):\n",
    "        xi, yi = Zx_te[:, i], Zx_hat_te[:, i]\n",
    "        si = np.std(xi); sj = np.std(yi)\n",
    "        corrs.append(0.0 if si < 1e-12 or sj < 1e-12 else float(np.corrcoef(xi, yi)[0, 1]))\n",
    "    return {'ve_latent_test': ve_lat, 'corrs_latent_test': np.array(corrs), 'A': A, 'fa': fa}\n",
    "\n",
    "# ---------- Main benchmark ----------\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    fa_ve_mean: float\n",
    "    fa_ve_std: float\n",
    "    cca_ve_mean: float\n",
    "    cca_ve_std: float\n",
    "    cca_corr_test_mean: np.ndarray\n",
    "    cca_corr_thresh95: np.ndarray\n",
    "    rrr_best_rank: int\n",
    "    rrr_ve_by_rank: Dict[int, float]\n",
    "    plots: Dict[str, Any]    # matplotlib Figure objects\n",
    "\n",
    "def population_latent_benchmark(X: np.ndarray, Y: np.ndarray, trial_ids: np.ndarray,\n",
    "                                *, n_splits: int = 5,\n",
    "                                fa_factors: int = 8,\n",
    "                                cca_components: int = 10,\n",
    "                                rrr_ranks: Iterable[int] = (1, 2, 3, 5, 8, 12, 16),\n",
    "                                shuffle_reps: int = 100,\n",
    "                                rng: int = 0) -> BenchmarkResult:\n",
    "    X = np.asarray(X, float); Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, 'X and Y must have same shape'\n",
    "    groups = np.asarray(trial_ids)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    fa_ves, cca_ves, cca_corrs_list, cca_thresh_list = [], [], [], []\n",
    "\n",
    "    # RRR CV across ranks (once; GroupKFold inside)\n",
    "    rrr = rrr_cv(X, Y, groups=groups, ranks=rrr_ranks, n_splits=n_splits, rng=rng)\n",
    "\n",
    "    # Fold loop for FA & CCA\n",
    "    for fold, (tr_idx, te_idx) in enumerate(gkf.split(X, groups=groups)):\n",
    "        Xtr, Ytr = X[tr_idx], Y[tr_idx]\n",
    "        Xte, Yte = X[te_idx], Y[te_idx]\n",
    "\n",
    "        # FA latent VE\n",
    "        fa_out = fa_train_test(Xtr, Ytr, Xte, Yte, n_factors=fa_factors, standardize=True, map_type='full')\n",
    "        fa_ves.append(fa_out['ve_latent_test'])\n",
    "\n",
    "        # CCA subspace VE + test corr spectrum + shuffle baseline\n",
    "        cca_out = cca_train_test(Xtr, Ytr, Xte, Yte, n_components=cca_components, standardize=True, map_type='diag')\n",
    "        cca_ves.append(cca_out['ve_test'])\n",
    "\n",
    "        corrs_real, thresh = cca_shuffle_test_on_test(Xte, Yte, cca_out['cca'],\n",
    "                                                      n_components=cca_components,\n",
    "                                                      n_shuffles=shuffle_reps, rng=rng + fold)\n",
    "        cca_corrs_list.append(corrs_real)\n",
    "        cca_thresh_list.append(thresh)\n",
    "\n",
    "    fa_ve_mean, fa_ve_std = float(np.mean(fa_ves)), float(np.std(fa_ves))\n",
    "    cca_ve_mean, cca_ve_std = float(np.mean(cca_ves)), float(np.std(cca_ves))\n",
    "    cca_corr_test_mean = np.mean(np.vstack(cca_corrs_list), axis=0)\n",
    "    cca_corr_thresh95 = np.mean(np.vstack(cca_thresh_list), axis=0)\n",
    "\n",
    "    # ---------- Plots ----------\n",
    "    figs: Dict[str, Any] = {}\n",
    "\n",
    "    # 1) CCA spectrum vs shuffle\n",
    "    fig1 = plt.figure(figsize=(4, 3))\n",
    "    x = np.arange(1, len(cca_corr_test_mean) + 1)\n",
    "    plt.plot(x, cca_corr_test_mean, marker='o', label='Test corr')\n",
    "    plt.plot(x, cca_corr_thresh95, linestyle='--', label='Shuffle 95%')\n",
    "    plt.xlabel('Canonical component')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.title('CCA test spectrum')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    figs['cca_spectrum'] = fig1\n",
    "\n",
    "    # 2) VE summary bars\n",
    "    fig2 = plt.figure(figsize=(4, 3))\n",
    "    names = ['FA-latent VE', 'CCA-VE', f'RRR-VE@r={rrr[\"best_rank\"]}']\n",
    "    vals = [fa_ve_mean, cca_ve_mean, rrr['ve_by_rank'][rrr['best_rank']]]\n",
    "    y = np.arange(len(names))\n",
    "    plt.bar(y, vals)\n",
    "    plt.xticks(y, names, rotation=20)\n",
    "    plt.ylim(0, max(0.01, max(vals) * 1.2))\n",
    "    plt.ylabel('Variance explained')\n",
    "    plt.title('Population VE summary')\n",
    "    plt.tight_layout()\n",
    "    figs['ve_summary'] = fig2\n",
    "\n",
    "    # 3) RRR VE vs rank\n",
    "    fig3 = plt.figure(figsize=(4, 3))\n",
    "    rs = sorted(rrr['ve_by_rank'].keys())\n",
    "    vrs = [rrr['ve_by_rank'][r] for r in rs]\n",
    "    plt.plot(rs, vrs, marker='o')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('CV VE')\n",
    "    plt.title('RRR VE vs rank')\n",
    "    plt.tight_layout()\n",
    "    figs['rrr_curve'] = fig3\n",
    "\n",
    "    return BenchmarkResult(\n",
    "        fa_ve_mean=fa_ve_mean,\n",
    "        fa_ve_std=fa_ve_std,\n",
    "        cca_ve_mean=cca_ve_mean,\n",
    "        cca_ve_std=cca_ve_std,\n",
    "        cca_corr_test_mean=cca_corr_test_mean,\n",
    "        cca_corr_thresh95=cca_corr_thresh95,\n",
    "        rrr_best_rank=rrr['best_rank'],\n",
    "        rrr_ve_by_rank=rrr['ve_by_rank'],\n",
    "        plots=figs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = observed (samples × neurons)\n",
    "# X_hat = predictions (same shape)\n",
    "# trial_ids = your per-row trial/segment ids\n",
    "res = population_latent_benchmark(X, X_hat, trial_ids,\n",
    "                                  n_splits=5,\n",
    "                                  fa_factors=8,\n",
    "                                  cca_components=10,\n",
    "                                  rrr_ranks=(1,2,3,5,8,12,16),\n",
    "                                  shuffle_reps=100,\n",
    "                                  rng=0)\n",
    "\n",
    "print('FA-latent VE (mean±sd):', res.fa_ve_mean, res.fa_ve_std)\n",
    "print('CCA-VE (mean±sd):      ', res.cca_ve_mean, res.cca_ve_std)\n",
    "print('RRR best rank:', res.rrr_best_rank)\n",
    "print('RRR VE by rank:', res.rrr_ve_by_rank)\n",
    "\n",
    "# show or save figures\n",
    "for name, fig in res.plots.items():\n",
    "    fig.show()           # or fig.savefig(f'{name}.png', dpi=200, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCA latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def _zscore(A, eps=1e-12):\n",
    "    mu = np.nanmean(A, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(A, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (A - mu) / sd, mu, sd\n",
    "\n",
    "def _sem(a, axis=0):\n",
    "    a = np.asarray(a, float)\n",
    "    n = np.sum(np.isfinite(a), axis=axis)\n",
    "    n = np.maximum(n, 1)\n",
    "    return np.nanstd(a, axis=axis) / np.sqrt(n)\n",
    "\n",
    "def _smooth_1d(x, k=1):\n",
    "    if k <= 1: \n",
    "        return x\n",
    "    k = int(k)\n",
    "    kern = np.ones(k) / k\n",
    "    return np.convolve(x, kern, mode='same')\n",
    "\n",
    "def visualize_shared_manifold(\n",
    "    X, Y, trial_ids, time_idx, cond=None, *,\n",
    "    n_components=3,\n",
    "    n_splits=5,\n",
    "    which_fold=0,            # pick which fold to visualize\n",
    "    standardize=True,\n",
    "    smooth_bins=1,           # boxcar smoothing across time for the mean curves\n",
    "    colors=('C0','C1'),      # (observed, predicted)\n",
    "    show_phase_plane=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit CCA on TRAIN trials, project TEST trials into canonical space, and plot\n",
    "    observed vs predicted trajectories (mean±SEM) over time for the first components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : arrays (samples × neurons), same shape\n",
    "    trial_ids : array (samples,)\n",
    "    time_idx : array (samples,) time within aligned window (e.g., -1.5..+1.5s or bin index)\n",
    "    cond : optional array-like (samples,), categorical condition label per sample\n",
    "    n_components : # of canonical components to visualize\n",
    "    n_splits : GroupKFold splits by trial_ids\n",
    "    which_fold : index of fold to visualize (0..n_splits-1)\n",
    "    standardize : z-score each view prior to CCA\n",
    "    smooth_bins : boxcar width for smoothing mean curves (in bins)\n",
    "    colors : tuple for (observed, predicted) lines\n",
    "    show_phase_plane : also plot Comp1 vs Comp2 phase-plane (obs vs pred)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, \"X and Y must have same shape\"\n",
    "\n",
    "    # group-wise split by trials\n",
    "    trial_ids = np.asarray(trial_ids)\n",
    "    time_idx = np.asarray(time_idx)\n",
    "    if cond is None:\n",
    "        cond = np.array(['all'] * len(time_idx))\n",
    "    else:\n",
    "        cond = np.asarray(cond)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    splits = list(gkf.split(X, groups=trial_ids))\n",
    "    tr_idx, te_idx = splits[which_fold]\n",
    "\n",
    "    Xtr, Ytr = X[tr_idx], Y[tr_idx]\n",
    "    Xte, Yte = X[te_idx], Y[te_idx]\n",
    "    trial_te = trial_ids[te_idx]\n",
    "    time_te  = time_idx[te_idx]\n",
    "    cond_te  = cond[te_idx]\n",
    "\n",
    "    # standardize each view\n",
    "    if standardize:\n",
    "        Xtr, mux, sdx = _zscore(Xtr)\n",
    "        Ytr, muy, sdy = _zscore(Ytr)\n",
    "        Xte = (Xte - mux) / sdx\n",
    "        Yte = (Yte - muy) / sdy\n",
    "\n",
    "    # fit CCA on TRAIN only\n",
    "    k = int(n_components)\n",
    "    cca = CCA(n_components=k, max_iter=1000)\n",
    "    Xc_tr, Yc_tr = cca.fit_transform(Xtr, Ytr)\n",
    "    Xc_te, Yc_te = cca.transform(Xte, Yte)\n",
    "\n",
    "    # package canonical coordinates for TEST into a tidy DataFrame\n",
    "    dfs = []\n",
    "    for view_name, Z in [('obs', Xc_te), ('pred', Yc_te)]:\n",
    "        df = pd.DataFrame({\n",
    "            'trial': trial_te,\n",
    "            'time': time_te,\n",
    "            'cond': cond_te,\n",
    "        })\n",
    "        for i in range(k):\n",
    "            df[f'can{i+1}'] = Z[:, i]\n",
    "        df['view'] = view_name\n",
    "        dfs.append(df)\n",
    "    df_all = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # aggregate mean ± SEM over trials at each time, by condition & view\n",
    "    # we align on time bins; ensure numeric sorting\n",
    "    df_all = df_all.sort_values(['cond', 'view', 'time'])\n",
    "    results = {}\n",
    "    for c in np.unique(cond_te):\n",
    "        results[c] = {}\n",
    "        dsub = df_all[df_all['cond'] == c]\n",
    "        # group by time & view, compute mean/sem per component\n",
    "        gb = dsub.groupby(['view', 'time'])\n",
    "        means = gb.mean(numeric_only=True)\n",
    "        sems  = gb.apply(lambda g: pd.Series({\n",
    "            **{f'can{i+1}': _sem(g[f'can{i+1}'].values) for i in range(k)}\n",
    "        }))\n",
    "        # pivot to have time index, separate views\n",
    "        means = means.reset_index().pivot(index='time', columns='view', values=[f'can{i+1}' for i in range(k)])\n",
    "        sems  = sems.reset_index().pivot(index='time', columns='view', values=[f'can{i+1}' for i in range(k)])\n",
    "        # sort by time\n",
    "        means = means.sort_index()\n",
    "        sems  = sems.sort_index()\n",
    "        results[c]['mean'] = means\n",
    "        results[c]['sem']  = sems\n",
    "\n",
    "    # --- plotting ---\n",
    "    figs = []\n",
    "    # 1) Trajectories for each canonical component\n",
    "    for i in range(k):\n",
    "        fig = plt.figure(figsize=(5, 3.2))\n",
    "        ax = plt.gca()\n",
    "        for j, c in enumerate(results.keys()):\n",
    "            means = results[c]['mean'][(f'can{i+1}',)]\n",
    "            sems  = results[c]['sem'][(f'can{i+1}',)]\n",
    "            for view_idx, view in enumerate(['obs','pred']):\n",
    "                y = means[view].values\n",
    "                s = sems[view].values\n",
    "                if smooth_bins > 1:\n",
    "                    y = _smooth_1d(y, smooth_bins)\n",
    "                    s = _smooth_1d(s, smooth_bins)\n",
    "                t = means.index.values\n",
    "                ax.plot(t, y, label=f'{c} — {view}', \n",
    "                        linestyle='-' if view=='obs' else '--',\n",
    "                        color=colors[view_idx])\n",
    "                ax.fill_between(t, y - s, y + s, alpha=0.15, color=colors[view_idx])\n",
    "        ax.set_title(f'Canonical component {i+1} (fold {which_fold+1}/{n_splits})')\n",
    "        ax.set_xlabel('Time (aligned bins)')\n",
    "        ax.set_ylabel('Canonical coordinate')\n",
    "        ax.legend(ncol=2, fontsize=8)\n",
    "        ax.axvline(0, color='k', linewidth=1, alpha=0.2)\n",
    "        ax.grid(alpha=0.2, linestyle=':')\n",
    "        plt.tight_layout()\n",
    "        figs.append(fig)\n",
    "\n",
    "    # 2) Phase-plane plot (Comp1 vs Comp2)\n",
    "    if show_phase_plane and k >= 2:\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        ax = plt.gca()\n",
    "        for j, c in enumerate(results.keys()):\n",
    "            m1 = results[c]['mean'][('can1',)]\n",
    "            m2 = results[c]['mean'][('can2',)]\n",
    "            t = m1.index.values\n",
    "            for view_idx, view in enumerate(['obs','pred']):\n",
    "                x = m1[view].values\n",
    "                y = m2[view].values\n",
    "                if smooth_bins > 1:\n",
    "                    x = _smooth_1d(x, smooth_bins)\n",
    "                    y = _smooth_1d(y, smooth_bins)\n",
    "                ax.plot(x, y, label=f'{c} — {view}',\n",
    "                        linestyle='-' if view=='obs' else '--',\n",
    "                        color=colors[view_idx])\n",
    "                # mark time zero if present\n",
    "                if 0 in t:\n",
    "                    idx0 = np.where(t==0)[0]\n",
    "                    if len(idx0): \n",
    "                        ax.scatter(x[idx0[0]], y[idx0[0]], s=30, color=colors[view_idx])\n",
    "        ax.set_xlabel('Can1')\n",
    "        ax.set_ylabel('Can2')\n",
    "        ax.set_title(f'Phase plane (fold {which_fold+1}/{n_splits})')\n",
    "        ax.legend(ncol=2, fontsize=8)\n",
    "        ax.grid(alpha=0.2, linestyle=':')\n",
    "        plt.tight_layout()\n",
    "        figs.append(fig)\n",
    "\n",
    "    return {'figs': figs, 'cca': cca}\n",
    "\n",
    "# X, X_hat: (samples, neurons)\n",
    "# trial_ids: trial/segment identifier per row (for GroupKFold)\n",
    "# time_idx: bin index (e.g., -30..+30) or seconds relative to event\n",
    "# cond: optional labels (e.g., visibility state or stop outcome)\n",
    "out = visualize_shared_manifold(\n",
    "    X, X_hat, trial_ids, data['new_bin'], cond=None,\n",
    "    n_components=3, n_splits=5, which_fold=0,\n",
    "    standardize=True, smooth_bins=3, show_phase_plane=True\n",
    ")\n",
    "# Show or save figures:\n",
    "for i, fig in enumerate(out['figs'], 1):\n",
    "    fig.show()  # or fig.savefig(f'shared_manifold_{i}.png', dpi=200, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def population_ve_cca(X, Y, n_components=10):\n",
    "    \"\"\"\n",
    "    Canonical correlation analysis between observed (X) and predicted (Y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (samples, neurons)\n",
    "    Y : array, shape (samples, neurons) -- same shape as X\n",
    "    n_components : number of canonical pairs to compute\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrs : array of canonical correlations (length n_components)\n",
    "    mean_corr : average canonical correlation across components\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, 'X and Y must have same shape'\n",
    "\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    Xc, Yc = cca.fit_transform(X, Y)\n",
    "    corrs = [np.corrcoef(Xc[:,i], Yc[:,i])[0,1] for i in range(Xc.shape[1])]\n",
    "    return np.array(corrs), np.mean(corrs)\n",
    "\n",
    "corrs, mean_corr = population_ve_cca(X, X_hat, n_components=10)\n",
    "print('Canonical correlations:', corrs)\n",
    "print('Mean correlation:', mean_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-pager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# --- helpers ---\n",
    "def _zscore(A, eps=1e-12):\n",
    "    mu = np.nanmean(A, 0, keepdims=True)\n",
    "    sd = np.nanstd(A, 0, keepdims=True); sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (A - mu)/sd, mu, sd\n",
    "\n",
    "def _sem(a, axis=0):\n",
    "    n = np.sum(np.isfinite(a), axis=axis)\n",
    "    n = np.maximum(n, 1)\n",
    "    return np.nanstd(a, axis=axis) / np.sqrt(n)\n",
    "\n",
    "def _smooth_1d(x, k=1):\n",
    "    if k <= 1: return x\n",
    "    k = int(k)\n",
    "    w = np.ones(k)/k\n",
    "    return np.convolve(x, w, mode='same')\n",
    "\n",
    "def cumulative_shared_variance(corrs):\n",
    "    \"\"\"Cumulative 'shared signal' (sum of corr^2) normalized to 1.\"\"\"\n",
    "    pwr = np.cumsum(np.square(np.asarray(corrs, float)))\n",
    "    tot = pwr[-1] if pwr[-1] > 0 else 1.0\n",
    "    return pwr / tot\n",
    "\n",
    "# --- one-pager assembly ---\n",
    "def save_one_pager(\n",
    "    benchmark_result,                # result from population_latent_benchmark(...)\n",
    "    cca_corr_test_mean, cca_thresh95,\n",
    "    traj_figs, rrr_rank_curve_fig, ve_summary_fig, cca_spectrum_fig,\n",
    "    out_path='one_pager.png'\n",
    "):\n",
    "    \"\"\"\n",
    "    Lay out 3–4 plots into a single page and save.\n",
    "    Pass the figure objects produced by your benchmark + enhanced manifold.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8), dpi=200)\n",
    "    gs = GridSpec(2, 3, figure=fig, height_ratios=[1,1.1])\n",
    "\n",
    "    # Top-left: CCA spectrum vs shuffle\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    ax1.imshow(np.zeros((1,1)), visible=False)  # placeholder; we'll draw onto it via the provided fig\n",
    "    plt.close(cca_spectrum_fig)  # prevent extra window\n",
    "    # replot quickly:\n",
    "    x = np.arange(1, len(cca_corr_test_mean)+1)\n",
    "    ax1.plot(x, cca_corr_test_mean, marker='o', label='Test corr')\n",
    "    ax1.plot(x, cca_thresh95, ls='--', label='Shuffle 95%')\n",
    "    ax1.set_title('CCA test spectrum'); ax1.set_xlabel('Component'); ax1.set_ylabel('Corr'); ax1.legend()\n",
    "\n",
    "    # Top-middle: cumulative shared variance\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    cum = cumulative_shared_variance(cca_corr_test_mean)\n",
    "    ax2.plot(x, cum, marker='o'); ax2.set_ylim(0,1.02)\n",
    "    ax2.set_title('Cumulative shared signal'); ax2.set_xlabel('Component'); ax2.set_ylabel('Cumulative (corr^2)')\n",
    "\n",
    "    # Top-right: VE summary bars\n",
    "    ax3 = fig.add_subplot(gs[0,2])\n",
    "    names = ['FA-latent VE', 'CCA-VE', f'RRR-VE@r={benchmark_result.rrr_best_rank}']\n",
    "    vals  = [benchmark_result.fa_ve_mean, benchmark_result.cca_ve_mean,\n",
    "             benchmark_result.rrr_ve_by_rank[benchmark_result.rrr_best_rank]]\n",
    "    ax3.bar(np.arange(len(names)), vals)\n",
    "    ax3.set_xticks(np.arange(len(names))); ax3.set_xticklabels(names, rotation=15)\n",
    "    ax3.set_ylim(0, max(0.01, max(vals)*1.2)); ax3.set_ylabel('Variance explained')\n",
    "    ax3.set_title('Population VE summary')\n",
    "\n",
    "    # Bottom: 2 manifold panels (Can1, Can2) + RRR curve\n",
    "    # Insert the first two trajectory figs by replotting (for simplicity we draw lines again is better, but here we just call .canvas to bitmap)\n",
    "    ax4 = fig.add_subplot(gs[1,0])\n",
    "    ax5 = fig.add_subplot(gs[1,1])\n",
    "    ax6 = fig.add_subplot(gs[1,2])\n",
    "\n",
    "    # For ax4/ax5, we’ll just draw titles and note to export separate detailed figs;\n",
    "    # if you prefer, render traj_figs to images and imshow here.\n",
    "\n",
    "    ax4.set_title('Manifold traj: Can1 (see detailed fig)')\n",
    "    ax4.axis('off')\n",
    "    ax5.set_title('Manifold traj: Can2 (see detailed fig)')\n",
    "    ax5.axis('off')\n",
    "\n",
    "    # RRR curve (copy from provided fig)\n",
    "    plt.close(rrr_rank_curve_fig);  # avoid extra window\n",
    "    # If you kept x,y from your RRR plot, replot here; otherwise leave placeholder:\n",
    "    ax6.set_title('RRR VE vs rank'); ax6.set_xlabel('Rank'); ax6.set_ylabel('CV VE')\n",
    "\n",
    "    fig.suptitle('Population latent benchmark — one-pager', fontsize=13)\n",
    "    fig.tight_layout(rect=[0,0,1,0.96])\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# --- small helpers ---\n",
    "def _zscore(A, eps=1e-12):\n",
    "    mu = np.nanmean(A, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(A, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (A - mu)/sd, mu, sd\n",
    "\n",
    "def _sem(a, axis=0):\n",
    "    a = np.asarray(a, float)\n",
    "    n = np.sum(np.isfinite(a), axis=axis)\n",
    "    n = np.maximum(n, 1)\n",
    "    return np.nanstd(a, axis=axis)/np.sqrt(n)\n",
    "\n",
    "def _smooth_1d(x, k=1):\n",
    "    if k <= 1: return x\n",
    "    k = int(k)\n",
    "    w = np.ones(k)/k\n",
    "    return np.convolve(x, w, mode='same')\n",
    "\n",
    "def _clip_quantile(y, lo=0.01, hi=0.99):\n",
    "    if lo is None or hi is None: return y\n",
    "    ql, qh = np.nanpercentile(y, [lo*100, hi*100])\n",
    "    return np.clip(y, ql, qh)\n",
    "\n",
    "def plot_shared_manifold_enhanced(\n",
    "    X, Y, trial_ids, time_idx, cond=None, *,\n",
    "    n_components=3, n_splits=5, which_fold=0,\n",
    "    standardize=True, smooth_bins=1,\n",
    "    # --- overlays ---\n",
    "    overlay_mode='separate',        # 'overlay' | 'separate'\n",
    "    n_trials_overlay=12, alpha_trials=0.22, linewidth_trials=0.7,\n",
    "    overlay_norm='zscore_to_sem',   # 'none' | 'zscore' | 'zscore_to_sem'\n",
    "    clip_quantiles=(0.01, 0.99),    # quantile clip for overlays (None to disable)\n",
    "    # --- y-limit control for MEAN panels ---\n",
    "    ylimit_mode='mean_sem_3sd',     # 'auto' | 'mean_sem_3sd' | ('fixed', (ymin,ymax))\n",
    "    # --- colors ---\n",
    "    cmap_name='viridis', colors=('C0','C1'),\n",
    "    show_phase_plane=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train CCA on train folds; plot test-fold shared-manifold trajectories.\n",
    "\n",
    "    Panels per canonical component:\n",
    "      - If overlay_mode='overlay': single panel with mean±SEM + faint single trials.\n",
    "      - If overlay_mode='separate': two panels: Left=mean±SEM, Right=single trials.\n",
    "    Also produces a time-colored phase-plane (Can1 vs Can2).\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float); Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, \"X and Y must have same shape\"\n",
    "    trial_ids = np.asarray(trial_ids); time_idx = np.asarray(time_idx)\n",
    "    if cond is None: cond = np.array(['all']*len(time_idx))\n",
    "    else: cond = np.asarray(cond)\n",
    "\n",
    "    # Split by trial\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    splits = list(gkf.split(X, groups=trial_ids))\n",
    "    tr_idx, te_idx = splits[which_fold]\n",
    "    Xtr, Ytr = X[tr_idx], Y[tr_idx]\n",
    "    Xte, Yte = X[te_idx], Y[te_idx]\n",
    "    trial_te = trial_ids[te_idx]; time_te = time_idx[te_idx]; cond_te = cond[te_idx]\n",
    "\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        Xtr, mux, sdx = _zscore(Xtr); Xte = (Xte - mux)/sdx\n",
    "        Ytr, muy, sdy = _zscore(Ytr); Yte = (Yte - muy)/sdy\n",
    "\n",
    "    # CCA\n",
    "    k = int(n_components)\n",
    "    cca = CCA(n_components=k, max_iter=1000)\n",
    "    cca.fit(Xtr, Ytr)\n",
    "    Xc_te, Yc_te = cca.transform(Xte, Yte)\n",
    "\n",
    "    # Tidy DF (test only)\n",
    "    dfs = []\n",
    "    for view, Z in [('obs', Xc_te), ('pred', Yc_te)]:\n",
    "        d = pd.DataFrame({'trial': trial_te, 'time': time_te, 'cond': cond_te, 'view': view})\n",
    "        for i in range(k): d[f'can{i+1}'] = Z[:, i]\n",
    "        dfs.append(d)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    figs = []\n",
    "\n",
    "    # === Per-component panels ===\n",
    "    for ic in range(k):\n",
    "        comp_col = f'can{ic+1}'\n",
    "\n",
    "        # mean/sem per time\n",
    "        dmean = (df.groupby(['view','time'])\n",
    "                   .mean(numeric_only=True)[[comp_col]]\n",
    "                   .rename(columns={comp_col:'mean'})\n",
    "                 ).reset_index()\n",
    "        dsem  = (df.groupby(['view','time'])\n",
    "                   .apply(lambda g: pd.Series({'sem': _sem(g[comp_col].values)}))\n",
    "                 ).reset_index()\n",
    "\n",
    "        # Pivot for plotting\n",
    "        m_obs = dmean[dmean.view=='obs'][['time','mean']].sort_values('time').values\n",
    "        m_pred= dmean[dmean.view=='pred'][['time','mean']].sort_values('time').values\n",
    "        s_obs = dsem [dsem .view=='obs'][['time','sem']].sort_values('time').values\n",
    "        s_pred= dsem [dsem .view=='pred'][['time','sem']].sort_values('time').values\n",
    "\n",
    "        t = m_obs[:,0]\n",
    "        mu_obs, mu_pred = _smooth_1d(m_obs[:,1], smooth_bins), _smooth_1d(m_pred[:,1], smooth_bins)\n",
    "        se_obs, se_pred = _smooth_1d(s_obs[:,1], smooth_bins), _smooth_1d(s_pred[:,1], smooth_bins)\n",
    "\n",
    "        if overlay_mode == 'separate':\n",
    "            fig, (ax_mean, ax_trials) = plt.subplots(1, 2, figsize=(9, 3.2), sharey=False)\n",
    "        else:  # overlay\n",
    "            fig, ax_mean = plt.subplots(1, 1, figsize=(5.4, 3.2))\n",
    "            ax_trials = ax_mean\n",
    "\n",
    "        # --- MEAN ± SEM panel ---\n",
    "        ax_mean.plot(t, mu_obs, color=colors[0], label='all — obs', linestyle='-')\n",
    "        ax_mean.fill_between(t, mu_obs-se_obs, mu_obs+se_obs, color=colors[0], alpha=0.18)\n",
    "        ax_mean.plot(t, mu_pred, color=colors[1], label='all — pred', linestyle='--')\n",
    "        ax_mean.fill_between(t, mu_pred-se_pred, mu_pred+se_pred, color=colors[1], alpha=0.18)\n",
    "        ax_mean.axvline(0, color='k', lw=1, alpha=0.25)\n",
    "        ax_mean.grid(alpha=0.2, ls=':')\n",
    "        ax_mean.set_title(f'Canonical component {ic+1} (fold {which_fold+1}/{n_splits})')\n",
    "        ax_mean.set_xlabel('Time (aligned bins)')\n",
    "        ax_mean.set_ylabel('Canonical coordinate')\n",
    "        ax_mean.legend(ncol=2, fontsize=8)\n",
    "\n",
    "        # y-limit logic for MEAN panel\n",
    "        if isinstance(ylimit_mode, tuple) and len(ylimit_mode) == 2 and ylimit_mode[0] == 'fixed':\n",
    "            ax_mean.set_ylim(*ylimit_mode[1])\n",
    "        elif ylimit_mode == 'mean_sem_3sd':\n",
    "            # robust limits from mean ± (3 * pooled SEM)\n",
    "            pooled = np.maximum(se_obs, 0) + np.maximum(se_pred, 0)\n",
    "            lo = np.nanmin([mu_obs - 3*pooled, mu_pred - 3*pooled])\n",
    "            hi = np.nanmax([mu_obs + 3*pooled, mu_pred + 3*pooled])\n",
    "            pad = 0.05*(hi - lo + 1e-9)\n",
    "            ax_mean.set_ylim(lo - pad, hi + pad)\n",
    "        else:\n",
    "            pass  # 'auto'\n",
    "\n",
    "        # --- SINGLE-TRIAL panel (or overlays) ---\n",
    "        # pick subset of trials for speed/clarity\n",
    "        rng = np.random.default_rng(0)\n",
    "        all_trials = df['trial'].unique()\n",
    "        pick = all_trials if len(all_trials) <= n_trials_overlay else rng.choice(all_trials, n_trials_overlay, replace=False)\n",
    "\n",
    "        for view, col in [('obs','C0'), ('pred','C1')]:\n",
    "            for tr in pick:\n",
    "                one = df[(df.trial==tr) & (df.view==view)][['time', comp_col]].sort_values('time').values\n",
    "                tt, yy = one[:,0], one[:,1].copy()\n",
    "                # quantile clip to tame rare spikes\n",
    "                if clip_quantiles is not None:\n",
    "                    yy = _clip_quantile(yy, *clip_quantiles)\n",
    "                # per-trial normalization\n",
    "                if overlay_norm in ('zscore', 'zscore_to_sem'):\n",
    "                    sd = np.nanstd(yy); sd = 1.0 if sd < 1e-12 else sd\n",
    "                    yy = (yy - np.nanmean(yy)) / sd\n",
    "                    if overlay_norm == 'zscore_to_sem':\n",
    "                        # rescale to sit inside the mean ± SEM band of the corresponding view\n",
    "                        if view == 'obs':\n",
    "                            yy = yy * np.nanmean(se_obs)\n",
    "                        else:\n",
    "                            yy = yy * np.nanmean(se_pred)\n",
    "                yy = _smooth_1d(yy, smooth_bins)\n",
    "                ax_trials.plot(tt, yy, color=col, alpha=alpha_trials, lw=linewidth_trials)\n",
    "\n",
    "        if overlay_mode == 'separate':\n",
    "            ax_trials.set_title('Single trials')\n",
    "            ax_trials.set_xlabel('Time (aligned bins)')\n",
    "            ax_trials.set_ylabel('Canonical coordinate (scaled)')\n",
    "            ax_trials.grid(alpha=0.2, ls=':')\n",
    "            ax_trials.axvline(0, color='k', lw=1, alpha=0.25)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        figs.append(fig)\n",
    "\n",
    "    # === Phase-plane (Can1 vs Can2), time-colored ===\n",
    "    if show_phase_plane and k >= 2:\n",
    "        fig = plt.figure(figsize=(4.4, 4.4)); ax = plt.gca()\n",
    "        norm = Normalize(vmin=np.min(time_te), vmax=np.max(time_te))\n",
    "        cmap = cm.get_cmap(cmap_name)\n",
    "        # mean trajectories only (clean)\n",
    "        for view, ls, base_color in [('obs','-','C0'), ('pred','--','C1')]:\n",
    "            sub = df[df['view']==view]\n",
    "            m1 = (sub.groupby('time')['can1'].mean()).sort_index()\n",
    "            m2 = (sub.groupby('time')['can2'].mean()).sort_index()\n",
    "            t  = m1.index.values\n",
    "            x  = _smooth_1d(m1.values, smooth_bins)\n",
    "            y  = _smooth_1d(m2.values, smooth_bins)\n",
    "            for i in range(len(t)-1):\n",
    "                ax.plot(x[i:i+2], y[i:i+2], ls=ls, color=cmap(norm(t[i])))\n",
    "            if 0 in t:\n",
    "                i0 = np.where(t==0)[0][0]\n",
    "                ax.scatter(x[i0], y[i0], s=30, color=base_color, edgecolor='k', zorder=3)\n",
    "        sm = cm.ScalarMappable(norm=norm, cmap=cmap); sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Time (aligned bins)')\n",
    "        ax.set_xlabel('Can1'); ax.set_ylabel('Can2')\n",
    "        ax.set_title(f'Phase plane (fold {which_fold+1}/{n_splits})')\n",
    "        ax.grid(alpha=0.2, ls=':')\n",
    "        plt.tight_layout()\n",
    "        figs.append(fig)\n",
    "\n",
    "    return {'figs': figs, 'cca': cca, 'df': df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_idx = data['new_bin']\n",
    "condition_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Run your benchmark (from earlier code) to get VE bars/CCA spectrum/RRR curve:\n",
    "res = population_latent_benchmark(X, X_hat, trial_ids,\n",
    "                                  n_splits=5, fa_factors=8, cca_components=10,\n",
    "                                  rrr_ranks=(1,2,3,5,8,12,16), shuffle_reps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Make enhanced shared-manifold plots for a fold (and reuse the fitted CCA df if needed):\n",
    "traj_figs, cca_obj, df_can = plot_shared_manifold_enhanced(\n",
    "    X, X_hat, trial_ids, time_idx, cond=condition_labels,\n",
    "    n_components=3, n_splits=5, which_fold=0,\n",
    "    smooth_bins=3, n_trials_overlay=40, show_phase_plane=True\n",
    ")\n",
    "\n",
    "# 3) Save a one-pager (PNG or PDF)\n",
    "out_path = save_one_pager(\n",
    "    benchmark_result=res,\n",
    "    cca_corr_test_mean=res.cca_corr_test_mean,\n",
    "    cca_thresh95=res.cca_corr_thresh95,\n",
    "    traj_figs=traj_figs,\n",
    "    rrr_rank_curve_fig=res.plots['rrr_curve'],\n",
    "    ve_summary_fig=res.plots['ve_summary'],\n",
    "    cca_spectrum_fig=res.plots['cca_spectrum'],\n",
    "    out_path='one_pager.png'  # or 'one_pager.pdf'\n",
    ")\n",
    "print('Saved:', out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idx = 4\n",
    "\n",
    "rebinned_y_var['rel_time'] = rebinned_y_var['time'] - rebinned_y_var['new_seg_start_time']\n",
    "\n",
    "for event_id in range(40, 52):\n",
    "    # If your GLM used offset_log = np.log(exposure_s), you can omit exposure_s:\n",
    "    plot_spikes.plot_observed_vs_predicted_event(\n",
    "        binned_feats_sc=df_X,\n",
    "        binned_spikes=df_Y,\n",
    "        meta_used=rebinned_y_var,\n",
    "        offset_log=offset_log,\n",
    "        model_res=report['results'][cluster_idx],   # GLM for cluster 0\n",
    "        cluster_idx=cluster_idx,\n",
    "        seg_id=event_id,\n",
    "        time_col='rel_time',\n",
    "        seg_col='new_segment'\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tuning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_tuning_func)\n",
    "\n",
    "\n",
    "\n",
    "for cluster_id in df_Y.columns:\n",
    "    \n",
    "    print('==========================================')\n",
    "    model_res = report['results'][cluster_id]\n",
    "    \n",
    "    for var in ['nxt_in_memory', 'cur_ff_distance', 'log_cur_ff_distance']:\n",
    "\n",
    "        exposure_s = np.exp(offset_log)\n",
    "        # make empirical curve\n",
    "        tc_emp = plot_tuning_func.empirical_tuning_curve(\n",
    "            binned_spikes=df_Y[cluster_id].to_numpy(),\n",
    "            predictor_vals=df_X[var].to_numpy(),\n",
    "            exposure_s=exposure_s,\n",
    "            nbins=20\n",
    "        )\n",
    "\n",
    "        # just empirical\n",
    "        plot_tuning_func.plot_tuning_with_ci(tc_emp, title='Distance → rate (empirical)',\n",
    "                    kind='line', ci_style='band', show_counts=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # # make GLM curve\n",
    "        # tc_glm = plot_tuning_func.glm_tuning_curve(\n",
    "        #     model_res, df_X,\n",
    "        #     var=var,\n",
    "        #     offset_log=offset_log,\n",
    "        #     average='marginal',\n",
    "        #     weights=exposure_s,                 # time-weighted average rate (recommended)\n",
    "        #     return_ci=True\n",
    "        # )\n",
    "\n",
    "        # # overlay\n",
    "        # plot_tuning_func.overlay_tuning_curves(tc_emp, tc_glm, xcol=var,\n",
    "        #                     title=f'Unit {cluster_id}: {var} tuning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_emp = plot_tuning_func.empirical_tuning_curve(binned_spikes=df_Y[cluster_id].to_numpy(),\n",
    "            predictor_vals=df_X[var].to_numpy(),\n",
    "            exposure_s=exposure_s,\n",
    "            nbins=20)\n",
    "plot_tuning_func.plot_tuning_with_ci(tc_emp, title='Distance → rate (empirical)',\n",
    "                    kind='line', ci_style='band', show_counts=True)\n",
    "\n",
    "# # for discrete variables\n",
    "# df_emp = empirical_tuning_curve(spikes, cur_visible.astype(int), bin_durations)\n",
    "# plot_tuning_with_ci(df_emp, title='Visibility → rate (empirical)',\n",
    "#                     kind='bar', ci_style='errorbar', show_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(meta['groups'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.glm_tools import forward_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(forward_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = meta['groups']\n",
    "df_X_full = design_df\n",
    "\n",
    "# df_X_full.columns is your long Index([...]) from the message\n",
    "# groups is exactly your dict\n",
    "# y is spikes, trial_ids from data, offset_log = np.log(exposure_s)\n",
    "\n",
    "# sensible tiny base: intercept + one robust driver\n",
    "base_groups = ['log_cur_ff_distance']  # 'const' is already a column in df_X_full; keep it inside a group if you want\n",
    "if 'const' not in groups:\n",
    "    # treat the literal 'const' column as its own group\n",
    "    groups = {'const': ['const'], **groups}\n",
    "base_groups = ['const', 'log_cur_ff_distance']\n",
    "\n",
    "# candidates = everything else (you can curate)\n",
    "candidates = [g for g in groups.keys() if g not in base_groups]\n",
    "\n",
    "X_star, selected_groups, log_df = forward_selection.forward_block_select(\n",
    "    df_X_full=df_X_full,\n",
    "    groups=groups,\n",
    "    y=y,\n",
    "    trial_ids=data['new_segment'].to_numpy(),\n",
    "    offset_log=offset_log,  # or scalar np.log(dt)\n",
    "    base_groups=base_groups,\n",
    "    candidate_groups=candidates,\n",
    "    nfolds=5,\n",
    "    max_steps=20,\n",
    "    rng=0\n",
    ")\n",
    "\n",
    "print('Selected order:', selected_groups)\n",
    "print('Final X shape:', X_star.shape)\n",
    "print(log_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['added_group'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups: dict[str, list[str]]\n",
    "# design_df: your full design matrix\n",
    "# log_df: from forward selection\n",
    "\n",
    "# which groups were actually added (drop the None base row)\n",
    "selected_groups = [g for g in log_df['added_group'].dropna().unique() if g in groups]\n",
    "\n",
    "# flatten the column list for these groups\n",
    "selected_cols = sum((groups[g] for g in selected_groups), [])\n",
    "\n",
    "# keep only those columns\n",
    "design_selected = design_df[selected_cols].copy()\n",
    "\n",
    "print('Groups:', selected_groups)\n",
    "print('Cols:', selected_cols[:10], '...')\n",
    "print(design_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What groups almost made it?\n",
    "\n",
    "# # quick “one-more-block” screen\n",
    "# base = ['const','log_cur_ff_distance','LDz_mag']\n",
    "# remaining = [g for g in groups if g not in base]\n",
    "# base_X = df_X_full[sum((groups.get(g, []) for g in base), [])].copy()\n",
    "\n",
    "# def cv_dev(X):\n",
    "#     return forward_selection._cv_dev_poisson(\n",
    "#         y, X, trial_ids=data['new_segment'].to_numpy(),\n",
    "#         offset_log=offset_log, nfolds=5, rng=0\n",
    "#     )\n",
    "\n",
    "# base_score = cv_dev(base_X)\n",
    "# rows = []\n",
    "# for g in remaining:\n",
    "#     cols = [c for c in groups[g] if c in df_X_full.columns]\n",
    "#     if not cols: \n",
    "#         continue\n",
    "#     X_try = base_X.join(df_X_full[cols])\n",
    "#     s = cv_dev(X_try)\n",
    "#     rows.append((g, s, (base_score - s)/max(base_score, 1e-12)))\n",
    "# rank = (pd.DataFrame(rows, columns=['group','cv_dev','rel_improve'])\n",
    "#           .sort_values('cv_dev').head(10))\n",
    "# print(rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit single neuron (w spike history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rebinned_y_var.copy()\n",
    "trial_ids = data['new_segment']\n",
    "dt = pn.bin_width\n",
    "\n",
    "design_df, meta0, meta = create_design_df.get_initial_design_df(data, dt, trial_ids)\n",
    "\n",
    "cluster_num = 3\n",
    "y = rebinned_x_var[f'cluster_{cluster_num}']\n",
    "\n",
    "design_df, meta = create_design_df.add_spike_history(\n",
    "    design_df, y, meta0['trial_ids'], dt,\n",
    "    n_basis=4, t_max=0.20, edge='zero',\n",
    "    prefix='spk_hist', style='bjk',\n",
    "    meta=meta\n",
    ")\n",
    "\n",
    "chk = predictor_utils.check_design_vs_bases(design_df, meta, strict=True)\n",
    "assert chk['ok'], chk['problems']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = glm_fit.fit_poisson_glm_trials(\n",
    "    design_df, y,\n",
    "    dt=dt,\n",
    "    trial_ids=meta0['trial_ids'],\n",
    "    add_const=False,            # we already inserted 'const' in design_df\n",
    "    cluster_se=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, info = predictor_utils.drop_aliased_columns(design_df)\n",
    "info['dropped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) rank of the history block\n",
    "H = design_df.filter(regex=r'^spk_hist:').to_numpy()\n",
    "print(\"hist shape:\", H.shape, \"rank:\", np.linalg.matrix_rank(H))\n",
    "\n",
    "# 2) condition / singular values\n",
    "sv = np.linalg.svd(H, compute_uv=False)\n",
    "print(\"singular values:\", sv)\n",
    "\n",
    "# 3) is that column almost in the span of the others?\n",
    "import numpy as np\n",
    "c = design_df['spk_hist:b0:3'].to_numpy()\n",
    "R = design_df[[c for c in design_df.columns if c.startswith('spk_hist:') and c != 'spk_hist:b0:3']].to_numpy()\n",
    "beta, *_ = np.linalg.lstsq(R, c, rcond=None)\n",
    "resid = c - R @ beta\n",
    "print(\"relative residual:\", np.linalg.norm(resid)/max(np.linalg.norm(c), 1e-12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.04\n",
    "rates = df_Y.sum(axis=0) / (len(df_Y) * dt)\n",
    "# rates is a Series indexed by unit, in Hz\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def firing_rates_from_df(spikes_df, time_col='time', cluster_col='cluster'):\n",
    "    start_s = spikes_df[time_col].min()\n",
    "    end_s = spikes_df[time_col].max()\n",
    "    duration = end_s - start_s\n",
    "\n",
    "    counts = spikes_df.groupby(cluster_col).size()\n",
    "    rates_hz = counts / duration\n",
    "    return rates_hz.rename('rate_hz').reset_index()\n",
    "\n",
    "firing_rates_from_df(pn.spikes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, f, std, info =glm_plotting.plot_angle_tuning_function(\n",
    "    res, design_df, meta,\n",
    "    base_prefix='cur_ff_angle',\n",
    "    M=None,                 # auto-detect harmonics\n",
    "    polar=False,             # pretty polar plot\n",
    "    z=1.96                  # 95% CI\n",
    ")\n",
    "\n",
    "theta, f, std, info =glm_plotting.plot_angle_tuning_function(\n",
    "    res, design_df, meta,\n",
    "    base_prefix='cur_ff_angle',\n",
    "    M=None,                 # auto-detect harmonics\n",
    "    polar=True,             # pretty polar plot\n",
    "    z=1.96                  # 95% CI\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_plotting.plot_fitted_kernels(\n",
    "    res, design_df, meta, dt,\n",
    "    prefixes=['cur_vis_on','cur_vis_off','nxt_vis_on','nxt_vis_off','spk_hist'],\n",
    "    z=1.96  # 95% CI\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## across neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = [col for col in rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "cluster_nums = [int(col.split('_')[1]) for col in cluster_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "designs = []\n",
    "B_hist_ref = None\n",
    "\n",
    "for cluster in cluster_nums:\n",
    "    # .....\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Fit GLM (cluster-robust by trial)\n",
    "    res = fit_poisson_glm_trials(design_df, y_fit, dt, trial_ids, add_const=True, l2=0.0, cluster_se=False)\n",
    "    results.append(res)\n",
    "    designs.append(design_df)\n",
    "\n",
    "meta = {\"B_hist\": B_hist_ref}\n",
    "\n",
    "# Collect population history kernels\n",
    "hist_df = collect_history_kernels_across_neurons(results, designs, meta, dt)\n",
    "\n",
    "# Plot overlays + heatmap\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=True, max_overlays=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you looped over N neurons and saved:\n",
    "results = [res_n0, res_n1, ..., res_nN]         # statsmodels results\n",
    "designs = [X_n0_df, X_n1_df, ..., X_nN_df]      # matching design DataFrames\n",
    "meta    = meta_from_any_single_fit              # must contain 'B_hist'\n",
    "dt      = 0.01                                  # your bin size (s)\n",
    "\n",
    "hist_df = collect_history_kernels_across_neurons(\n",
    "    results, designs, meta, dt, neuron_ids=None  # or e.g. list of unit IDs\n",
    ")\n",
    "\n",
    "\n",
    "# Overlay individual kernels (up to max_overlays) + population mean ± 95% CI\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=False, max_overlays=60)\n",
    "\n",
    "# Heatmap only (neuron × lag)\n",
    "plot_history_kernels_population(hist_df, overlay_mean=False, heatmap=True)\n",
    "\n",
    "# Both\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
