{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -E 's/(=[^=]+)=.*$/\\1/' multiff_analysis/environment.yml > multiff_analysis/environment_nobuild.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -E 's/(=[^=]+)=.*$/\\1/' multiff_analysis/pgam_environment.yml > multiff_analysis/pgam_environment_nobuild.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, ml_methods_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools import glm_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.tpg import glm_bases, plot_glm_fit, glm_on_multiff\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "from numpy import pi\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pn_helper_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = False\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_seg.PlanningAndNeuralSegmentAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "    pn.planning_data_by_point)\n",
    "# pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "pn.rebin_data_in_new_segments(segment_duration=2, rebinned_max_x_lag_number=2)\n",
    "\n",
    "for col in ['cur_vis', 'nxt_vis', 'cur_in_memory', 'nxt_in_memory']:\n",
    "    pn.rebinned_y_var[col] = (pn.rebinned_y_var[col] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in design_df.columns if c.startswith('cur_angle_cos_rc')]\n",
    "design_df[cols].var().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) How much signal is actually in the cos block?\n",
    "\n",
    "\n",
    "# B) Are those params actually ~0 and SE huge?\n",
    "res.params.loc[['const'] + cols]           # param values\n",
    "res.cov_params().loc[cols, cols].abs().max().max()  # largest abs entry in cov submatrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# design_df.dtypes[design_df.dtypes == \"object\"]\n",
    "# design_df.loc[:, design_df.dtypes == \"object\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pn.rebinned_y_var.copy()\n",
    "spike_data = pn.rebinned_x_var['cluster_0']\n",
    "dt = pn.bin_width\n",
    "\n",
    "\n",
    "\n",
    "res, design_df, metrics, meta = glm_on_multiff.fit_multiff_glm(\n",
    "    dt=dt, trial_ids=data[\"new_segment\"],\n",
    "    cur_vis=data[\"cur_in_memory\"], nxt_vis=data[\"nxt_in_memory\"],\n",
    "    cur_dist=data[\"cur_ff_distance\"], nxt_dist=data[\"nxt_ff_distance\"],\n",
    "    cur_angle=data[\"cur_ff_angle\"], nxt_angle=data[\"nxt_ff_angle\"],\n",
    "    heading=None, speed=data[\"monkey_speed\"], curvature=data[\"curv_of_traj\"],\n",
    "    spike_counts=spike_data, l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")\n",
    "print(\"Metrics:\\n\", {k: (v if k != 'per_trial_deviance' else '... DataFrame ...') for k, v in metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_nr = sm.GLM(spike_data, sm.add_constant(design_df),\n",
    "#                 family=sm.families.Poisson(),\n",
    "#                 exposure=np.full(len(design_df), float(pn.bin_width))\n",
    "#                ).fit()  # non-robust (model-based) cov\n",
    "# plot_glm_fit.plot_angle_kernels_with_ci(res_nr, design_df, meta, float(pn.bin_width),\n",
    "#                                         base_prefix='cur_angle', show_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_glm_fit.plot_fitted_kernels(res, design_df, meta, data[\"dt\"])\n",
    "# plot_glm_fit.plot_angle_tuning(res, design_df, meta, data[\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for base_prefix in ['cur_angle', 'nxt_angle', 'cur_dist', 'nxt_dist', 'cur_vis', 'nxt_vis', 'speed', 'curvature']:\n",
    "    plot_glm_fit.plot_angle_kernels_with_ci(res, design_df, meta, dt, base_prefix=base_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## across neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = [col for col in pn.rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "cluster_nums = [int(col.split('_')[1]) for col in cluster_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, design_df, metrics, meta = glm_on_multiff.fit_multiff_glm(\n",
    "    dt=pn.bin_width, trial_ids=data[\"new_segment\"],\n",
    "    cur_vis=data[\"cur_in_memory\"], nxt_vis=data[\"nxt_in_memory\"],\n",
    "    cur_dist=data[\"cur_ff_distance\"], nxt_dist=data[\"nxt_ff_distance\"],\n",
    "    cur_angle=data[\"cur_ff_angle\"], nxt_angle=data[\"nxt_ff_angle\"],\n",
    "    heading=None, speed=data[\"monkey_speed\"], curvature=data[\"curv_of_traj\"],\n",
    "    spike_counts=spike_data, l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "designs = []\n",
    "B_hist_ref = None\n",
    "\n",
    "for cluster in cluster_nums:\n",
    "    # Simulate one \"neuron\" worth of data (independent draws)\n",
    "    trial_ids, y, stim_dict, stim_basis_dict, B_hist = simulate_spikes_with_trials(\n",
    "        n_trials=12, trial_len=300, dt=dt, seed=seed + i\n",
    "    )\n",
    "    if B_hist_ref is None:\n",
    "        B_hist_ref = B_hist\n",
    "\n",
    "    # Build design + history for this neuron\n",
    "    design_df, y_fit = build_glm_design_with_trials(\n",
    "        dt=dt,\n",
    "        trial_ids=trial_ids,\n",
    "        stimulus_dict=stim_dict,\n",
    "        stimulus_basis_dict=stim_basis_dict,\n",
    "        spike_counts=y,\n",
    "        history_basis=B_hist,\n",
    "        extra_covariates=None,\n",
    "        use_trial_FE=True,\n",
    "    )\n",
    "\n",
    "    # Fit GLM (cluster-robust by trial)\n",
    "    res = fit_poisson_glm_trials(design_df, y_fit, dt, trial_ids, add_const=True, l2=0.0, cluster_se=False)\n",
    "    results.append(res)\n",
    "    designs.append(design_df)\n",
    "\n",
    "meta = {\"B_hist\": B_hist_ref}\n",
    "\n",
    "# Collect population history kernels\n",
    "hist_df = collect_history_kernels_across_neurons(results, designs, meta, dt)\n",
    "\n",
    "# Plot overlays + heatmap\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=True, max_overlays=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you looped over N neurons and saved:\n",
    "results = [res_n0, res_n1, ..., res_nN]         # statsmodels results\n",
    "designs = [X_n0_df, X_n1_df, ..., X_nN_df]      # matching design DataFrames\n",
    "meta    = meta_from_any_single_fit              # must contain 'B_hist'\n",
    "dt      = 0.01                                  # your bin size (s)\n",
    "\n",
    "hist_df = collect_history_kernels_across_neurons(\n",
    "    results, designs, meta, dt, neuron_ids=None  # or e.g. list of unit IDs\n",
    ")\n",
    "\n",
    "\n",
    "# Overlay individual kernels (up to max_overlays) + population mean ± 95% CI\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=False, max_overlays=60)\n",
    "\n",
    "# Heatmap only (neuron × lag)\n",
    "plot_history_kernels_population(hist_df, overlay_mean=False, heatmap=True)\n",
    "\n",
    "# Both\n",
    "plot_history_kernels_population(hist_df, overlay_mean=True, heatmap=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glm_on_multiff.simulate_multiff_trials(n_trials=8, trial_len=350, dt=0.01, seed=3)\n",
    "res, design_df, metrics, meta = glm_on_multiff.fit_multiff_glm(\n",
    "    dt=data[\"dt\"], trial_ids=data[\"trial_ids\"],\n",
    "    cur_vis=data[\"cur_vis\"], nxt_vis=data[\"nxt_vis\"],\n",
    "    cur_dist=data[\"cur_dist\"], nxt_dist=data[\"nxt_dist\"],\n",
    "    cur_angle=data[\"cur_angle\"], nxt_angle=data[\"nxt_angle\"],\n",
    "    heading=data[\"heading\"], speed=data[\"speed\"], curvature=data[\"curvature\"],\n",
    "    spike_counts=data[\"spike_counts\"], l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")\n",
    "print(\"Metrics:\\n\", {k: (v if k != 'per_trial_deviance' else '... DataFrame ...') for k, v in metrics.items()})\n",
    "# plot_glm_fit.plot_fitted_kernels(res, design_df, meta, data[\"dt\"])\n",
    "# plot_glm_fit.plot_angle_tuning(res, design_df, meta, data[\"dt\"])\n",
    "plot_glm_fit.plot_angle_kernels_with_ci(res, design_df, meta, data[\"dt\"], base_prefix='cur_angle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in data.items():\n",
    "    try:\n",
    "        print(key, item.shape)\n",
    "    except:\n",
    "        print(key, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGAM functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(pn.x_var, pn.y_var_reduced, pn.bin_width, pn.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.prepare_for_pgam(num_total_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal kernel\n",
    "\n",
    "modified from PGAM_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_temporal_features_to_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gdh.smooths_handler.add_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_spatial_features_to_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.run_pgam(neural_cluster_number=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.post_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "# Degree of spline\n",
    "k = 3\n",
    "\n",
    "# Example 1: Not enough start knots\n",
    "knots1 = [0, 2, 4, 6, 8, 10, 10, 10, 10]\n",
    "coeffs = np.ones(len(knots1) - k - 1)\n",
    "spline1 = BSpline(knots1, coeffs, k)\n",
    "\n",
    "# Example 2: Proper repeated start knots\n",
    "knots2 = [0, 0, 0, 0, 2, 4, 6, 8, 10, 10, 10, 10]\n",
    "coeffs2 = np.ones(len(knots2) - k - 1)\n",
    "spline2 = BSpline(knots2, coeffs2, k)\n",
    "\n",
    "x = np.linspace(-1, 11, 400)\n",
    "\n",
    "plt.plot(x, spline1(x), label=\"Start knot once\")\n",
    "plt.plot(x, spline2(x), label=\"Start knot repeated\")\n",
    "plt.axvline(0, color='gray', linestyle='--', label=\"x = 0\")\n",
    "plt.legend()\n",
    "plt.title(\"Effect of Repeating Start Knot\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Spline Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate through all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(pn.x_var, pn.y_var, pn.bin_width, pn.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(pn.x_var.shape[1]):\n",
    "    print(f'neural_cluster_number: {i} out of {pn.x_var.shape[1]}')\n",
    "    pgam_inst.streamline_pgam(neural_cluster_number=i, num_total_trials=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
