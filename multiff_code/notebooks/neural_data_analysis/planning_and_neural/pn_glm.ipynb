{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, ml_methods_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils\n",
    "from neural_data_analysis.design_kits.design_by_segment import create_design_df, predictor_utils, other_feats\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event, pn_glm_utils\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.tpg import glm_bases, glm_plotting, glm_plotting2, glm_fit\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_fit import stop_glm_fit, cv_stop_glm, glm_fit_utils, variance_explained\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_plotting import plot_spikes, plot_glm_fit, plot_tuning_func\n",
    "from neural_data_analysis.design_kits.design_around_event import event_binning, stop_design, cluster_design, design_checks\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_hyperparams import compare_glm_configs, glm_hyperparams_class\n",
    "from neural_data_analysis.design_kits.design_by_segment import spike_history\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "from numpy import pi\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pn_aligned_by_event)\n",
    "reload(pn_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "pn.rebin_data_in_new_segments(cur_or_nxt='cur', first_or_last='first', time_limit_to_count_sighting=2,\n",
    "                                 start_t_rel_event=0, end_t_rel_event=1.5, rebinned_max_x_lag_number=2)\n",
    "\n",
    "for col in ['cur_vis', 'nxt_vis', 'cur_in_memory', 'nxt_in_memory']:\n",
    "    pn.rebinned_y_var[col] = (pn.rebinned_y_var[col] > 0).astype(int)\n",
    "    \n",
    "rebinned_x_var = pn.rebinned_x_var.copy()\n",
    "rebinned_y_var = pn.rebinned_y_var.copy()\n",
    "\n",
    "rebinned_y_var.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get heading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_info_df, heading_df = pn_glm_utils.get_test_heading_df(raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebinned_x_var, rebinned_y_var = pn_glm_utils.select_ff_subset(heading_df, pn.rebinned_x_var, pn.rebinned_y_var, \n",
    "#                                                                top=False, pct=0.5)\n",
    "# rebinned_x_var = pn_glm_utils.drop_constant_columns(rebinned_x_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(create_design_df)\n",
    "reload(other_feats)\n",
    "\n",
    "data = rebinned_y_var.copy()\n",
    "trial_ids = data['new_segment']\n",
    "dt = pn.bin_width\n",
    "\n",
    "design_df, meta0, meta = create_design_df.get_initial_design_df(data, dt, trial_ids)\n",
    "\n",
    "cluster_num = 3\n",
    "y = rebinned_x_var[f'cluster_{cluster_num}']\n",
    "\n",
    "# design_df, meta = temporal_feats.add_spike_history(\n",
    "#     design_df, y, meta0['trial_ids'], dt,\n",
    "#     n_basis=4, t_max=0.20, edge='zero',\n",
    "#     prefix='spk_hist', style='bjk',\n",
    "#     meta=meta\n",
    "# )\n",
    "\n",
    "chk = predictor_utils.check_design_vs_bases(design_df, meta, strict=True)\n",
    "assert chk['ok'], chk['problems']\n",
    "\n",
    "\n",
    "cluster_cols = [col for col in rebinned_x_var.columns if col.startswith('cluster_')]\n",
    "df_Y = rebinned_x_var[cluster_cols]\n",
    "df_Y.columns = df_Y.columns.str.replace('cluster_', '').astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other ways of taking out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_X = design_df\n",
    "\n",
    "# # cur_ff_distance_cols = [col for col in design_df.columns if 'cur_ff_distance' in col]\n",
    "# # df_X = design_df[cur_ff_distance_cols + ['const']]\n",
    "\n",
    "cur_ff_cols = [col for col in design_df.columns if ('cur_' in col) and ('cur_in_memory' not in col)]\n",
    "nxt_ff_cols = [col for col in design_df.columns if 'nxt_' in col]\n",
    "\n",
    "# # df_X = design_selected\n",
    "\n",
    "# # df_X = design_selected.copy()\n",
    "\n",
    "# df_X = design_df[['speed_z', 'time_since_last_capture',\n",
    "#        'ang_accel_mag_spline:s0', 'ang_accel_mag_spline:s1',\n",
    "#        'ang_accel_mag_spline:s2', 'ang_accel_mag_spline:s3']].copy()\n",
    "# df_X[cur_ff_distance_cols] = design_df[cur_ff_distance_cols].copy()\n",
    "\n",
    "# print('df_X.shape:', df_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_this = False\n",
    "if use_this:\n",
    "    df_X = design_df[['speed_z', 'time_since_last_capture',\n",
    "        'ang_accel_mag_spline:s0', 'ang_accel_mag_spline:s1',\n",
    "        'ang_accel_mag_spline:s2', 'ang_accel_mag_spline:s3'] + cur_ff_cols + nxt_ff_cols].copy()\n",
    "\n",
    "    print('df_X.shape:', df_X.shape)\n",
    "else:\n",
    "    df_X = design_df.copy()\n",
    "\n",
    "df_X['random_0_or_1'] = np.random.randint(0, 2, len(df_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pruned, vif_report = design_checks.check_design(df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pruned_new, vif_report = design_checks.check_design(X_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure = np.full_like(y, fill_value=pn.bin_width, dtype=float)\n",
    "offset_log = np.log(exposure)\n",
    "\n",
    "report = stop_glm_fit.glm_mini_report(\n",
    "    # df_X=df_X, \n",
    "    df_X=X_pruned,\n",
    "    df_Y=df_Y, offset_log=offset_log,\n",
    "    cov_type='HC1', \n",
    "    fast_mle=True,\n",
    "    do_inference=False, \n",
    "    make_plots=True,\n",
    "    show_plots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['coefs_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report['coefs_df'][report['coefs_df']['term'] == 'random_0_or_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add spike history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.02\n",
    "t_max = 0.20\n",
    "\n",
    "# 1) Rebin with padding\n",
    "design_pad, pad_info = spike_history.rebin_spike_data_with_pad(\n",
    "    spikes_df=pn.spikes_df,\n",
    "    new_seg_info=pn.new_seg_info,\n",
    "    bin_width=dt,\n",
    "    t_max=t_max,\n",
    ")\n",
    "\n",
    "# 2) Precompute history for all neurons\n",
    "X_hist, basis, colnames = spike_history.compute_spike_history_designs(\n",
    "    design_pad,\n",
    "    rebinned_x_var,\n",
    "    n_pad_bins=pad_info['n_pad_bins'],\n",
    "    dt=dt,\n",
    "    t_max=t_max,\n",
    "    n_basis=5,\n",
    ")\n",
    "\n",
    "# 3) For now, since we include both self-history and cross-history, the design_df is the same for all neurons\n",
    "spike_cols = [c for c in design_pad.columns if c.startswith('cluster_')]\n",
    "\n",
    "for target_col in spike_cols:\n",
    "    cross_neurons = [c for c in spike_cols if c != target_col]\n",
    "\n",
    "    design_w_history, meta['groups'] = spike_history.add_spike_history_to_design(\n",
    "        X_pruned,\n",
    "        colnames,\n",
    "        X_hist,\n",
    "        target_col,\n",
    "        include_self=True,\n",
    "        cross_neurons=cross_neurons,\n",
    "        meta_groups=meta['groups'],\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pruned1, vif_report = design_checks.check_design(design_w_history)\n",
    "print('Selected final columns: ', X_pruned1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pruned2, vif_report = design_checks.check_design(X_pruned1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_inference = True\n",
    "\n",
    "exposure = np.full_like(y, fill_value=pn.bin_width, dtype=float)\n",
    "offset_log = np.log(exposure)\n",
    "\n",
    "report = stop_glm_fit.glm_mini_report(\n",
    "    df_X=X_pruned1,\n",
    "    df_Y=df_Y, offset_log=offset_log,\n",
    "    cov_type='HC1', \n",
    "    fast_mle=True,\n",
    "    do_inference=do_inference, \n",
    "    make_plots=True,\n",
    "    show_plots=True,\n",
    "    meta_groups=meta['groups']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df = report['coefs_df']\n",
    "coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_utils.check_na_in_df(report['coefs_df'], df_name=\"DataFrame\", return_rows_and_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df    = report['coefs_df']\n",
    "metrics_df  = report['metrics_df']\n",
    "pop_tests   = report['population_tests_df']\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = variance_explained.run_variance_explained_analysis(\n",
    "    report=report,\n",
    "    df_X=df_X,\n",
    "    df_Y=df_Y,\n",
    "    offset_log=offset_log,\n",
    "    event_ids=rebinned_x_var['new_segment'].to_numpy(),\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "print('=' * 80)\n",
    "print(pd.Series(results['summary_metrics']))\n",
    "print(results['per_event_df'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## population latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_eval import shared_manifold, parity_utils, population_latent_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_idx = data['new_bin']\n",
    "condition_labels = None\n",
    "\n",
    "res = population_latent_benchmark.population_latent_benchmark_function(\n",
    "    X, X_hat, trial_ids, time_idx, condition_labels,\n",
    "    fa_factors=8,\n",
    "    max_rank=10,\n",
    "    shuffle_mode='latent'\n",
    ")\n",
    "\n",
    "res['figs']['cca_spectrum']\n",
    "res['figs']['ve_summary']\n",
    "res['figs']['rrr_curve']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_eval.shared_manifold import plot_shared_components\n",
    "import pandas as pd\n",
    "\n",
    "df_lat = pd.concat(\n",
    "    res['parity']['latents_by_fold'].values(),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "best_rank = res['rrr']['best_rank']\n",
    "\n",
    "figs = plot_shared_components(\n",
    "    df_lat,\n",
    "    components=[1, 2], # or use n_components=best_rank instead\n",
    "    overlay_mode='separate'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: one fold\n",
    "fig = shared_manifold.plot_shared_phase_plane(\n",
    "    res['parity']['latents_by_fold'][0],\n",
    "    components=(1, 2),\n",
    "    smooth_bins=3\n",
    ")\n",
    "\n",
    "# or concat all folds (one caveat: Different folds use slightly different CCA fits.)\n",
    "\n",
    "df_lat = pd.concat(\n",
    "    res['parity']['latents_by_fold'].values(),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "fig = shared_manifold.plot_shared_phase_plane(\n",
    "    df_lat,\n",
    "    components=(1, 2),\n",
    "    smooth_bins=3\n",
    ")\n",
    "\n",
    "\n",
    "fig = shared_manifold.plot_shared_phase_plane_by_fold(\n",
    "    res['parity']['latents_by_fold'],\n",
    "    components=(1, 2),\n",
    "    smooth_bins=3,\n",
    "    alpha_fold=0.6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idx = 4\n",
    "\n",
    "rebinned_y_var['rel_time'] = rebinned_y_var['bin_mid_time_rel_to_event']\n",
    "\n",
    "for event_id in range(40, 52):\n",
    "    # If your GLM used offset_log = np.log(exposure_s), you can omit exposure_s:\n",
    "    plot_spikes.plot_observed_vs_predicted_event(\n",
    "        #binned_feats_sc=df_X,\n",
    "        binned_feats_sc=X_pruned1,\n",
    "        binned_spikes=df_Y,\n",
    "        meta_used=rebinned_y_var,\n",
    "        offset_log=offset_log,\n",
    "        model_res=report['results'][cluster_idx],   # GLM for cluster 0\n",
    "        cluster_idx=cluster_idx,\n",
    "        seg_id=event_id,\n",
    "        time_col='rel_time',\n",
    "        seg_col='new_segment'\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tuning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in df_Y.columns:\n",
    "    \n",
    "    print('==========================================')\n",
    "    model_res = report['results'][cluster_id]\n",
    "    \n",
    "    # for var in ['nxt_in_memory', 'cur_ff_distance', 'log_cur_ff_distance']:\n",
    "    for var in ['cur_vis_off:b0:0', 'ang_accel_mag_spline:s2']:\n",
    "\n",
    "        exposure_s = np.exp(offset_log)\n",
    "        # make empirical curve\n",
    "        tc_emp = plot_tuning_func.empirical_tuning_curve(\n",
    "            binned_spikes=df_Y[cluster_id].to_numpy(),\n",
    "            predictor_vals=df_X[var].to_numpy(),\n",
    "            exposure_s=exposure_s,\n",
    "            nbins=20\n",
    "        )\n",
    "\n",
    "        # # just empirical\n",
    "        # plot_tuning_func.plot_tuning_with_ci(tc_emp, title='Distance → rate (empirical)',\n",
    "        #             kind='line', ci_style='band', show_counts=True)\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # make GLM curve\n",
    "        tc_glm = plot_tuning_func.glm_tuning_curve(\n",
    "            model_res, X_pruned1,\n",
    "            var=var,\n",
    "            offset_log=offset_log,\n",
    "            average='marginal',\n",
    "            weights=exposure_s,                 # time-weighted average rate (recommended)\n",
    "            return_ci=True\n",
    "        )\n",
    "\n",
    "        # overlay\n",
    "        plot_tuning_func.overlay_tuning_curves(tc_emp, tc_glm, xcol=var,\n",
    "                            title=f'Unit {cluster_id}: {var} tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_emp = plot_tuning_func.empirical_tuning_curve(binned_spikes=df_Y[cluster_id].to_numpy(),\n",
    "            predictor_vals=df_X[var].to_numpy(),\n",
    "            exposure_s=exposure_s,\n",
    "            nbins=20)\n",
    "plot_tuning_func.plot_tuning_with_ci(tc_emp, title='Distance → rate (empirical)',\n",
    "                    kind='line', ci_style='band', show_counts=True)\n",
    "\n",
    "# # for discrete variables\n",
    "# df_emp = empirical_tuning_curve(spikes, cur_visible.astype(int), bin_durations)\n",
    "# plot_tuning_with_ci(df_emp, title='Visibility → rate (empirical)',\n",
    "#                     kind='bar', ci_style='errorbar', show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.decoding_tools.general_decoding import standard_decoding\n",
    "\n",
    "cols_to_decode = ['cur_vis', 'nxt_vis', 'random_0_or_1']\n",
    "groups = np.array(data['new_segment'])\n",
    "standard_decoding.population_decoding_cv(\n",
    "    cols_to_decode=cols_to_decode,\n",
    "    df_X=df_X,\n",
    "    df_Y=df_Y,\n",
    "    groups=groups,\n",
    "    decoders=('logreg', 'lda'),\n",
    "    n_splits=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(meta['groups'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.glm_tools import forward_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(forward_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = meta['groups']\n",
    "df_X_full = design_df\n",
    "\n",
    "# df_X_full.columns is your long Index([...]) from the message\n",
    "# groups is exactly your dict\n",
    "# y is spikes, trial_ids from data, offset_log = np.log(exposure_s)\n",
    "\n",
    "# sensible tiny base: intercept + one robust driver\n",
    "base_groups = ['log_cur_ff_distance']  # 'const' is already a column in df_X_full; keep it inside a group if you want\n",
    "if 'const' not in groups:\n",
    "    # treat the literal 'const' column as its own group\n",
    "    groups = {'const': ['const'], **groups}\n",
    "base_groups = ['const', 'log_cur_ff_distance']\n",
    "\n",
    "# candidates = everything else (you can curate)\n",
    "candidates = [g for g in groups.keys() if g not in base_groups]\n",
    "\n",
    "X_star, selected_groups, log_df = forward_selection.forward_block_select(\n",
    "    df_X_full=df_X_full,\n",
    "    groups=groups,\n",
    "    y=y,\n",
    "    trial_ids=data['new_segment'].to_numpy(),\n",
    "    offset_log=offset_log,  # or scalar np.log(dt)\n",
    "    base_groups=base_groups,\n",
    "    candidate_groups=candidates,\n",
    "    nfolds=5,\n",
    "    max_steps=20,\n",
    "    rng=0\n",
    ")\n",
    "\n",
    "print('Selected order:', selected_groups)\n",
    "print('Final X shape:', X_star.shape)\n",
    "print(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['added_group'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups: dict[str, list[str]]\n",
    "# design_df: your full design matrix\n",
    "# log_df: from forward selection\n",
    "\n",
    "# which groups were actually added (drop the None base row)\n",
    "selected_groups = [g for g in log_df['added_group'].dropna().unique() if g in groups]\n",
    "\n",
    "# flatten the column list for these groups\n",
    "selected_cols = sum((groups[g] for g in selected_groups), [])\n",
    "\n",
    "# keep only those columns\n",
    "design_selected = design_df[selected_cols].copy()\n",
    "\n",
    "print('Groups:', selected_groups)\n",
    "print('Cols:', selected_cols[:10], '...')\n",
    "print(design_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What groups almost made it?\n",
    "\n",
    "# # quick “one-more-block” screen\n",
    "# base = ['const','log_cur_ff_distance','LDz_mag']\n",
    "# remaining = [g for g in groups if g not in base]\n",
    "# base_X = df_X_full[sum((groups.get(g, []) for g in base), [])].copy()\n",
    "\n",
    "# def cv_dev(X):\n",
    "#     return forward_selection._cv_dev_poisson(\n",
    "#         y, X, trial_ids=data['new_segment'].to_numpy(),\n",
    "#         offset_log=offset_log, nfolds=5, rng=0\n",
    "#     )\n",
    "\n",
    "# base_score = cv_dev(base_X)\n",
    "# rows = []\n",
    "# for g in remaining:\n",
    "#     cols = [c for c in groups[g] if c in df_X_full.columns]\n",
    "#     if not cols: \n",
    "#         continue\n",
    "#     X_try = base_X.join(df_X_full[cols])\n",
    "#     s = cv_dev(X_try)\n",
    "#     rows.append((g, s, (base_score - s)/max(base_score, 1e-12)))\n",
    "# rank = (pd.DataFrame(rows, columns=['group','cv_dev','rel_improve'])\n",
    "#           .sort_values('cv_dev').head(10))\n",
    "# print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit single neuron (w spike history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.design_kits.design_by_segment import temporal_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rebinned_y_var.copy()\n",
    "trial_ids = data['new_segment']\n",
    "dt = pn.bin_width\n",
    "\n",
    "design_df, meta0, meta = create_design_df.get_initial_design_df(data, dt, trial_ids)\n",
    "\n",
    "cluster_num = 3\n",
    "y = rebinned_x_var[f'cluster_{cluster_num}']\n",
    "\n",
    "design_df, meta = temporal_feats.add_spike_history(\n",
    "    design_df, y, meta0['trial_ids'], dt,\n",
    "    n_basis=4, t_max=0.20, edge='zero',\n",
    "    prefix='spk_hist', style='bjk',\n",
    "    meta=meta\n",
    ")\n",
    "\n",
    "chk = predictor_utils.check_design_vs_bases(design_df, meta, strict=True)\n",
    "assert chk['ok'], chk['problems']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = glm_fit.fit_poisson_glm_trials(\n",
    "    design_df, y,\n",
    "    dt=dt,\n",
    "    trial_ids=meta0['trial_ids'],\n",
    "    add_const=False,            # we already inserted 'const' in design_df\n",
    "    cluster_se=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, info = predictor_utils.drop_aliased_columns(design_df)\n",
    "info['dropped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) rank of the history block\n",
    "H = design_df.filter(regex=r'^spk_hist:').to_numpy()\n",
    "print(\"hist shape:\", H.shape, \"rank:\", np.linalg.matrix_rank(H))\n",
    "\n",
    "# 2) condition / singular values\n",
    "sv = np.linalg.svd(H, compute_uv=False)\n",
    "print(\"singular values:\", sv)\n",
    "\n",
    "# 3) is that column almost in the span of the others?\n",
    "import numpy as np\n",
    "c = design_df['spk_hist:b0:3'].to_numpy()\n",
    "R = design_df[[c for c in design_df.columns if c.startswith('spk_hist:') and c != 'spk_hist:b0:3']].to_numpy()\n",
    "beta, *_ = np.linalg.lstsq(R, c, rcond=None)\n",
    "resid = c - R @ beta\n",
    "print(\"relative residual:\", np.linalg.norm(resid)/max(np.linalg.norm(c), 1e-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.04\n",
    "rates = df_Y.sum(axis=0) / (len(df_Y) * dt)\n",
    "# rates is a Series indexed by unit, in Hz\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def firing_rates_from_df(spikes_df, time_col='time', cluster_col='cluster'):\n",
    "    start_s = spikes_df[time_col].min()\n",
    "    end_s = spikes_df[time_col].max()\n",
    "    duration = end_s - start_s\n",
    "\n",
    "    counts = spikes_df.groupby(cluster_col).size()\n",
    "    rates_hz = counts / duration\n",
    "    return rates_hz.rename('rate_hz').reset_index()\n",
    "\n",
    "firing_rates_from_df(pn.spikes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, f, std, info =glm_plotting.plot_angle_tuning_function(\n",
    "    res, design_df, meta,\n",
    "    base_prefix='cur_ff_angle',\n",
    "    M=None,                 # auto-detect harmonics\n",
    "    polar=False,             # pretty polar plot\n",
    "    z=1.96                  # 95% CI\n",
    ")\n",
    "\n",
    "theta, f, std, info =glm_plotting.plot_angle_tuning_function(\n",
    "    res, design_df, meta,\n",
    "    base_prefix='cur_ff_angle',\n",
    "    M=None,                 # auto-detect harmonics\n",
    "    polar=True,             # pretty polar plot\n",
    "    z=1.96                  # 95% CI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_plotting.plot_fitted_kernels(\n",
    "    res, design_df, meta, dt,\n",
    "    prefixes=['cur_vis_on','cur_vis_off','nxt_vis_on','nxt_vis_off','spk_hist'],\n",
    "    z=1.96  # 95% CI\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
