{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural.pn_decoding import pn_decoding_utils, plot_pn_decoding, pn_decoding_model_specs\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural.pn_decoding.interactions import add_interactions, discrete_decoders, conditional_decoding, interaction_decoding, interaction_plots\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "import quantities as pq\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0321\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0329\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0403\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0316\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "bin_width = 0.1\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path, bin_width=bin_width)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "    pn.planning_data_by_point)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get planning_data by segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data and fit gpfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.prepare_seg_aligned_data(start_t_rel_event=-0.25, end_t_rel_event=1.25, end_at_stop_time=False)\n",
    "pn.get_gpfa_traj(latent_dimensionality=7, exists_ok=True)\n",
    "\n",
    "# for regression later\n",
    "use_raw_spike_data_instead = False\n",
    "use_lagged_rebinned_behav_data = False\n",
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=use_raw_spike_data_instead,\n",
    "                                  use_lagged_rebinned_behav_data=use_lagged_rebinned_behav_data,\n",
    "                                  apply_pca_on_raw_spike_data=True,\n",
    "                                  use_lagged_raw_spike_data=False,) \n",
    "\n",
    "\n",
    "pn.print_data_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpfa DURING train test split (also point-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.prepare_seg_aligned_data()\n",
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data (try only a few features right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_features = [\n",
    "    'new_bin', 'new_segment', 'whether_test',\n",
    "    'cur_eye_hor_l', 'cur_eye_ver_l', 'cur_eye_hor_r', 'cur_eye_ver_r',\n",
    "    'nxt_eye_hor_l', 'nxt_eye_ver_l', 'nxt_eye_hor_r', 'nxt_eye_ver_r',\n",
    "    'LDz', 'RDz', 'LDx', 'RDx',\n",
    "    'gaze_mky_view_x', 'gaze_mky_view_y', 'gaze_mky_view_angle',\n",
    "    'cur_opt_arc_dheading',\n",
    "    'cur_ff_distance',\n",
    "    'cur_ff_rel_x',\n",
    "    'cur_ff_rel_y',\n",
    "    'nxt_ff_rel_x',\n",
    "    'nxt_ff_rel_y',\n",
    "    'nxt_ff_distance',\n",
    "    'num_ff_visible',\n",
    "    'num_ff_in_memory',\n",
    "    'cur_ff_distance_at_ref',\n",
    "    'cur_ff_angle_boundary_at_ref',\n",
    "    'nxt_ff_distance_at_ref',\n",
    "    'ang_speed',\n",
    "    'speed',\n",
    "    'accel',\n",
    "    'ang_accel',\n",
    "    'monkey_speeddummy',\n",
    "    'curv_of_traj',\n",
    "    'angle_from_cur_ff_to_nxt_ff',\n",
    "    'time_since_last_capture',\n",
    "    'bin_mid_time_rel_to_event',\n",
    "    'time', \n",
    "    'target_index',\n",
    "    # categorical modeling for the below:\n",
    "    'cur_vis',\n",
    "    'nxt_vis',\n",
    "    'nxt_in_memory',\n",
    "    'any_ff_visible',\n",
    "    # 'cur_in_memory', # don't used those two cause they will just be one\n",
    "    # 'any_ff_in_memory',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_duplicates = len(key_features) != len(set(key_features)) \n",
    "print(has_duplicates)\n",
    "\n",
    "dupes = {x for x in key_features if key_features.count(x) > 1}\n",
    "print(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pn.prepare_seg_aligned_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=False,\n",
    "                                    apply_pca_on_raw_spike_data=False,\n",
    "                                    use_lagged_raw_spike_data=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_behav_trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pn.concat_behav_trials, added_cols = pn_decoding_utils.prep_behav(pn.concat_behav_trials)\n",
    "pn.rebinned_behav_data, _ = pn_decoding_utils.prep_behav(pn.rebinned_behav_data)\n",
    "key_features = list(set(key_features + added_cols))\n",
    "key_features = [f for f in key_features if f in pn.concat_behav_trials.columns]\n",
    "pn.concat_behav_trials = pn.concat_behav_trials[key_features].copy()\n",
    "pn.rebinned_behav_data = pn.rebinned_behav_data[key_features].copy()\n",
    "    \n",
    "# (Optional) peek at shapes\n",
    "print('concat_behav_trials:', pn.concat_behav_trials.shape)\n",
    "print('rebinned_behav_data:', pn.rebinned_behav_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = pn.concat_behav_trials['bin_mid_time_rel_to_event'] > 0\n",
    "pn.concat_behav_trials = pn.concat_behav_trials[mask]\n",
    "pn.concat_neural_trials = pn.concat_neural_trials[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_behav_trials, cols_added = pn_decoding_utils.add_interaction_terms_and_features(pn.concat_behav_trials)\n",
    "key_features2 = (\n",
    "    ['cur_ff_distance', 'log1p_cur_ff_distance', 'speed',\n",
    "        'accel', 'time_since_last_capture']\n",
    "    + cols_added\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Speed quantiles:\")\n",
    "for q in [0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1]:\n",
    "    val = pn.concat_behav_trials['speed'].quantile(q)\n",
    "    print(f\"  {q:.2f}: {val:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build interaction labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find ranges of vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_behav_trials.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also should i use 'cur_ff_angle' too?\n",
    "\n",
    "# CAN ALSO have interaction based on stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['speed', 'ang_speed', 'accel', 'ang_accel', 'cur_ff_distance', 'nxt_ff_distance', 'cur_ff_angle', 'nxt_ff_angle', 'cur_ff_rel_x', 'cur_ff_rel_y', 'cur_ff_distance_at_ref']:\n",
    "    print(f\"Quantiles for {var}:\")\n",
    "    for q in [0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1]:\n",
    "        val = pn.planning_data_by_point[var].quantile(q)\n",
    "        print(f\"  {q:.2f}: {val:.5f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=True) \n",
    "df = pn.concat_behav_trials.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_interactions.add_behavior_bands(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_interactions.add_pairwise_interaction(\n",
    "    df=df,\n",
    "    var_a='speed_band',\n",
    "    var_b='cur_ff_angle_band',\n",
    "    new_col='speed_angle_state',\n",
    ")\n",
    "\n",
    "y_var, x_var = add_interactions.prune_rare_states_two_dfs(\n",
    "    df,\n",
    "    pn.concat_neural_trials,\n",
    "    label_col='speed_angle_state',\n",
    "    min_count=200\n",
    ")\n",
    "\n",
    "results_df = discrete_decoders.sweep_decoders_xy(\n",
    "    x_df=x_var,\n",
    "    y_df=y_var,\n",
    "    label_col='speed_angle_state',\n",
    "    model_types=['logreg', 'svm', 'ridge'],\n",
    ")\n",
    "\n",
    "summary_df = (\n",
    "    results_df\n",
    "    .groupby('model', as_index=False)\n",
    "    .agg(mean_bal_acc=('balanced_accuracy', 'mean'))\n",
    ")\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRWISE_INTERACTIONS = [\n",
    "\n",
    "    # =========================================================\n",
    "    # 1. Core movement control (highest priority)\n",
    "    # =========================================================\n",
    "\n",
    "    # Speed × geometry: classic steering regimes\n",
    "    ('speed_band', 'cur_ff_angle_band', 'speed_angle_state'),\n",
    "    ('speed_band', 'cur_ff_dist_band', 'speed_distance_state'),\n",
    "\n",
    "    # Speed × motor output\n",
    "    ('speed_band', 'ang_speed_band', 'speed_turnrate_state'),\n",
    "    ('speed_band', 'accel_band', 'speed_accel_state'),\n",
    "\n",
    "    # =========================================================\n",
    "    # 2. Geometry × distance (navigation state)\n",
    "    # =========================================================\n",
    "\n",
    "    # How far + how misaligned am I from current target?\n",
    "    ('cur_ff_angle_band', 'cur_ff_dist_band', 'angle_distance_state'),\n",
    "\n",
    "    # Lateral geometry vs forward progress (optional but clean)\n",
    "    ('cur_ff_rel_x_band', 'cur_ff_dist_band', 'lateral_distance_state'),\n",
    "\n",
    "    # =========================================================\n",
    "    # 3. Planning / lookahead geometry\n",
    "    # =========================================================\n",
    "\n",
    "    # Current vs next target geometry (planning competition)\n",
    "    ('cur_ff_angle_band', 'nxt_ff_angle_band', 'cur_next_angle_state'),\n",
    "\n",
    "    # Commitment stage × next-target relevance\n",
    "    ('cur_ff_dist_band', 'nxt_ff_dist_band', 'curdist_nextdist_state'),\n",
    "\n",
    "    # =========================================================\n",
    "    # 4. Control dynamics (policy change / replanning)\n",
    "    # =========================================================\n",
    "\n",
    "    # Acceleration conditioned on geometry\n",
    "    ('accel_band', 'cur_ff_angle_band', 'accel_angle_state'),\n",
    "    ('accel_band', 'cur_ff_dist_band', 'accel_distance_state'),\n",
    "\n",
    "    # Turn acceleration vs current geometry (replanning signal)\n",
    "    ('ang_accel_band', 'cur_ff_angle_band', 'angaccel_angle_state'),\n",
    "\n",
    "    # =========================================================\n",
    "    # 5. Commitment / learning (late-stage, optional)\n",
    "    # =========================================================\n",
    "\n",
    "    # Early vs late commitment interacting with geometry\n",
    "    ('cur_ff_dist_ref_band', 'cur_ff_angle_band', 'commit_angle_state'),\n",
    "\n",
    "    # Commitment timing × speed (hesitation vs execution)\n",
    "    ('cur_ff_dist_ref_band', 'speed_band', 'commit_speed_state'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_to_plot = 10\n",
    "counter = 0\n",
    "for var_a, var_b, new_col in PAIRWISE_INTERACTIONS:\n",
    "    out = interaction_decoding.run_pairwise_interaction_analysis(\n",
    "        x_df=pn.concat_neural_trials,\n",
    "        y_df=df,\n",
    "        var_a=var_a,\n",
    "        var_b=var_b,\n",
    "        interaction_col=new_col,\n",
    "    )\n",
    "\n",
    "\n",
    "    fig = interaction_plots.plot_pairwise_interaction_analysis(\n",
    "        analysis_out=out,\n",
    "        interaction_name=new_col,\n",
    "        var_a=var_a,\n",
    "        var_b=var_b,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if counter >= max_to_plot:\n",
    "        break\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['cond_var_a_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_interactions.add_pairwise_interaction(\n",
    "    df=df,\n",
    "    var_a='speed_band',\n",
    "    var_b='cur_ff_angle_band',\n",
    "    new_col='speed_angle_state',\n",
    ")\n",
    "\n",
    "y_var, x_var = add_interactions.prune_rare_states_two_dfs(\n",
    "    df,\n",
    "    pn.concat_neural_trials,\n",
    "    label_col='speed_angle_state',\n",
    "    min_count=200\n",
    ")\n",
    "\n",
    "results_df = discrete_decoders.sweep_decoders_xy(\n",
    "    x_df=x_var,\n",
    "    y_df=y_var,\n",
    "    label_col='speed_angle_state',\n",
    "    model_types=['logreg', 'svm', 'ridge'],\n",
    ")\n",
    "\n",
    "summary_df = (\n",
    "    results_df\n",
    "    .groupby('model', as_index=False)\n",
    "    .agg(mean_bal_acc=('balanced_accuracy', 'mean'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(discrete_decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = conditional_decoding.compare_component_conditioned_vs_global(\n",
    "    x_df=x_var,\n",
    "    y_df=y_var,\n",
    "    target_col='cur_ff_angle_band',\n",
    "    condition_col='speed_band',\n",
    "    model_type='logreg',\n",
    ")\n",
    "\n",
    "summary_df = (\n",
    "    results_df\n",
    "    .groupby('context', as_index=False)\n",
    "    .agg(mean_bal_acc=('balanced_accuracy', 'mean'))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_df = discrete_decoders.hierarchical_decode_speed_angle(\n",
    "    x_df=x_var,\n",
    "    y_df=y_var,\n",
    "    speed_col='speed_band',\n",
    "    angle_col='cur_ff_angle_band',\n",
    "    model_type='logreg',\n",
    ")\n",
    "\n",
    "hier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_condition_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "rows.append(discrete_decoders.cross_condition_decode(\n",
    "    x_var, y_var,\n",
    "    target_col='cur_ff_angle_band',\n",
    "    condition_col='speed_band',\n",
    "    train_conditions=['FAST', 'SLOW'],\n",
    "    test_conditions=['CRUISE'],\n",
    "))\n",
    "\n",
    "rows.append(discrete_decoders.cross_condition_decode(\n",
    "    x_var, y_var,\n",
    "    target_col='cur_ff_angle_band',\n",
    "    condition_col='speed_band',\n",
    "    train_conditions=['CRUISE'],\n",
    "    test_conditions=['FAST'],\n",
    "))\n",
    "\n",
    "cross_df = pd.DataFrame([r for r in rows if r is not None])\n",
    "cross_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = discrete_decoders.decode_with_param_sweep_xy(\n",
    "    x_df=x_var,                         # neural features\n",
    "    y_df=y_var,                         # behavior labels\n",
    "    label_col='speed_angle_state',     # what you decode\n",
    "    model_type='logreg',               # ONE model family\n",
    "    n_splits=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chance_level(results_df, df, label_col):\n",
    "    probs = df[label_col].value_counts(normalize=True)\n",
    "    chance = np.sum(probs ** 2)\n",
    "\n",
    "    results_df = results_df.copy()\n",
    "    results_df['chance'] = chance\n",
    "    results_df['above_chance'] = results_df['accuracy'] - chance\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = (\n",
    "    results_df\n",
    "    .groupby(['label', 'model'], as_index=False)\n",
    "    .agg(\n",
    "        mean_acc=('accuracy', 'mean'),\n",
    "        sem_acc=('accuracy', lambda x: x.std() / np.sqrt(len(x)))\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pn.concat_behav_trials.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.planning_data_by_bin.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pn.concat_behav_trials.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pn.concat_neural_trials.shape)\n",
    "print(pn.concat_behav_trials.shape)\n",
    "\n",
    "pn.separate_test_and_control_data()\n",
    "# columns_of_interest = ['whether_test', 'cur_ff_distance', 'cur_ff_angle', 'nxt_ff_distance', 'nxt_ff_rel_y', 'nxt_opt_arc_dheading', 'nxt_ff_rel_x', 'nxt_ff_angle', 'nxt_ff_angle_at_ref']\n",
    "columns_of_interest = pn.concat_behav_trials.columns\n",
    "all_results = []\n",
    "#for test_or_control in ['both']: #['test', 'control', 'both']:\n",
    "for test_or_control in ['test', 'control', 'both']:\n",
    "    x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control=test_or_control)\n",
    "    \n",
    "    results_summary = ml_methods_utils.run_segment_split_regression_cv(\n",
    "        x_var, \n",
    "        y_var, \n",
    "        columns_of_interest, \n",
    "        num_folds=5, \n",
    "    )\n",
    "    results_summary['test_or_control'] = test_or_control\n",
    "    all_results.append(results_summary)\n",
    "\n",
    "all_results = pd.concat(all_results)\n",
    "all_results.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reg_results = all_results[all_results['Model'] == 'Linear Regression']\n",
    "class_results = all_results[all_results['Model'] == 'Logistic Regression']\n",
    "\n",
    "\n",
    "\n",
    "# first only plot key_features2 cur_ff_distance\n",
    "for metric in ['test_r2']:\n",
    "    ml_methods_utils.make_barplot_to_compare_results(\n",
    "            reg_results, \n",
    "            metric=metric, \n",
    "            features=key_features2,\n",
    "        )\n",
    "    print('='*100)\n",
    "    print('='*100)\n",
    "\n",
    "\n",
    "\n",
    "rest_of_features = [c for c in reg_results['Feature'].unique() if c not in key_features2]\n",
    "\n",
    "# regression results\n",
    "for metric in ['test_r2']:\n",
    "    ml_methods_utils.make_barplot_to_compare_results(\n",
    "            reg_results, \n",
    "            metric=metric, \n",
    "            features=rest_of_features,\n",
    "        )\n",
    "    print('='*100)\n",
    "    print('='*100)\n",
    "    \n",
    "# classification results\n",
    "for metric in ['test_roc_auc', 'test_f1']:\n",
    "    ml_methods_utils.make_barplot_to_compare_results(\n",
    "        class_results, \n",
    "        metric=metric, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pn_decoding_utils.run_cv_decoding(\n",
    "    X=x_var,\n",
    "    y_df=y_var,\n",
    "    behav_features=key_features2,\n",
    "    groups=y_var['new_segment'].values,\n",
    "    n_splits=5,\n",
    "    config=pn_decoding_utils.DecodingRunConfig(\n",
    "        fast_mode=False,\n",
    "        make_plots=True,\n",
    "        n_jobs=-1,\n",
    "        use_early_stopping=False,  # matches original behavior\n",
    "    ),\n",
    "    context_label='pooled',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num visible ff (Regression on Subsets)\n",
    "\n",
    "(Take out subsets of data based on num_ff_visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pn.planning_data_by_point['num_ff_visible'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control='both')\n",
    "y_var, cols_added = pn_decoding_utils.add_interaction_terms_and_features(y_var)\n",
    "y_var, added_cols = pn_decoding_utils.prep_behav(y_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_visibility_col = 'num_ff_visible'\n",
    "\n",
    "config = pn_decoding_utils.DecodingRunConfig(\n",
    "    fast_mode=False,\n",
    "    make_plots=False,\n",
    "    n_jobs=-1,\n",
    "    use_early_stopping=True,\n",
    ")\n",
    "\n",
    "results_df = pn_decoding_utils.decode_by_num_ff_visible_or_in_memory(\n",
    "    x_var,\n",
    "    y_var,\n",
    "    key_features2,\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for behav_feature in results_df['behav_feature'].unique():\n",
    "    display(\n",
    "        results_df[results_df['behav_feature'] == behav_feature][[ff_visibility_col, 'r2_cv', 'r_cv', 'rmse_cv', 'n_samples']]\n",
    "        .sort_values(by='r2_cv', ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .style\n",
    "        .format(precision=3)\n",
    "        .set_caption(behav_feature)\n",
    "    )\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_visibility_col = 'num_ff_in_memory'\n",
    "ff_visibility_col = 'num_ff_visible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "save_path = os.path.join(pn.planning_and_neural_folder_path, 'pn_decoding', 'conditioned_on_ff_visibility')\n",
    "for model_name, spec in pn_decoding_model_specs.MODEL_SPECS.items():\n",
    "    \n",
    "    config = pn_decoding_utils.DecodingRunConfig(\n",
    "        model_class=spec['model_class'],\n",
    "        model_kwargs=spec['model_kwargs'],\n",
    "        use_early_stopping=False,\n",
    "        make_plots=False,\n",
    "    )\n",
    "\n",
    "    results_df = pn_decoding_utils.decode_by_num_ff_visible_or_in_memory(\n",
    "        x_var,\n",
    "        y_var,\n",
    "        key_features2,\n",
    "        config=config,\n",
    "        save_path=save_path,\n",
    "        ff_visibility_col=ff_visibility_col,\n",
    "        \n",
    "    )\n",
    "\n",
    "    results_df['model_name'] = model_name\n",
    "    all_results.append(results_df)\n",
    "\n",
    "all_results_df = pd.concat(all_results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pn_decoding.plot_decoding_heatmaps_with_n(\n",
    "    all_results_df,\n",
    "    ff_visibility_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for behav_feature in all_results_df['behav_feature'].unique():\n",
    "    sub = all_results_df.query('behav_feature == @behav_feature')\n",
    "\n",
    "    df = (\n",
    "        sub\n",
    "        .pivot_table(\n",
    "            index=ff_visibility_col,\n",
    "            columns='model_name',\n",
    "            values='r2_cv',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # ---- compute n_samples per visibility level ----\n",
    "    n_per_row = (\n",
    "        sub\n",
    "        .groupby(ff_visibility_col)['n_samples']\n",
    "        .mean()   # or .first() if guaranteed identical\n",
    "        .loc[df.index]\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # ---- build ytick labels with sample size ----\n",
    "    yticklabels = [\n",
    "        f'{idx} ({n})'\n",
    "        for idx, n in zip(df.index, n_per_row)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(1.2 * df.shape[1], 1.2 * df.shape[0]))\n",
    "    vmax = max(0.1, df.max().max())\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        df,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='viridis',\n",
    "        vmin=0,\n",
    "        vmax=vmax,\n",
    "        cbar_kws={'label': 'CV $R^2$'}\n",
    "    )\n",
    "\n",
    "    ax.set_yticklabels(yticklabels, rotation=0)\n",
    "\n",
    "    plt.title(behav_feature)\n",
    "    plt.ylabel(f'{ff_visibility_col} (with sample size)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "# for behav_feature in all_results_df['behav_feature'].unique():\n",
    "#     display(\n",
    "#         all_results_df[all_results_df['behav_feature'] == behav_feature][[ff_visibility_col, 'model_name', 'r2_cv', 'r_cv', 'rmse_cv', 'n_samples']]\n",
    "#         .sort_values(by='r2_cv', ascending=False)\n",
    "#         .reset_index(drop=True)\n",
    "#         .style\n",
    "#         .format(precision=3)\n",
    "#         .set_caption(behav_feature)\n",
    "#     )\n",
    "    \n",
    "#     # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cur ff visible only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_cur_only = pn_decoding_utils.decode_cur_ff_only(\n",
    "    x_var,\n",
    "    y_var,\n",
    "    key_features2,\n",
    "    ff_visibility_col=ff_visibility_col,\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df2 = pd.concat(\n",
    "    [all_results_df, results_cur_only],\n",
    "    ignore_index=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_visibility_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pn_decoding.plot_decoding_heatmaps_with_n(\n",
    "    all_results_df2,\n",
    "    ff_visibility_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point-wise regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point-wise segment regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.retrieve_or_make_time_resolved_cv_scores_gpfa(latent_dimensionality=7, exists_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.plot_time_resolved_regression(time_resolved_cv_scores = pn.time_resolved_cv_scores_gpfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.time_resolved_cv_scores_gpfa['trial_count'] = pn.time_resolved_cv_scores_gpfa['train_trial_count'].astype(int)\n",
    "# features_to_plot = None\n",
    "features_to_plot=['time_rel_to_stop', 'cur_ff_distance', 'cur_ff_distance_at_ref', 'time_since_last_capture']\n",
    "pn.plot_time_resolved_regression(time_resolved_cv_scores = pn.time_resolved_cv_scores_gpfa, features_to_plot=features_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_resolved_regression.plot_trial_counts_by_timepoint(\n",
    "            pn.time_resolved_cv_scores_gpfa, 'trial_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_behav_trials[features_to_plot].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point-wise segment regression (for ppt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_time_resolved_cv_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_to_plot = [\n",
    "'time', 'time_rel_to_stop',\n",
    "'target_distance',\n",
    "'target_angle',\n",
    "'target_rel_x',\n",
    "'target_rel_y',\n",
    "'speed',\n",
    "'stop']\n",
    "\n",
    "pn.time_resolved_cv_scores.loc[pn.time_resolved_cv_scores['feature'] == 'monkey_speeddummy', 'feature'] = 'stop'\n",
    "pn.plot_time_resolved_regression(features_to_plot=features_to_plot, n_behaviors_per_plot=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.time_resolved_cv_scores.loc[pn.time_resolved_cv_scores['feature'] == 'monkey_speeddummy', 'feature'] = 'stop'\n",
    "for features in [['target_distance', 'target_rel_y'],\n",
    "                 ['target_rel_x', 'target_angle'],\n",
    "                 ['time', 'time_rel_to_stop'],\n",
    "                 ['speed', 'stop']]:\n",
    "    \n",
    "    pn.plot_time_resolved_regression(features_to_plot=features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.plot_trial_counts_by_timepoint()  # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point-wise segment regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.prepare_seg_aligned_data()\n",
    "pn.get_gpfa_traj(latent_dimensionality=7, exists_ok=True)\n",
    "\n",
    "use_raw_spike_data_instead = False\n",
    "use_lagged_rebinned_behav_data = False\n",
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=use_raw_spike_data_instead,\n",
    "                                  use_lagged_rebinned_behav_data=use_lagged_rebinned_behav_data,\n",
    "                                  apply_pca_on_raw_spike_data=True,\n",
    "                                  use_lagged_raw_spike_data=False,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.retrieve_or_make_time_resolved_cv_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.plot_time_resolved_regression(features_to_plot=['time_rel_to_stop', 'cur_ff_distance', 'cur_ff_distance_at_ref', 'time_since_last_capture'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.plot_time_resolved_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare cv score: GPFA Inside vs Outside CV Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'event_time'\n",
    "new_cv_scores = pd.concat([pn.time_resolved_cv_scores[['new_bin', 'bin_mid_time', 'trial_count', feature]], \n",
    "                           pn.time_resolved_cv_scores_gpfa[[feature]].rename(columns={feature: f'{feature}_cv_w_gpfa'})], axis=1)\n",
    "pn.plot_time_resolved_regression(time_resolved_cv_scores=new_cv_scores, score_threshold_to_plot=None,\n",
    "                                 rank_by_max_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_features = pn.time_resolved_cv_scores.max().sort_values(ascending=False).index.values\n",
    "features_not_to_plot = ['new_bin', 'new_seg_duration', 'trial_count', 'bin_mid_time']\n",
    "ranked_features = [feature for feature in ranked_features if feature not in features_not_to_plot]\n",
    "for feature in ranked_features:\n",
    "    print(feature)\n",
    "    print('='*100)\n",
    "    new_cv_scores = pd.concat([pn.time_resolved_cv_scores[['new_bin', 'bin_mid_time', 'trial_count', feature]], \n",
    "                            pn.time_resolved_cv_scores_gpfa[[feature]].rename(columns={feature: f'{feature}_cv_w_gpfa'})], axis=1)\n",
    "    pn.plot_time_resolved_regression(time_resolved_cv_scores=new_cv_scores, score_threshold_to_plot=None,\n",
    "                                        rank_by_max_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trial count per time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.plot_trial_counts_by_timepoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_timecolored_average(pn.trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_uniform_color(pn.trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, enable interactive mode in your notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create the interactive plot\n",
    "fig, ax = plot_gpfa_utils.plot_gpfa_traj_3d(\n",
    "    trajectories=pn.trajectories,\n",
    "    figsize=(15, 5),\n",
    "    linewidth_single_trial=0.75,\n",
    "    alpha_single_trial=0.3,\n",
    "    linewidth_trial_average=2,\n",
    "    title='Latent dynamics extracted by GPFA',\n",
    "    view_azim=-5,\n",
    "    view_elev=60\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_gpfa_utils.plot_gpfa_traj_3d_plotly(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variance explained by each latent dimension\n",
    "traj_stack = np.stack(pn.trajectories, axis=0)  # shape: (n_trials, 3, T)\n",
    "var_by_dim = np.var(traj_stack, axis=(0, 2))    # variance across trials and time\n",
    "var_by_dim /= var_by_dim.sum()               # normalize to get explained variance ratio\n",
    "print(\"Variance explained by each latent dimension:\", var_by_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.set_title('Latent dynamics extracted by GPFA')\n",
    "ax.set_xlabel('Time [s]')\n",
    "\n",
    "average_trajectory = np.mean(pn.trajectories, axis=0)\n",
    "time = np.arange(len(average_trajectory[0])) * pn.bin_width  # assuming all trajectories have the same length\n",
    "\n",
    "for i, x in enumerate(average_trajectory):\n",
    "    ax.plot(time, x, label=f'Dim {i+1}')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check corr between vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'new_segment'\n",
    "feature2 = 'target_index'\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "x = pn.concat_behav_trials[feature1].values\n",
    "y = pn.concat_behav_trials[feature2].values\n",
    "\n",
    "r, p = pearsonr(x, y)\n",
    "print(f\"Pearson r: {r:.16f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data\n",
    "x = pn.concat_behav_trials[feature1].values.reshape(-1, 1)  # Ensure x is 2D\n",
    "y = pn.concat_behav_trials[feature2].values          # y can remain 1D\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predict y values for the regression line\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# Plot scatter and regression line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, color='blue', label='Data points', alpha=0.6)\n",
    "plt.plot(x, y_pred, color='red', linewidth=2, label='Regression line')\n",
    "plt.xlabel('Behavioral Time')\n",
    "plt.ylabel('Neural Event Time')\n",
    "plt.title('Linear Regression of Neural vs Behavioral Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why poor performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_data_analysis.neural_analysis_tools.gpfa_methods.time_resolved_regression as time_resolved_regression\n",
    "\n",
    "# 1. Print number of trials per timepoint\n",
    "time_resolved_regression.print_trials_per_timepoint(pn.gpfa_neural_trials)\n",
    "\n",
    "# 2. Check for NaNs\n",
    "time_resolved_regression.check_for_nans_in_trials(pn.gpfa_neural_trials, name='latent')\n",
    "time_resolved_regression.check_for_nans_in_trials(pn.behav_trials, name='behavioral')\n",
    "\n",
    "# 3. Standardize trials\n",
    "latent_trials_std = time_resolved_regression.standardize_trials(pn.gpfa_neural_trials)\n",
    "behav_trials_std = time_resolved_regression.standardize_trials(pn.behav_trials)\n",
    "\n",
    "# 4. Plot latent and behavioral variables for a few trials\n",
    "time_resolved_regression.plot_latents_and_behav_trials(latent_trials_std, behav_trials_std, pn.bin_width, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why time prediction is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see rel_cur_ff_first_seen_time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_seen_time_df = pn.planning_data_by_point[['rel_cur_ff_first_seen_time_bbas', 'rel_cur_ff_last_seen_time_bbas']].drop_duplicates().reset_index(drop=True)\n",
    "sns.histplot(rel_seen_time_df['rel_cur_ff_first_seen_time_bbas'], bins=50, label='cur ff first seen')\n",
    "sns.histplot(rel_seen_time_df['rel_cur_ff_last_seen_time_bbas'], bins=50, label='cur ff last seen')\n",
    "plt.xlabel('Time relative to stop (s)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Time relative to stop')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug inconsistent number of new_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example trajectories\n",
    "for traj in pn.trajectories[:]:\n",
    "    print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj in pn.behav_trials[:]:\n",
    "    print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = pn.rebinned_behav_data.groupby('new_segment').size()[pn.rebinned_behav_data.groupby('new_segment').size() < 10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_behav_data[pn.rebinned_behav_data['new_segment'].isin(segments)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp: to match (new_segment, new_bin) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pn_aligned_by_event)\n",
    "reload(gpfa_helper_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.prepare_seg_aligned_data(cur_or_nxt='cur', first_or_last='last', time_limit_to_count_sighting=2,\n",
    "                              start_t_rel_event=1, end_t_rel_event=1.25, rebinned_max_x_lag_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var['bin_mid_time_rel_to_event'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var.groupby('new_segment').min()['new_bin'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_x_var.groupby('new_segment').min()['new_bin'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var.groupby('new_segment').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example trajectories\n",
    "for traj in pn.trajectories[:]:\n",
    "    print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.gpfa_neural_trials[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_gpfa_traj(latent_dimensionality=7, exists_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression later\n",
    "use_raw_spike_data_instead = False\n",
    "\n",
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=False,\n",
    "                                    use_lagged_raw_spike_data=False,\n",
    "                                    apply_pca_on_raw_spike_data=False,\n",
    "                                    num_pca_components=7)\n",
    "\n",
    "\n",
    "pn.print_data_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `pn.concat_behav_trials` and `pn.concat_neural_trials` are DataFrames:\n",
    "\n",
    "# Convert the relevant columns to sets of tuples\n",
    "behav_set = set(map(tuple, pn.concat_behav_trials[['new_segment', 'new_bin']].values))\n",
    "neural_set = set(map(tuple, pn.concat_neural_trials[['new_segment', 'new_bin']].values))\n",
    "\n",
    "# Compute the difference\n",
    "diff = behav_set - neural_set\n",
    "diff2 = neural_set - behav_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pn.rebinned_behav_data[['new_segment', 'new_bin']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_neural_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_behav_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.concat_neural_trials[['new_segment', 'new_bin']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example trajectories\n",
    "for traj in pn.trajectories[:5]:\n",
    "    print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj in pn.behav_trials[:5]:\n",
    "    print(traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pn.rebinned_behav_data[pn.rebinned_behav_data['new_segment'].isin([45])]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_behav_data.loc[700:730]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point-wise regression on one var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import sys\n",
    "from data_wrangling import process_monkey_information, specific_utils, further_processing_class, specific_utils, general_utils\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from null_behaviors import curvature_utils, curv_of_traj_utils\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from matplotlib import rc\n",
    "from os.path import exists\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from elephant.gpfa import GPFA\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_cols = [col for col in pn.concat_neural_trials.columns if col.startswith('dim_')]\n",
    "# x_df = pn.concat_neural_trials[x_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for new_bin in pn.concat_neural_trials['new_bin'].unique():\n",
    "    x_df2 = pn.concat_neural_trials[pn.concat_neural_trials['new_bin'] == new_bin]\n",
    "    x_df2 = x_df2[[col for col in x_df2.columns if col.startswith('dim_')]]\n",
    "\n",
    "    # Add intercept\n",
    "    x_df2 = sm.add_constant(x_df2)\n",
    "\n",
    "    y_df2 = pn.concat_behav_trials[pn.concat_behav_trials['new_bin'] == new_bin][['segment']].copy()\n",
    "\n",
    "    # Train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_df2, y_df2, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit OLS\n",
    "    model = sm.OLS(y_train, x_train)\n",
    "    results = model.fit()\n",
    "\n",
    "    print(results.summary())\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = results.predict(x_test)\n",
    "    y_test_flat = y_test.squeeze()\n",
    "    r2_test = 1 - ((y_test_flat - y_pred) ** 2).sum() / ((y_test_flat - y_test_flat.mean()) ** 2).sum()\n",
    "    print(f\"Test R² score: {r2_test:.4f}\")\n",
    "    \n",
    "    # Create a comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'y_test': y_test.squeeze().values,  # Ground truth\n",
    "        'y_pred': y_pred                    # Model predictions\n",
    "    })\n",
    "\n",
    "    print(comparison_df.head(10))  # Show the first 10 rows\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Ensure both arrays are 1D\n",
    "y_test_flat = y_test.squeeze().values\n",
    "y_pred_flat = y_pred\n",
    "\n",
    "# Manually compute R² (already done, for reference)\n",
    "r2_test = 1 - ((y_test_flat - y_pred_flat) ** 2).sum() / ((y_test_flat - y_test_flat.mean()) ** 2).sum()\n",
    "\n",
    "# Compute Pearson correlation coefficient (R)\n",
    "if len(np.unique(y_test_flat)) > 1:\n",
    "    r_test = np.corrcoef(y_test_flat, y_pred_flat)[0, 1]\n",
    "    # Or alternatively: r_test, _ = pearsonr(y_test_flat, y_pred_flat)\n",
    "else:\n",
    "    r_test = np.nan  # Correlation is undefined when y is constant\n",
    "\n",
    "print(f\"Test R² score: {r2_test:.4f}\")\n",
    "print(f\"Test R (Pearson correlation): {r_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-6, 6, 13)\n",
    "kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "model = RidgeCV(alphas=alphas, fit_intercept=True)\n",
    "try:\n",
    "    score = cross_val_score(\n",
    "        model, x_df2, y_df2.values.ravel(), cv=kf, scoring='r2', n_jobs=1)\n",
    "    print(score.mean())\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR no CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate linear regression\n",
    "pn.y_var_lr_df = neural_data_modeling.get_y_var_lr_df(\n",
    "                pn.concat_neural_trials, pn.concat_behav_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.y_var_lr_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR on ind var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_or_control in ['both']:\n",
    "    x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control=test_or_control)\n",
    "    y_var = y_var[['time_rel_to_stop', 'time_since_last_capture']].copy()\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=42)\n",
    "    # use linear regression\n",
    "    for y_var_column in y_var.columns:\n",
    "        summary_df, y_pred, results, r2_test = regression_utils.use_linear_regression(\n",
    "            X_train, X_test, y_train[y_var_column], y_test[y_var_column], show_plot=True, y_var_name=y_var_column)\n",
    "        print(summary_df)\n",
    "        print(y_pred)\n",
    "        print(results)\n",
    "        print(r2_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manually save scores_by_time_full_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimensionality = 7\n",
    "cv_folds = 5\n",
    "bin_width_str = f\"{pn.bin_width:.4f}\".rstrip(\n",
    "    '0').rstrip('.').replace('.', 'p')\n",
    "file_name = f'scores_bin{bin_width_str}_{pn.cur_or_nxt}_{pn.first_or_last}_d{latent_dimensionality}_cv{cv_folds}.csv'\n",
    "\n",
    "\n",
    "time_resolved_cv_scores_gpfa_folder_path = os.path.join(\n",
    "                    pn.gpfa_data_folder_path, \"time_resolved_cv_scores_gpfa\")\n",
    "os.makedirs(time_resolved_cv_scores_gpfa_folder_path, exist_ok=True)\n",
    "time_resolved_cv_scores_path = os.path.join(\n",
    "            time_resolved_cv_scores_gpfa_folder_path, file_name)\n",
    "\n",
    "time_resolved_cv_scores_gpfa.to_csv(time_resolved_cv_scores_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resolved_cv_scores_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resolved_cv_scores_gpfa = time_resolved_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
