{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, ml_methods_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools import glm_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "from numpy import pi\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pn_helper_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_seg.PlanningAndNeuralSegmentAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "    pn.planning_data_by_point)\n",
    "pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get planning_data by segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.prepare_seg_aligned_data(segment_duration=2, rebinned_max_x_lag_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.rebinned_x_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression later\n",
    "use_raw_spike_data_instead = False\n",
    "\n",
    "pn.get_concat_data_for_regression(use_raw_spike_data_instead=True,\n",
    "                                    use_lagged_raw_spike_data=False,\n",
    "                                    apply_pca_on_raw_spike_data=False,\n",
    "                                    num_pca_components=7)\n",
    "pn.print_data_dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get train and test trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM most recent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials=8\n",
    "trial_len=300\n",
    "dt=0.01\n",
    "seed=7\n",
    "data = simulate_multiff_trials(n_trials=n_trials, trial_len=trial_len, dt=dt, seed=seed)\n",
    "res, design_df, metrics, meta = fit_multiff_glm(\n",
    "    dt=data[\"dt\"], trial_ids=data[\"trial_ids\"],\n",
    "    cur_vis=data[\"cur_vis\"], nxt_vis=data[\"nxt_vis\"],\n",
    "    cur_dist=data[\"cur_dist\"], nxt_dist=data[\"nxt_dist\"],\n",
    "    cur_angle=data[\"cur_angle\"], nxt_angle=data[\"nxt_angle\"],\n",
    "    heading=data[\"heading\"], speed=data[\"speed\"], curvature=data[\"curvature\"],\n",
    "    spike_counts=data[\"spike_counts\"], l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")\n",
    "print(res.summary())\n",
    "print(\"Overall deviance:\", metrics[\"deviance\"])  \n",
    "print(\"Pseudo-R^2:\", metrics[\"pseudo_R2\"])  \n",
    "print(\"Per-trial deviance (head):\", metrics[\"per_trial_deviance\"].head())\n",
    "cv = fit_and_score_cv(design_df, data[\"spike_counts\"], data[\"dt\"], data[\"trial_ids\"], n_splits=5, l2=0.0, cluster_se=False)\n",
    "print(\"Trial-wise CV:\", cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_kernels(res, design_df, meta, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_all_kernels_flat(res, design_df, data[\"spike_counts\"], data[\"trial_ids\"], meta, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trial-aware Poisson GLM for multiFF neural spike data (statsmodels)\n",
    "------------------------------------------------------------------\n",
    "End-to-end, runnable module:\n",
    "- Raised-cosine bases\n",
    "- Trial-aware stimulus & spike-history design (no cross-trial leakage)\n",
    "- Optional trial fixed effects or cluster-robust SEs by trial\n",
    "- Poisson fitting (statsmodels.GLM), prediction, metrics, trial-wise CV\n",
    "- MultiFF adapter with visibility/distance/angle/heading features\n",
    "- Stable simulators using capped Poisson intensities\n",
    "- Demos: run `python this_file.py` or call `run_multiff_demo()`\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "from scipy import signal\n",
    "from scipy.linalg import toeplitz\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# =====================\n",
    "# Helpers & bases\n",
    "# =====================\n",
    "\n",
    "def _unique_trials(trial_ids: np.ndarray) -> np.ndarray:\n",
    "    return np.unique(np.asarray(trial_ids))\n",
    "\n",
    "\n",
    "def raised_cosine_basis(n_basis: int, t_max: float, dt: float, *, t_min: float = 0.0,\n",
    "                        log_spaced: bool = True, eps: float = 1e-3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Causal raised-cosine basis that tiles [t_min, t_max].\n",
    "    Returns lags (L,), B (L x K) with unit-area columns (sum * dt = 1).\n",
    "    \"\"\"\n",
    "    lags = np.arange(0.0, t_max + 1e-12, dt)\n",
    "    K = int(n_basis)\n",
    "\n",
    "    def warp(x):\n",
    "        return np.log(x + eps) if log_spaced else x\n",
    "\n",
    "    W = warp(lags)\n",
    "    W_min, W_max = warp(t_min), warp(t_max)\n",
    "    centers = np.linspace(W_min, W_max, K)\n",
    "    delta = centers[1] - centers[0] if K > 1 else (W_max - W_min + 1e-12)\n",
    "    width = delta * 1.5\n",
    "\n",
    "    B = []\n",
    "    for c in centers:\n",
    "        arg = (W - c) / width\n",
    "        bk = np.cos(np.clip(arg, -np.pi, np.pi))\n",
    "        bk[np.abs(arg) > np.pi] = 0.0\n",
    "        bk = np.maximum(bk, 0.0)\n",
    "        bk[lags < t_min] = 0.0\n",
    "        area = bk.sum() * dt\n",
    "        if area > 0:\n",
    "            bk = bk / area\n",
    "        B.append(bk)\n",
    "    B = np.column_stack(B) if B else np.zeros((len(lags), 0))\n",
    "    return lags, B\n",
    "\n",
    "\n",
    "def safe_poisson_lambda(eta: float | np.ndarray, dt: float, *, max_rate_hz: float = 200.0) -> np.ndarray:\n",
    "    \"\"\"Convert log-rate `eta` (per-second) to expected bin count lambda, with a cap.\"\"\"\n",
    "    log_min = np.log(1e-6)\n",
    "    log_max = np.log(max_rate_hz)\n",
    "    eta_clipped = np.clip(eta, log_min, log_max)\n",
    "    rate_hz = np.exp(eta_clipped)\n",
    "    return rate_hz * dt\n",
    "\n",
    "\n",
    "def wrap_angle(theta: np.ndarray) -> np.ndarray:\n",
    "    return (theta + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "\n",
    "def angle_sin_cos(theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    th = wrap_angle(theta)\n",
    "    return np.sin(th), np.cos(th)\n",
    "\n",
    "\n",
    "def onset_from_mask_trials(mask: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    mask = (mask > 0).astype(int)\n",
    "    on = np.zeros_like(mask, dtype=float)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        m = mask[idx]\n",
    "        d = np.diff(np.r_[0, m])\n",
    "        on[idx] = (d == 1).astype(float)\n",
    "    return on\n",
    "\n",
    "\n",
    "def offset_from_mask_trials(mask: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    mask = (mask > 0).astype(int)\n",
    "    off = np.zeros_like(mask, dtype=float)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        m = mask[idx]\n",
    "        d = np.diff(np.r_[m, 0])\n",
    "        off[idx] = (d == 1).astype(float)\n",
    "    return off\n",
    "\n",
    "# =====================\n",
    "# Trial-aware design builders\n",
    "# =====================\n",
    "\n",
    "def lagged_design_from_signal_trials(x: np.ndarray, basis: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    T = len(x)\n",
    "    L, K = basis.shape\n",
    "    Xk = np.zeros((T, K))\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        xt = x[idx]\n",
    "        for k in range(K):\n",
    "            y = signal.convolve(xt, basis[:, k], mode=\"full\")[: len(idx)]\n",
    "            Xk[idx, k] = y\n",
    "    return Xk\n",
    "\n",
    "\n",
    "def spike_history_design_with_trials(y_counts: np.ndarray, basis: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    T = len(y_counts)\n",
    "    L, K = basis.shape\n",
    "    Xh = np.zeros((T, K))\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        y_tr = y_counts[idx]\n",
    "        col0 = np.r_[0, y_tr[:-1]]  # strictly past\n",
    "        toepl = toeplitz(col0, np.zeros(L))\n",
    "        Xh[idx, :] = toepl @ basis\n",
    "    return Xh\n",
    "\n",
    "\n",
    "def build_glm_design_with_trials(\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    stimulus_dict: Optional[Dict[str, np.ndarray]] = None,\n",
    "    stimulus_basis_dict: Optional[Dict[str, np.ndarray]] = None,\n",
    "    spike_counts: Optional[np.ndarray] = None,\n",
    "    history_basis: Optional[np.ndarray] = None,\n",
    "    extra_covariates: Optional[Dict[str, np.ndarray]] = None,\n",
    "    use_trial_FE: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Optional[np.ndarray]]:\n",
    "    T = len(trial_ids)\n",
    "    cols: List[np.ndarray] = []\n",
    "    names: List[str] = []\n",
    "\n",
    "    if stimulus_dict is not None:\n",
    "        for name, x in stimulus_dict.items():\n",
    "            if stimulus_basis_dict is not None and name in stimulus_basis_dict:\n",
    "                B = stimulus_basis_dict[name]\n",
    "                Xk = lagged_design_from_signal_trials(x, B, trial_ids)\n",
    "                for k in range(Xk.shape[1]):\n",
    "                    cols.append(Xk[:, k]); names.append(f\"{name}_rc{k+1}\")\n",
    "            else:\n",
    "                cols.append(x); names.append(name)\n",
    "\n",
    "    if spike_counts is not None and history_basis is not None:\n",
    "        Xh = spike_history_design_with_trials(spike_counts, history_basis, trial_ids)\n",
    "        for k in range(Xh.shape[1]):\n",
    "            cols.append(Xh[:, k]); names.append(f\"hist_rc{k+1}\")\n",
    "\n",
    "    if extra_covariates is not None:\n",
    "        for n, v in extra_covariates.items():\n",
    "            cols.append(v); names.append(n)\n",
    "\n",
    "    X = np.column_stack(cols) if cols else np.zeros((T, 0))\n",
    "    design_df = pd.DataFrame(X, columns=names)\n",
    "\n",
    "    if use_trial_FE:\n",
    "        trial_FE = pd.get_dummies(trial_ids, prefix=\"trial\", drop_first=True)\n",
    "        design_df = pd.concat([design_df, trial_FE], axis=1)\n",
    "\n",
    "    y = spike_counts if spike_counts is not None else None\n",
    "    return design_df, y\n",
    "\n",
    "# =====================\n",
    "# Fitting & metrics\n",
    "# =====================\n",
    "\n",
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    return np.column_stack([np.ones(len(X)), X])\n",
    "\n",
    "\n",
    "\n",
    "def fit_poisson_glm_trials(\n",
    "    design_df: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    *,\n",
    "    add_const: bool = True,\n",
    "    l2: float = 0.0,\n",
    "    cluster_se: bool = False,\n",
    "):\n",
    "    \"\"\"Fit Poisson GLM keeping column names (DataFrame) so kernels map correctly.\n",
    "    Uses exposure=dt. If l2>0, uses fit_regularized (SEs not available).\n",
    "    If cluster_se=False and l2==0, uses cluster-robust SEs grouped by trial.\n",
    "    \"\"\"\n",
    "    X_df = design_df.copy()\n",
    "    if add_const:\n",
    "        X_df = sm.add_constant(X_df, has_constant='add')  # preserves 'const' and column names\n",
    "    exposure = np.full_like(y, fill_value=dt, dtype=float)\n",
    "\n",
    "    model = sm.GLM(y, X_df, family=sm.families.Poisson(), exposure=exposure)\n",
    "    if l2 > 0:\n",
    "        res = model.fit_regularized(alpha=l2, L1_wt=0.0, maxiter=1000)\n",
    "        return res\n",
    "    else:\n",
    "        if cluster_se:\n",
    "            return model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": trial_ids})\n",
    "        else:\n",
    "            return model.fit()\n",
    "        \n",
    "def predict_mu(result, design_df: pd.DataFrame, dt: float, add_const: bool = True) -> np.ndarray:\n",
    "    \"\"\"Predict mean counts with exposure=dt, aligning columns to the fitted model.\"\"\"\n",
    "    X_df = design_df.copy()\n",
    "    if add_const:\n",
    "        X_df = sm.add_constant(X_df, has_constant='add')\n",
    "    mu = result.predict(X_df, exposure=np.full(len(X_df), dt))\n",
    "    return np.asarray(mu, dtype=float)\n",
    "\n",
    "\n",
    "def poisson_deviance(y: np.ndarray, mu: np.ndarray) -> float:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    mu = np.asarray(mu, dtype=float)\n",
    "    eps = 1e-12\n",
    "    term = np.where(y > 0, y * np.log((y + eps) / (mu + eps)), 0.0)\n",
    "    dev = 2.0 * np.sum(term - (y - mu))\n",
    "    return float(dev)\n",
    "\n",
    "\n",
    "def pseudo_R2(y: np.ndarray, mu_full: np.ndarray, mu_null: np.ndarray) -> float:\n",
    "    eps = 1e-12\n",
    "    ll_full = np.sum(y * np.log(mu_full + eps) - mu_full)\n",
    "    ll_null = np.sum(y * np.log(mu_null + eps) - mu_null)\n",
    "    return float(1.0 - ll_full / ll_null)\n",
    "\n",
    "\n",
    "def per_trial_deviance(y: np.ndarray, mu: np.ndarray, trial_ids: np.ndarray) -> pd.DataFrame:\n",
    "    dev = pd.DataFrame({\"trial\": trial_ids, \"y\": y, \"mu\": mu})\n",
    "    eps = 1e-12\n",
    "    dev[\"bin_dev\"] = np.where(dev[\"y\"] > 0, dev[\"y\"] * np.log((dev[\"y\"] + eps) / (dev[\"mu\"] + eps)), 0.0) - (dev[\"y\"] - dev[\"mu\"])\n",
    "    out = dev.groupby(\"trial\", as_index=False)[\"bin_dev\"].sum()\n",
    "    out.rename(columns={\"bin_dev\": \"trial_deviance\"}, inplace=True)\n",
    "    out[\"trial_deviance\"] *= 2.0\n",
    "    return out\n",
    "\n",
    "# =====================\n",
    "# CV utilities\n",
    "# =====================\n",
    "\n",
    "def trialwise_folds(trial_ids: np.ndarray, n_splits: int, *, shuffle: bool = False, random_state: Optional[int] = None) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    trials = _unique_trials(trial_ids)\n",
    "    if shuffle:\n",
    "        trials = trials.copy(); rng.shuffle(trials)\n",
    "    folds = np.array_split(trials, n_splits)\n",
    "    out = []\n",
    "    for k in range(n_splits):\n",
    "        val_trials = set(folds[k])\n",
    "        val_mask = np.isin(trial_ids, list(val_trials))\n",
    "        train_mask = ~val_mask\n",
    "        out.append((train_mask, val_mask))\n",
    "    return out\n",
    "\n",
    "\n",
    "def fit_and_score_cv(\n",
    "    design_df: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    *,\n",
    "    n_splits: int = 5,\n",
    "    l2: float = 0.0,\n",
    "    use_trial_FE: bool = True,\n",
    "    cluster_se: bool = False,\n",
    "    random_state: Optional[int] = 0,\n",
    ") -> pd.DataFrame:\n",
    "    folds = trialwise_folds(trial_ids, n_splits, shuffle=True, random_state=random_state)\n",
    "    rows = []\n",
    "    for i, (train_mask, val_mask) in enumerate(folds, start=1):\n",
    "        X_train = design_df.iloc[train_mask]\n",
    "        y_train = y[train_mask]\n",
    "        tr_ids_train = trial_ids[train_mask]\n",
    "\n",
    "        X_val = design_df.iloc[val_mask]\n",
    "        y_val = y[val_mask]\n",
    "        tr_ids_val = trial_ids[val_mask]\n",
    "\n",
    "        res = fit_poisson_glm_trials(X_train, y_train, dt, tr_ids_train,\n",
    "                                     add_const=True, l2=l2, cluster_se=False)\n",
    "        mu_val = predict_mu(res, X_val, dt)\n",
    "        mu_null = np.full_like(y_val, y_train.mean(), dtype=float)\n",
    "\n",
    "        fold_dev = poisson_deviance(y_val, mu_val)\n",
    "        fold_r2 = pseudo_R2(y_val, mu_val, mu_null)\n",
    "        rows.append({\"fold\": i, \"val_deviance\": fold_dev, \"val_pseudo_R2\": fold_r2, \"n_val_bins\": int(val_mask.sum())})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =====================\n",
    "# MultiFF adapter (features & end-to-end)\n",
    "# =====================\n",
    "\n",
    "def build_multiff_design(\n",
    "    *,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    cur_vis: np.ndarray,\n",
    "    nxt_vis: np.ndarray,\n",
    "    cur_dist: np.ndarray,\n",
    "    nxt_dist: np.ndarray,\n",
    "    cur_angle: np.ndarray,\n",
    "    nxt_angle: np.ndarray,\n",
    "    heading: Optional[np.ndarray] = None,\n",
    "    speed: Optional[np.ndarray] = None,\n",
    "    curvature: Optional[np.ndarray] = None,\n",
    "    spike_counts: Optional[np.ndarray] = None,\n",
    "    use_trial_FE: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Optional[np.ndarray], Dict[str, np.ndarray]]:\n",
    "    T = len(trial_ids)\n",
    "\n",
    "    # Bases\n",
    "    _, B_event = raised_cosine_basis(n_basis=6, t_max=0.60, dt=dt, t_min=0.0, log_spaced=True)\n",
    "    _, B_short = raised_cosine_basis(n_basis=5, t_max=0.30, dt=dt, t_min=0.0, log_spaced=True)\n",
    "    _, B_hist  = raised_cosine_basis(n_basis=5, t_max=0.20, dt=dt, t_min=dt,  log_spaced=True)\n",
    "\n",
    "    # Onsets and gated features\n",
    "    cur_on = onset_from_mask_trials(cur_vis, trial_ids)\n",
    "    nxt_on = onset_from_mask_trials(nxt_vis, trial_ids)\n",
    "\n",
    "    cur_dist_g = cur_dist * (cur_vis > 0)\n",
    "    nxt_dist_g = nxt_dist * (nxt_vis > 0)\n",
    "\n",
    "    cur_sin, cur_cos = angle_sin_cos(cur_angle)\n",
    "    nxt_sin, nxt_cos = angle_sin_cos(nxt_angle)\n",
    "    cur_sin *= (cur_vis > 0); cur_cos *= (cur_vis > 0)\n",
    "    nxt_sin *= (nxt_vis > 0); nxt_cos *= (nxt_vis > 0)\n",
    "\n",
    "    stimulus_dict: Dict[str, np.ndarray] = {\n",
    "        \"cur_on\": cur_on,\n",
    "        \"nxt_on\": nxt_on,\n",
    "        \"cur_dist\": cur_dist_g,\n",
    "        \"nxt_dist\": nxt_dist_g,\n",
    "        \"cur_angle_sin\": cur_sin,\n",
    "        \"cur_angle_cos\": cur_cos,\n",
    "        \"nxt_angle_sin\": nxt_sin,\n",
    "        \"nxt_angle_cos\": nxt_cos,\n",
    "    }\n",
    "\n",
    "    extra_covariates: Dict[str, np.ndarray] = {}\n",
    "    if heading is not None:\n",
    "        cur_align = np.cos(wrap_angle(heading - cur_angle)) * (cur_vis > 0)\n",
    "        nxt_align = np.cos(wrap_angle(heading - nxt_angle)) * (nxt_vis > 0)\n",
    "        stimulus_dict[\"cur_align\"] = cur_align\n",
    "        stimulus_dict[\"nxt_align\"] = nxt_align\n",
    "\n",
    "    if speed is not None:   extra_covariates[\"speed\"] = speed\n",
    "    if curvature is not None: extra_covariates[\"curvature\"] = curvature\n",
    "\n",
    "    stimulus_basis_dict: Dict[str, np.ndarray] = {\n",
    "        \"cur_on\": B_event, \"nxt_on\": B_event,\n",
    "        \"cur_dist\": B_short, \"nxt_dist\": B_short,\n",
    "        \"cur_angle_sin\": B_short, \"cur_angle_cos\": B_short,\n",
    "        \"nxt_angle_sin\": B_short, \"nxt_angle_cos\": B_short,\n",
    "        **({\"cur_align\": B_short, \"nxt_align\": B_short} if heading is not None else {}),\n",
    "    }\n",
    "\n",
    "    design_df, y = build_glm_design_with_trials(\n",
    "        dt=dt,\n",
    "        trial_ids=trial_ids,\n",
    "        stimulus_dict=stimulus_dict,\n",
    "        stimulus_basis_dict=stimulus_basis_dict,\n",
    "        spike_counts=spike_counts,\n",
    "        history_basis=B_hist,\n",
    "        extra_covariates=extra_covariates,\n",
    "        use_trial_FE=use_trial_FE,\n",
    "    )\n",
    "\n",
    "    meta = {\"B_event\": B_event, \"B_short\": B_short, \"B_hist\": B_hist}\n",
    "    return design_df, y, meta\n",
    "\n",
    "\n",
    "def fit_multiff_glm(\n",
    "    *,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    cur_vis: np.ndarray,\n",
    "    nxt_vis: np.ndarray,\n",
    "    cur_dist: np.ndarray,\n",
    "    nxt_dist: np.ndarray,\n",
    "    cur_angle: np.ndarray,\n",
    "    nxt_angle: np.ndarray,\n",
    "    heading: Optional[np.ndarray] = None,\n",
    "    speed: Optional[np.ndarray] = None,\n",
    "    curvature: Optional[np.ndarray] = None,\n",
    "    spike_counts: np.ndarray,\n",
    "    l2: float = 0.0,\n",
    "    use_trial_FE: bool = True,\n",
    "    cluster_se: bool = False,\n",
    "):\n",
    "    design_df, y, meta = build_multiff_design(\n",
    "        dt=dt, trial_ids=trial_ids,\n",
    "        cur_vis=cur_vis, nxt_vis=nxt_vis,\n",
    "        cur_dist=cur_dist, nxt_dist=nxt_dist,\n",
    "        cur_angle=cur_angle, nxt_angle=nxt_angle,\n",
    "        heading=heading, speed=speed, curvature=curvature,\n",
    "        spike_counts=spike_counts, use_trial_FE=use_trial_FE,\n",
    "    )\n",
    "    res = fit_poisson_glm_trials(design_df, y, dt, trial_ids, add_const=True, l2=l2, cluster_se=False)\n",
    "    mu = predict_mu(res, design_df, dt)\n",
    "    mu_null = np.full_like(y, y.mean(), dtype=float)\n",
    "    metrics = {\n",
    "        \"deviance\": poisson_deviance(y, mu),\n",
    "        \"pseudo_R2\": pseudo_R2(y, mu, mu_null),\n",
    "        \"per_trial_deviance\": per_trial_deviance(y, mu, trial_ids),\n",
    "    }\n",
    "    return res, design_df, metrics, meta\n",
    "\n",
    "# =====================\n",
    "# Stable simulators\n",
    "# =====================\n",
    "\n",
    "def simulate_spikes_with_trials(\n",
    "    *, n_trials: int = 20, trial_len: int = 300, dt: float = 0.01, seed: int = 1,\n",
    "    max_rate_hz: float = 200.0\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = n_trials * trial_len\n",
    "    trial_ids = np.repeat(np.arange(n_trials), trial_len)\n",
    "\n",
    "    _, B_stim = raised_cosine_basis(n_basis=5, t_max=0.300, dt=dt, t_min=0.0, log_spaced=True)\n",
    "    _, B_hist = raised_cosine_basis(n_basis=4, t_max=0.200, dt=dt, t_min=dt, log_spaced=True)\n",
    "\n",
    "    event = np.concatenate([(rng.random(trial_len) < 0.02).astype(float) for _ in range(n_trials)])\n",
    "    speed = np.concatenate([signal.lfilter([1.0], [1.0, -0.7], rng.normal(0.0, 1.0, size=trial_len)) for _ in range(n_trials)])\n",
    "    speed = (speed - speed.mean()) / (speed.std() + 1e-12)\n",
    "\n",
    "    beta_event = np.array([0.8, 0.5, 0.2, -0.1, -0.2])\n",
    "    beta_speed = np.array([0.4, 0.2, 0.0, -0.1, -0.2])\n",
    "    beta_hist  = np.array([-1.2, -0.8, -0.5, -0.2])\n",
    "\n",
    "    X_event = lagged_design_from_signal_trials(event, B_stim, trial_ids)\n",
    "    X_speed = lagged_design_from_signal_trials(speed, B_stim, trial_ids)\n",
    "\n",
    "    h_hist = B_hist @ beta_hist\n",
    "    Lh = len(h_hist)\n",
    "\n",
    "    baseline_per_trial = -2.8 + 0.4 * rng.standard_normal(n_trials)\n",
    "\n",
    "    y = np.zeros(T, dtype=int)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        past = np.zeros(Lh)\n",
    "        b0 = baseline_per_trial[tr]\n",
    "        for t in idx:\n",
    "            stim_drive = X_event[t] @ beta_event + X_speed[t] @ beta_speed\n",
    "            hist_drive = np.dot(past, h_hist[::-1])\n",
    "            eta = b0 + stim_drive + hist_drive\n",
    "            lam = safe_poisson_lambda(eta, dt, max_rate_hz=max_rate_hz)\n",
    "            y[t] = np.random.poisson(lam)\n",
    "            past = np.roll(past, 1); past[0] = y[t]\n",
    "\n",
    "    stimulus_dict = {\"event\": event, \"speed\": speed}\n",
    "    stimulus_basis_dict = {\"event\": B_stim, \"speed\": B_stim}\n",
    "    return trial_ids, y, stimulus_dict, stimulus_basis_dict, B_hist\n",
    "\n",
    "\n",
    "def simulate_multiff_trials(\n",
    "    *, n_trials: int = 12, trial_len: int = 400, dt: float = 0.01, seed: int = 3,\n",
    "    max_rate_hz: float = 200.0\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = n_trials * trial_len\n",
    "    trial_ids = np.repeat(np.arange(n_trials), trial_len)\n",
    "\n",
    "    def random_vis_mask():\n",
    "        m = np.zeros(trial_len, dtype=int)\n",
    "        t = 0\n",
    "        while t < trial_len:\n",
    "            off = rng.integers(20, 80)\n",
    "            on  = rng.integers(30, 120)\n",
    "            t += off\n",
    "            if t >= trial_len: break\n",
    "            m[t: min(trial_len, t + on)] = 1\n",
    "            t += on\n",
    "        return m\n",
    "\n",
    "    cur_vis = np.concatenate([random_vis_mask() for _ in range(n_trials)])\n",
    "    nxt_vis = np.concatenate([np.r_[np.zeros(rng.integers(40, 120)), random_vis_mask()][:trial_len] for _ in range(n_trials)])\n",
    "\n",
    "    def distance_from_vis(m):\n",
    "        d = np.zeros_like(m, dtype=float)\n",
    "        run = 0.0\n",
    "        for i, v in enumerate(m):\n",
    "            if v:\n",
    "                run = 1.0 if run == 0 else max(0.0, run - 0.02)\n",
    "                d[i] = run\n",
    "            else:\n",
    "                run = 0.0; d[i] = 0.0\n",
    "        return d\n",
    "\n",
    "    cur_dist = np.concatenate([distance_from_vis(cur_vis[i*trial_len:(i+1)*trial_len]) for i in range(n_trials)])\n",
    "    nxt_dist = np.concatenate([distance_from_vis(nxt_vis[i*trial_len:(i+1)*trial_len]) for i in range(n_trials)])\n",
    "\n",
    "    def rand_angle_series(L):\n",
    "        a = np.cumsum(rng.normal(0, 0.05, size=L))\n",
    "        return ((a + np.pi) % (2*np.pi)) - np.pi\n",
    "\n",
    "    cur_angle = np.concatenate([rand_angle_series(trial_len) for _ in range(n_trials)])\n",
    "    nxt_angle = np.concatenate([rand_angle_series(trial_len) for _ in range(n_trials)])\n",
    "    heading   = np.concatenate([rand_angle_series(trial_len) for _ in range(n_trials)])\n",
    "\n",
    "    speed = np.concatenate([signal.lfilter([1.0], [1.0, -0.8], rng.normal(0, 1.0, size=trial_len)) for _ in range(n_trials)])\n",
    "    speed = (speed - speed.mean()) / (speed.std() + 1e-12)\n",
    "    curvature = np.concatenate([rng.normal(0.0, 0.4, size=trial_len) for _ in range(n_trials)])\n",
    "\n",
    "    # Build design (no history yet) to compute stim_drive for simulation\n",
    "    design_nohist, _, meta = build_multiff_design(\n",
    "        dt=dt, trial_ids=trial_ids,\n",
    "        cur_vis=cur_vis, nxt_vis=nxt_vis,\n",
    "        cur_dist=cur_dist, nxt_dist=nxt_dist,\n",
    "        cur_angle=cur_angle, nxt_angle=nxt_angle,\n",
    "        heading=heading, speed=speed, curvature=curvature,\n",
    "        spike_counts=None, use_trial_FE=True,\n",
    "    )\n",
    "\n",
    "    cols = list(design_nohist.columns)\n",
    "    def idxs(prefix):\n",
    "        return [i for i, c in enumerate(cols) if c.startswith(prefix)]\n",
    "\n",
    "    beta = np.zeros(design_nohist.shape[1])\n",
    "    for p, gain in [(\"cur_on\", 0.9), (\"nxt_on\", 0.5)]:\n",
    "        j = idxs(p); \n",
    "        if j: beta[j] = np.linspace(gain, 0.0, num=len(j))\n",
    "    for p, gain in [(\"cur_dist\", -0.7), (\"nxt_dist\", -0.5)]:\n",
    "        j = idxs(p); \n",
    "        if j: beta[j] = np.linspace(gain, 0.0, num=len(j))\n",
    "    for p, gain in [(\"cur_angle_sin\", 0.15), (\"cur_angle_cos\", 0.15), (\"nxt_angle_sin\", 0.10), (\"nxt_angle_cos\", 0.10)]:\n",
    "        j = idxs(p);\n",
    "        if j: beta[j] = gain / max(1, len(j))\n",
    "    for p, gain in [(\"cur_align\", 0.35), (\"nxt_align\", 0.20)]:\n",
    "        j = idxs(p);\n",
    "        if j: beta[j] = np.linspace(gain, 0.0, num=len(j))\n",
    "    if \"speed\" in cols:     beta[cols.index(\"speed\")] = 0.2\n",
    "    if \"curvature\" in cols: beta[cols.index(\"curvature\")] = -0.25\n",
    "\n",
    "    stim_drive = design_nohist.values @ beta\n",
    "\n",
    "    B_hist = meta[\"B_hist\"]\n",
    "    h_hist = B_hist @ np.array([-1.2, -0.8, -0.5, -0.3, -0.2])\n",
    "    Lh = len(h_hist)\n",
    "\n",
    "    baseline = -3.0 + 0.3 * rng.standard_normal(n_trials)\n",
    "\n",
    "    y = np.zeros(T, dtype=int)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        past = np.zeros(Lh)\n",
    "        b0 = baseline[tr]\n",
    "        for t in idx:\n",
    "            eta = b0 + stim_drive[t] + np.dot(past, h_hist[::-1])\n",
    "            lam = safe_poisson_lambda(eta, dt, max_rate_hz=max_rate_hz)\n",
    "            y[t] = np.random.poisson(lam)\n",
    "            past = np.roll(past, 1); past[0] = y[t]\n",
    "\n",
    "    return {\n",
    "        \"trial_ids\": trial_ids,\n",
    "        \"cur_vis\": cur_vis,\n",
    "        \"nxt_vis\": nxt_vis,\n",
    "        \"cur_dist\": cur_dist,\n",
    "        \"nxt_dist\": nxt_dist,\n",
    "        \"cur_angle\": cur_angle,\n",
    "        \"nxt_angle\": nxt_angle,\n",
    "        \"heading\": heading,\n",
    "        \"speed\": speed,\n",
    "        \"curvature\": curvature,\n",
    "        \"spike_counts\": y,\n",
    "        \"dt\": dt,\n",
    "    }\n",
    "\n",
    "# =====================\n",
    "# Demos\n",
    "# =====================\n",
    "\n",
    "# ---------- Plotting helpers ----------\n",
    "\n",
    "def _coef_series(result, design_df):\n",
    "    if hasattr(result, 'params'):\n",
    "        params = np.asarray(result.params).ravel()\n",
    "        names = getattr(result, 'exog_names', None)\n",
    "        if names is None and hasattr(result.model, 'exog_names'):\n",
    "            names = result.model.exog_names\n",
    "        if names and names[0] == 'const' and len(params) == len(names):\n",
    "            params = params[1:]\n",
    "            names = names[1:]\n",
    "        return pd.Series(params, index=names if names else list(design_df.columns))\n",
    "    return pd.Series(np.zeros(design_df.shape[1]), index=list(design_df.columns))\n",
    "\n",
    "\n",
    "def reconstruct_kernel(prefix: str, basis: np.ndarray, coef_s: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cols = [c for c in coef_s.index if c.startswith(prefix + \"_rc\")]\n",
    "    if not cols:\n",
    "        return np.arange(basis.shape[0]), np.zeros(basis.shape[0])\n",
    "    def rc_idx(c):\n",
    "        try:\n",
    "            return int(c.split('_rc')[-1])\n",
    "        except:\n",
    "            return 0\n",
    "    cols.sort(key=rc_idx)\n",
    "    w = coef_s.loc[cols].values\n",
    "    k = basis @ w\n",
    "    t = np.arange(basis.shape[0])\n",
    "    return t, k\n",
    "\n",
    "\n",
    "def reconstruct_history_kernel(B_hist: np.ndarray, coef_s: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cols = [c for c in coef_s.index if c.startswith('hist_rc')]\n",
    "    if not cols:\n",
    "        return np.arange(B_hist.shape[0]), np.zeros(B_hist.shape[0])\n",
    "    def rc_idx(c):\n",
    "        try:\n",
    "            return int(c.split('_rc')[-1])\n",
    "        except:\n",
    "            return 0\n",
    "    cols.sort(key=rc_idx)\n",
    "    w = coef_s.loc[cols].values\n",
    "    k = B_hist @ w\n",
    "    t = np.arange(B_hist.shape[0])\n",
    "    return t, k\n",
    "\n",
    "\n",
    "def plot_fitted_kernels(result, design_df, meta, dt, *, prefixes=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if prefixes is None:\n",
    "        prefixes = ['cur_on', 'nxt_on', 'cur_dist', 'nxt_dist', 'cur_angle_sin', 'cur_angle_cos', 'nxt_angle_sin', 'nxt_angle_cos']\n",
    "    coef_s = _coef_series(result, design_df)\n",
    "    B_event, B_short, B_hist = meta['B_event'], meta['B_short'], meta['B_hist']\n",
    "\n",
    "    def pick_basis(p):\n",
    "        if p in ['cur_on', 'nxt_on']:\n",
    "            return B_event\n",
    "        elif p.startswith(('cur_', 'nxt_')):\n",
    "            return B_short\n",
    "        else:\n",
    "            return B_short\n",
    "\n",
    "    for p in prefixes:\n",
    "        B = pick_basis(p)\n",
    "        t, k = reconstruct_kernel(p, B, coef_s)\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.plot(t * dt, k)\n",
    "        plt.xlabel('Time lag (s)'); plt.ylabel('Kernel weight')\n",
    "        plt.title(f'{p} kernel')\n",
    "        plt.show()\n",
    "\n",
    "    coef_s = _coef_series(result, design_df)\n",
    "    t_h, k_h = reconstruct_history_kernel(B_hist, coef_s)\n",
    "    plt.figure()\n",
    "    plt.plot(t_h * dt, k_h)\n",
    "    plt.xlabel('Time lag (s)'); plt.ylabel('History weight')\n",
    "    plt.title('Spike history kernel')\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Demos ----------\n",
    "\n",
    "# =====================\n",
    "# Debugging utilities\n",
    "# =====================\n",
    "\n",
    "def _column_blocks(design_df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    \"\"\"Group columns by prefix (before _rc if present).\"\"\"\n",
    "    blocks: Dict[str, List[str]] = {}\n",
    "    for c in design_df.columns:\n",
    "        if '_rc' in c:\n",
    "            p = c.split('_rc')[0]\n",
    "        else:\n",
    "            p = c\n",
    "        blocks.setdefault(p, []).append(c)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def design_summary(design_df: pd.DataFrame, y: np.ndarray, *, topk: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Quick stats per column: variance, nonzero %, |corr| with y (if defined).\"\"\"\n",
    "    X = design_df.values\n",
    "    var = X.var(axis=0)\n",
    "    nz = (np.abs(X) > 0).mean(axis=0)\n",
    "    # correlation with y (guard zero-variance)\n",
    "    y0 = (y - y.mean()) if y is not None else None\n",
    "    cors = np.full(X.shape[1], np.nan)\n",
    "    if y0 is not None and y0.std() > 0:\n",
    "        for j in range(X.shape[1]):\n",
    "            xj = X[:, j]\n",
    "            if xj.std() > 0:\n",
    "                cors[j] = np.corrcoef(xj, y0)[0, 1]\n",
    "    df = pd.DataFrame({\"col\": design_df.columns, \"var\": var, \"nonzero_frac\": nz, \"corr_y\": cors})\n",
    "    df[\"abs_corr_y\"] = np.abs(df[\"corr_y\"]) \n",
    "    return df.sort_values(\"abs_corr_y\", ascending=False).head(topk)\n",
    "\n",
    "\n",
    "def block_summary(design_df: pd.DataFrame, y: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate per block: ncols, mean var, max |corr| with y, zero-var count.\"\"\"\n",
    "    blocks = _column_blocks(design_df)\n",
    "    rows = []\n",
    "    X = design_df\n",
    "    for p, cols in blocks.items():\n",
    "        sub = X[cols]\n",
    "        v = sub.var().mean()\n",
    "        zero = int((sub.var() == 0).sum())\n",
    "        # max abs corr with y\n",
    "        mabs = np.nan\n",
    "        if y is not None and y.std() > 0:\n",
    "            cors = []\n",
    "            for c in cols:\n",
    "                xv = sub[c].values\n",
    "                if xv.std() > 0:\n",
    "                    cors.append(np.corrcoef(xv, y)[0, 1])\n",
    "            mabs = float(np.nanmax(np.abs(cors))) if len(cors) else np.nan\n",
    "        rows.append({\"block\": p, \"ncols\": len(cols), \"mean_var\": v, \"zero_var\": zero, \"max_abs_corr_y\": mabs})\n",
    "    return pd.DataFrame(rows).sort_values(\"max_abs_corr_y\", ascending=False)\n",
    "\n",
    "\n",
    "def constant_or_near_constant_columns(design_df: pd.DataFrame, tol: float = 1e-12) -> List[str]:\n",
    "    v = design_df.var()\n",
    "    return list(v.index[v <= tol])\n",
    "\n",
    "\n",
    "def svd_report(design_df: pd.DataFrame, *, k: int = 20) -> Dict[str, object]:\n",
    "    X = design_df.values\n",
    "    # center columns for SVD diagnostics\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    U, s, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    cond = float(s[0] / s[-1]) if s[-1] > 0 else np.inf\n",
    "    return {\"rank\": int((s > 1e-10).sum()), \"ncols\": X.shape[1], \"nrows\": X.shape[0], \"cond_number\": cond, \"singular_values_top\": s[:k]}\n",
    "\n",
    "\n",
    "def check_param_mapping(result, design_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    names = getattr(result, 'exog_names', None)\n",
    "    if names is None and hasattr(result.model, 'exog_names'):\n",
    "        names = result.model.exog_names\n",
    "    if names is None:\n",
    "        # fallback: assume intercept + design columns\n",
    "        names = ['const'] + list(design_df.columns)\n",
    "    params = np.asarray(result.params).ravel()\n",
    "    if len(params) == len(names):\n",
    "        return pd.DataFrame({'name': names, 'param': params})\n",
    "    # if intercept dropped\n",
    "    return pd.DataFrame({'name': ['const'] + list(design_df.columns[:len(params)-1]), 'param': params})\n",
    "\n",
    "\n",
    "def single_block_fit(prefix: str, design_df: pd.DataFrame, y: np.ndarray, dt: float, trial_ids: np.ndarray):\n",
    "    cols = [c for c in design_df.columns if c.startswith(prefix + '_rc')] or [prefix]\n",
    "    X = design_df[cols]\n",
    "    res = fit_poisson_glm_trials(X, y, dt, trial_ids, add_const=True, l2=0.0, cluster_se=False)\n",
    "    mu = predict_mu(res, X, dt)\n",
    "    dev = poisson_deviance(y, mu)\n",
    "    null = poisson_deviance(y, np.full_like(y, y.mean()))\n",
    "    return {\"prefix\": prefix, \"deviance\": dev, \"null_dev\": null, \"pseudo_R2\": 1 - dev/null}\n",
    "\n",
    "\n",
    "def peth_from_onsets(onsets: np.ndarray, y: np.ndarray, trial_ids: np.ndarray, *, window_bins: int = 40) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Simple PSTH around onsets, computed within trials.\n",
    "    Returns (lags, mean_counts)\n",
    "    \"\"\"\n",
    "    lags = np.arange(-window_bins, window_bins+1)\n",
    "    snippets = []\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        on = np.where(onsets[idx] > 0)[0]\n",
    "        for t in on:\n",
    "            left = t - window_bins\n",
    "            right = t + window_bins\n",
    "            if left >= 0 and right < len(idx):\n",
    "                seg = y[idx][left:right+1]\n",
    "                snippets.append(seg)\n",
    "    if not snippets:\n",
    "        return lags, np.zeros_like(lags, dtype=float)\n",
    "    M = np.vstack(snippets)\n",
    "    return lags, M.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _kernel_with_ci(result, design_df, prefix: str, basis: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Mean and 95% CI for a single prefix kernel using delta method.\n",
    "    Returns (t_idx, mean, std) for the time-domain kernel.\n",
    "    \"\"\"\n",
    "    coef_names = getattr(result, 'model', None).exog_names if hasattr(result, 'model') else None\n",
    "    if coef_names is None:\n",
    "        raise ValueError('No coefficient names found; fit must use a DataFrame design.')\n",
    "    cols = _prefix_cols(prefix, coef_names)\n",
    "    if not cols:\n",
    "        L = basis.shape[0]\n",
    "        return np.arange(L), np.zeros(L), np.zeros(L)\n",
    "\n",
    "    # weights and covariance for the basis weights of this prefix\n",
    "    w = result.params[result.params.index.isin(cols)] if hasattr(result.params, 'index') else None\n",
    "    if w is None or len(w) != len(cols):\n",
    "        # fall back to locating by name via cov_params DataFrame\n",
    "        params_series = _coef_series(result, design_df)\n",
    "        w = params_series.loc[cols]\n",
    "    cov = result.cov_params().loc[cols, cols].values\n",
    "\n",
    "    # kernel mean and variance across lags\n",
    "    L = basis.shape[0]\n",
    "    mean = basis @ w.values\n",
    "    var = np.einsum('li,ij,lj->l', basis, cov, basis)\n",
    "    std = np.sqrt(np.maximum(var, 0.0))\n",
    "    t_idx = np.arange(L)\n",
    "    return t_idx, mean, std\n",
    "\n",
    "\n",
    "def plot_angle_kernels_with_ci(result, design_df, meta, dt, base_prefix: str = 'cur_angle'):\n",
    "    \"\"\"Plot sin, cos, and amplitude (with 95% CIs) for angle tuning over time.\n",
    "    Uses cluster/robust covariance if that was used in the fit.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    B = meta['B_short']\n",
    "    # sin\n",
    "    t_idx, ksin, std_s = _kernel_with_ci(result, design_df, f'{base_prefix}_sin', B)\n",
    "    # cos\n",
    "    _,    kcos, std_c = _kernel_with_ci(result, design_df, f'{base_prefix}_cos', B)\n",
    "\n",
    "    # Cross-covariance between sin and cos weights to get Var(amplitude)\n",
    "    coef_names = result.model.exog_names\n",
    "    sin_cols = _prefix_cols(f'{base_prefix}_sin', coef_names)\n",
    "    cos_cols = _prefix_cols(f'{base_prefix}_cos', coef_names)\n",
    "    cov_full = result.cov_params()\n",
    "    cov_ss = cov_full.loc[sin_cols, sin_cols].values\n",
    "    cov_cc = cov_full.loc[cos_cols, cos_cols].values\n",
    "    cov_sc = cov_full.loc[sin_cols, cos_cols].values\n",
    "\n",
    "    # Build amplitude mean and variance per lag\n",
    "    A = np.sqrt(ksin**2 + kcos**2)\n",
    "    var_A = np.zeros_like(A)\n",
    "    for l in range(B.shape[0]):\n",
    "        b = B[l, :][:, None]  # (K,1)\n",
    "        var_s = float(b.T @ cov_ss @ b)\n",
    "        var_c = float(b.T @ cov_cc @ b)\n",
    "        cov_sc_l = float(b.T @ cov_sc @ b)\n",
    "        # delta method\n",
    "        eps = 1e-12\n",
    "        denom = max(A[l], eps)\n",
    "        g = np.array([ksin[l] / denom, kcos[l] / denom])\n",
    "        Sigma = np.array([[var_s, cov_sc_l], [cov_sc_l, var_c]])\n",
    "        var_A[l] = float(g.T @ Sigma @ g)\n",
    "\n",
    "    std_A = np.sqrt(np.maximum(var_A, 0.0))\n",
    "\n",
    "    # --- plots ---\n",
    "    t = t_idx * dt\n",
    "    # sin\n",
    "    plt.figure()\n",
    "    plt.plot(t, ksin, label='sin')\n",
    "    plt.fill_between(t, ksin - 1.96*std_s, ksin + 1.96*std_s, alpha=0.2)\n",
    "    plt.xlabel('Time lag (s)'); plt.ylabel('Kernel weight'); plt.title(f'{base_prefix}_sin kernel (95% CI)')\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    # cos\n",
    "    plt.figure()\n",
    "    plt.plot(t, kcos, label='cos')\n",
    "    plt.fill_between(t, kcos - 1.96*std_c, kcos + 1.96*std_c, alpha=0.2)\n",
    "    plt.xlabel('Time lag (s)'); plt.ylabel('Kernel weight'); plt.title(f'{base_prefix}_cos kernel (95% CI)')\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    # amplitude\n",
    "    plt.figure()\n",
    "    plt.plot(t, A, label='amplitude')\n",
    "    plt.fill_between(t, np.maximum(0.0, A - 1.96*std_A), A + 1.96*std_A, alpha=0.2)\n",
    "    plt.xlabel('Time lag (s)'); plt.ylabel('Amplitude'); plt.title(f'{base_prefix} amplitude (95% CI)')\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    t_h, k_h = reconstruct_history_kernel(B_hist, coef_s)\n",
    "    plt.figure()\n",
    "    plt.plot(t_h * dt, k_h)\n",
    "    plt.xlabel('Time lag (s)'); plt.ylabel('History weight')\n",
    "    plt.title('Spike history kernel')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _prefix_cols(prefix: str, names: List[str]) -> List[str]:\n",
    "    cols = [n for n in names if n.startswith(prefix + '_rc')]\n",
    "    def rc_idx(c):\n",
    "        try:\n",
    "            return int(c.split('_rc')[-1])\n",
    "        except:\n",
    "            return 0\n",
    "    return sorted(cols, key=rc_idx)\n",
    "\n",
    "\n",
    "def angle_tuning_vs_time(result, design_df, meta, base_prefix: str = 'cur_angle'):\n",
    "    \"\"\"Return time-index, sin/cos kernels, amplitude and preferred angle over lags.\n",
    "    `base_prefix` should be 'cur_angle' or 'nxt_angle'.\n",
    "    \"\"\"\n",
    "    coef_s = _coef_series(result, design_df)\n",
    "    B = meta['B_short']\n",
    "    t_idx, k_sin = reconstruct_kernel(f'{base_prefix}_sin', B, coef_s)\n",
    "    _,    k_cos = reconstruct_kernel(f'{base_prefix}_cos', B, coef_s)\n",
    "    A = np.sqrt(k_sin**2 + k_cos**2)\n",
    "    phi = np.arctan2(k_sin, k_cos)\n",
    "    return t_idx, k_sin, k_cos, A, phi\n",
    "\n",
    "\n",
    "def plot_angle_tuning(result, design_df, meta, dt):\n",
    "    import matplotlib.pyplot as plt\n",
    "    t_idx, k_sin, k_cos, A, phi = angle_tuning_vs_time(result, design_df, meta)\n",
    "\n",
    "    # kernels\n",
    "    plt.figure(); plt.plot(t_idx*dt, k_cos, label=\"cos\"); plt.plot(t_idx*dt, k_sin, label=\"sin\")\n",
    "    plt.xlabel(\"Time lag (s)\"); plt.ylabel(\"Kernel weight\"); plt.title(\"Angle kernels\"); plt.legend(); plt.show()\n",
    "\n",
    "    # amplitude\n",
    "    plt.figure(); plt.plot(t_idx*dt, A)\n",
    "    plt.xlabel(\"Time lag (s)\"); plt.ylabel(\"Amplitude\"); plt.title(\"Directional tuning amplitude vs lag\"); plt.show()\n",
    "\n",
    "    # preferred angle\n",
    "    plt.figure(); plt.plot(t_idx*dt, phi)\n",
    "    plt.xlabel(\"Time lag (s)\"); plt.ylabel(\"Preferred angle (rad)\"); plt.title(\"Preferred angle vs lag\"); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def debug_all_kernels_flat(res, design_df, y, trial_ids, meta, dt):\n",
    "    \"\"\"Run a suite of checks and print concise results.\"\"\"\n",
    "    print(\"[DEBUG] Constant/near-constant columns:\")\n",
    "    print(constant_or_near_constant_columns(design_df)[:20])\n",
    "\n",
    "    print(\"[DEBUG] Top-10 columns by |corr(y)|:\")\n",
    "    print(design_summary(design_df, y, topk=10))\n",
    "\n",
    "    print(\"[DEBUG] Block summary (max |corr(y)| per block):\")\n",
    "    print(block_summary(design_df, y))\n",
    "\n",
    "    print(\"[DEBUG] SVD report:\")\n",
    "    print(svd_report(design_df))\n",
    "\n",
    "    print(\"[DEBUG] Param mapping (first 20):\")\n",
    "    pm = check_param_mapping(res, design_df)\n",
    "    print(pm.head(20))\n",
    "\n",
    "    # Quick single-block tests\n",
    "    prefixes = sorted(set([c.split('_rc')[0] if '_rc' in c else c for c in design_df.columns]))\n",
    "    tests = []\n",
    "    for p in prefixes:\n",
    "        tests.append(single_block_fit(p, design_df, y, dt, trial_ids))\n",
    "    tests_df = pd.DataFrame(tests).sort_values('pseudo_R2', ascending=False)\n",
    "    print(\"[DEBUG] Single-block fits (sorted by pseudo_R2):\")\n",
    "    print(tests_df)\n",
    "\n",
    "    # Example PETH if cur_on is present\n",
    "    if 'cur_on_rc1' in design_df.columns or 'cur_on' in design_df.columns:\n",
    "        on_name = 'cur_on' if 'cur_on' in design_df.columns else None\n",
    "        if on_name is None:\n",
    "            # reconstruct onsets from basis columns by deconvolving first basis roughly (just for visualization)\n",
    "            on_name = 'cur_on'\n",
    "            print(\"Note: cur_on onset vector not present raw; consider passing it to peth_from_onsets directly from your data.\")\n",
    "        else:\n",
    "            lags, mean_counts = peth_from_onsets(design_df[on_name].values, y, trial_ids, window_bins=40)\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure()\n",
    "            plt.plot(lags * dt, mean_counts)\n",
    "            plt.axvline(0, linestyle='--')\n",
    "            plt.xlabel('Time (s)'); plt.ylabel('Mean spikes/bin')\n",
    "            plt.title('PETH around cur_on onsets')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_angle_tuning(res, design_df, meta, dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fitting:\n",
    "\n",
    "\n",
    "# Plots with 95% CIs\n",
    "plot_angle_kernels_with_ci(res, design_df, meta, dt, base_prefix='cur_angle')\n",
    "\n",
    "# If you also model next-angle:\n",
    "plot_angle_kernels_with_ci(res, design_df, meta, dt, base_prefix='nxt_angle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# run_multiff_demo\n",
    "data = simulate_multiff_trials(n_trials=8, trial_len=300, dt=0.01, seed=7)\n",
    "res, design_df, metrics, meta = fit_multiff_glm(\n",
    "    dt=data[\"dt\"], trial_ids=data[\"trial_ids\"],\n",
    "    cur_vis=data[\"cur_vis\"], nxt_vis=data[\"nxt_vis\"],\n",
    "    cur_dist=data[\"cur_dist\"], nxt_dist=data[\"nxt_dist\"],\n",
    "    cur_angle=data[\"cur_angle\"], nxt_angle=data[\"nxt_angle\"],\n",
    "    heading=data[\"heading\"], speed=data[\"speed\"], curvature=data[\"curvature\"],\n",
    "    spike_counts=data[\"spike_counts\"], l2=0.0, use_trial_FE=True, cluster_se=False,\n",
    ")\n",
    "print(res.summary())\n",
    "print(\"Overall deviance:\", metrics[\"deviance\"])\n",
    "print(\"Pseudo-R^2:\", metrics[\"pseudo_R2\"])\n",
    "print(\"Per-trial deviance (head):\", metrics[\"per_trial_deviance\"].head())\n",
    "cv = fit_and_score_cv(design_df, data[\"spike_counts\"], data[\"dt\"], data[\"trial_ids\"], n_splits=5, l2=0.0, cluster_se=False)\n",
    "print(\"Trial-wise CV:\", cv)\n",
    "\n",
    "plot_fitted_kernels(res, design_df, meta, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_spike_only_demo()\n",
    "run_multiff_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fitted_kernels(res, design_df, meta, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions (early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trial-aware Poisson GLM for multiFF neural spike data (statsmodels)\n",
    "------------------------------------------------------------------\n",
    "End-to-end, runnable module:\n",
    "- Raised-cosine bases\n",
    "- Trial-aware stimulus & spike-history design (no cross-trial leakage)\n",
    "- Optional trial fixed effects or cluster-robust SEs by trial\n",
    "- Poisson fitting (statsmodels.GLM), prediction, metrics, trial-wise CV\n",
    "- MultiFF adapter with visibility/distance/angle/heading features\n",
    "- Stable simulators using capped Poisson intensities\n",
    "- Demos: run `python this_file.py` or call `run_multiff_demo()`\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "from scipy import signal\n",
    "from scipy.linalg import toeplitz\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# =====================\n",
    "# Helpers & bases\n",
    "# =====================\n",
    "\n",
    "def _unique_trials(trial_ids: np.ndarray) -> np.ndarray:\n",
    "    return np.unique(np.asarray(trial_ids))\n",
    "\n",
    "\n",
    "def raised_cosine_basis(n_basis: int, t_max: float, dt: float, *, t_min: float = 0.0,\n",
    "                        log_spaced: bool = True, eps: float = 1e-3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Causal raised-cosine basis that tiles [t_min, t_max].\n",
    "    Returns lags (L,), B (L x K) with unit-area columns (sum * dt = 1).\n",
    "    \"\"\"\n",
    "    lags = np.arange(0.0, t_max + 1e-12, dt)\n",
    "    K = int(n_basis)\n",
    "\n",
    "    def warp(x):\n",
    "        return np.log(x + eps) if log_spaced else x\n",
    "\n",
    "    W = warp(lags)\n",
    "    W_min, W_max = warp(t_min), warp(t_max)\n",
    "    centers = np.linspace(W_min, W_max, K)\n",
    "    delta = centers[1] - centers[0] if K > 1 else (W_max - W_min + 1e-12)\n",
    "    width = delta * 1.5\n",
    "\n",
    "    B = []\n",
    "    for c in centers:\n",
    "        arg = (W - c) / width\n",
    "        bk = np.cos(np.clip(arg, -np.pi, np.pi))\n",
    "        bk[np.abs(arg) > np.pi] = 0.0\n",
    "        bk = np.maximum(bk, 0.0)\n",
    "        bk[lags < t_min] = 0.0\n",
    "        area = bk.sum() * dt\n",
    "        if area > 0:\n",
    "            bk = bk / area\n",
    "        B.append(bk)\n",
    "    B = np.column_stack(B) if B else np.zeros((len(lags), 0))\n",
    "    return lags, B\n",
    "\n",
    "\n",
    "def safe_poisson_lambda(eta: float | np.ndarray, dt: float, *, max_rate_hz: float = 200.0) -> np.ndarray:\n",
    "    \"\"\"Convert log-rate `eta` (per-second) to expected bin count lambda, with a cap.\"\"\"\n",
    "    log_min = np.log(1e-6)\n",
    "    log_max = np.log(max_rate_hz)\n",
    "    eta_clipped = np.clip(eta, log_min, log_max)\n",
    "    rate_hz = np.exp(eta_clipped)\n",
    "    return rate_hz * dt\n",
    "\n",
    "\n",
    "def wrap_angle(theta: np.ndarray) -> np.ndarray:\n",
    "    return (theta + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "\n",
    "def angle_sin_cos(theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    th = wrap_angle(theta)\n",
    "    return np.sin(th), np.cos(th)\n",
    "\n",
    "\n",
    "def onset_from_mask_trials(mask: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    mask = (mask > 0).astype(int)\n",
    "    on = np.zeros_like(mask, dtype=float)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        m = mask[idx]\n",
    "        d = np.diff(np.r_[0, m])\n",
    "        on[idx] = (d == 1).astype(float)\n",
    "    return on\n",
    "\n",
    "\n",
    "def offset_from_mask_trials(mask: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    mask = (mask > 0).astype(int)\n",
    "    off = np.zeros_like(mask, dtype=float)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        m = mask[idx]\n",
    "        d = np.diff(np.r_[m, 0])\n",
    "        off[idx] = (d == 1).astype(float)\n",
    "    return off\n",
    "\n",
    "# =====================\n",
    "# Trial-aware design builders\n",
    "# =====================\n",
    "\n",
    "def lagged_design_from_signal_trials(x: np.ndarray, basis: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    T = len(x)\n",
    "    L, K = basis.shape\n",
    "    Xk = np.zeros((T, K))\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        xt = x[idx]\n",
    "        for k in range(K):\n",
    "            y = signal.convolve(xt, basis[:, k], mode=\"full\")[: len(idx)]\n",
    "            Xk[idx, k] = y\n",
    "    return Xk\n",
    "\n",
    "\n",
    "def spike_history_design_with_trials(y_counts: np.ndarray, basis: np.ndarray, trial_ids: np.ndarray) -> np.ndarray:\n",
    "    T = len(y_counts)\n",
    "    L, K = basis.shape\n",
    "    Xh = np.zeros((T, K))\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        y_tr = y_counts[idx]\n",
    "        col0 = np.r_[0, y_tr[:-1]]  # strictly past\n",
    "        toepl = toeplitz(col0, np.zeros(L))\n",
    "        Xh[idx, :] = toepl @ basis\n",
    "    return Xh\n",
    "\n",
    "\n",
    "def build_glm_design_with_trials(\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    stimulus_dict: Optional[Dict[str, np.ndarray]] = None,\n",
    "    stimulus_basis_dict: Optional[Dict[str, np.ndarray]] = None,\n",
    "    spike_counts: Optional[np.ndarray] = None,\n",
    "    history_basis: Optional[np.ndarray] = None,\n",
    "    extra_covariates: Optional[Dict[str, np.ndarray]] = None,\n",
    "    use_trial_FE: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Optional[np.ndarray]]:\n",
    "    T = len(trial_ids)\n",
    "    cols: List[np.ndarray] = []\n",
    "    names: List[str] = []\n",
    "\n",
    "    if stimulus_dict is not None:\n",
    "        for name, x in stimulus_dict.items():\n",
    "            if stimulus_basis_dict is not None and name in stimulus_basis_dict:\n",
    "                B = stimulus_basis_dict[name]\n",
    "                Xk = lagged_design_from_signal_trials(x, B, trial_ids)\n",
    "                for k in range(Xk.shape[1]):\n",
    "                    cols.append(Xk[:, k]); names.append(f\"{name}_rc{k+1}\")\n",
    "            else:\n",
    "                cols.append(x); names.append(name)\n",
    "\n",
    "    if spike_counts is not None and history_basis is not None:\n",
    "        Xh = spike_history_design_with_trials(spike_counts, history_basis, trial_ids)\n",
    "        for k in range(Xh.shape[1]):\n",
    "            cols.append(Xh[:, k]); names.append(f\"hist_rc{k+1}\")\n",
    "\n",
    "    if extra_covariates is not None:\n",
    "        for n, v in extra_covariates.items():\n",
    "            cols.append(v); names.append(n)\n",
    "\n",
    "    X = np.column_stack(cols) if cols else np.zeros((T, 0))\n",
    "    design_df = pd.DataFrame(X, columns=names)\n",
    "\n",
    "    if use_trial_FE:\n",
    "        trial_FE = pd.get_dummies(trial_ids, prefix=\"trial\", drop_first=True)\n",
    "        design_df = pd.concat([design_df, trial_FE], axis=1)\n",
    "\n",
    "    y = spike_counts if spike_counts is not None else None\n",
    "    return design_df, y\n",
    "\n",
    "# =====================\n",
    "# Fitting & metrics\n",
    "# =====================\n",
    "\n",
    "def add_intercept(X: np.ndarray) -> np.ndarray:\n",
    "    return np.column_stack([np.ones(len(X)), X])\n",
    "\n",
    "\n",
    "def fit_poisson_glm_trials(\n",
    "    design_df: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    *,\n",
    "    add_const: bool = True,\n",
    "    l2: float = 0.0,\n",
    "    cluster_se: bool = False,\n",
    "):\n",
    "    X = design_df.values\n",
    "    if add_const:\n",
    "        X = add_intercept(X)\n",
    "    exposure = np.full_like(y, fill_value=dt, dtype=float)\n",
    "\n",
    "    model = sm.GLM(y, X, family=sm.families.Poisson(), exposure=exposure)\n",
    "    if l2 > 0:\n",
    "        res = model.fit_regularized(alpha=l2, L1_wt=0.0, maxiter=1000)\n",
    "        # attach names for convenience\n",
    "        res.exog_names = ([\"const\"] + list(design_df.columns)) if add_const else list(design_df.columns)\n",
    "        return res\n",
    "    else:\n",
    "        if cluster_se:\n",
    "            return model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": trial_ids})\n",
    "        else:\n",
    "            return model.fit()\n",
    "\n",
    "\n",
    "def predict_mu(result, design_df: pd.DataFrame, dt: float, add_const: bool = True) -> np.ndarray:\n",
    "    X = design_df.values\n",
    "    if add_const:\n",
    "        X = add_intercept(X)\n",
    "    mu = result.predict(X, exposure=np.full(len(X), dt))\n",
    "    return np.asarray(mu, dtype=float)\n",
    "\n",
    "\n",
    "def poisson_deviance(y: np.ndarray, mu: np.ndarray) -> float:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    mu = np.asarray(mu, dtype=float)\n",
    "    eps = 1e-12\n",
    "    term = np.where(y > 0, y * np.log((y + eps) / (mu + eps)), 0.0)\n",
    "    dev = 2.0 * np.sum(term - (y - mu))\n",
    "    return float(dev)\n",
    "\n",
    "\n",
    "def pseudo_R2(y: np.ndarray, mu_full: np.ndarray, mu_null: np.ndarray) -> float:\n",
    "    eps = 1e-12\n",
    "    ll_full = np.sum(y * np.log(mu_full + eps) - mu_full)\n",
    "    ll_null = np.sum(y * np.log(mu_null + eps) - mu_null)\n",
    "    return float(1.0 - ll_full / ll_null)\n",
    "\n",
    "\n",
    "def per_trial_deviance(y: np.ndarray, mu: np.ndarray, trial_ids: np.ndarray) -> pd.DataFrame:\n",
    "    dev = pd.DataFrame({\"trial\": trial_ids, \"y\": y, \"mu\": mu})\n",
    "    eps = 1e-12\n",
    "    dev[\"bin_dev\"] = np.where(dev[\"y\"] > 0, dev[\"y\"] * np.log((dev[\"y\"] + eps) / (dev[\"mu\"] + eps)), 0.0) - (dev[\"y\"] - dev[\"mu\"])\n",
    "    out = dev.groupby(\"trial\", as_index=False)[\"bin_dev\"].sum()\n",
    "    out.rename(columns={\"bin_dev\": \"trial_deviance\"}, inplace=True)\n",
    "    out[\"trial_deviance\"] *= 2.0\n",
    "    return out\n",
    "\n",
    "# =====================\n",
    "# CV utilities\n",
    "# =====================\n",
    "\n",
    "def trialwise_folds(trial_ids: np.ndarray, n_splits: int, *, shuffle: bool = False, random_state: Optional[int] = None) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    trials = _unique_trials(trial_ids)\n",
    "    if shuffle:\n",
    "        trials = trials.copy(); rng.shuffle(trials)\n",
    "    folds = np.array_split(trials, n_splits)\n",
    "    out = []\n",
    "    for k in range(n_splits):\n",
    "        val_trials = set(folds[k])\n",
    "        val_mask = np.isin(trial_ids, list(val_trials))\n",
    "        train_mask = ~val_mask\n",
    "        out.append((train_mask, val_mask))\n",
    "    return out\n",
    "\n",
    "\n",
    "def fit_and_score_cv(\n",
    "    design_df: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    *,\n",
    "    n_splits: int = 5,\n",
    "    l2: float = 0.0,\n",
    "    use_trial_FE: bool = True,\n",
    "    cluster_se: bool = False,\n",
    "    random_state: Optional[int] = 0,\n",
    ") -> pd.DataFrame:\n",
    "    folds = trialwise_folds(trial_ids, n_splits, shuffle=True, random_state=random_state)\n",
    "    rows = []\n",
    "    for i, (train_mask, val_mask) in enumerate(folds, start=1):\n",
    "        X_train = design_df.iloc[train_mask]\n",
    "        y_train = y[train_mask]\n",
    "        tr_ids_train = trial_ids[train_mask]\n",
    "\n",
    "        X_val = design_df.iloc[val_mask]\n",
    "        y_val = y[val_mask]\n",
    "        tr_ids_val = trial_ids[val_mask]\n",
    "\n",
    "        res = fit_poisson_glm_trials(X_train, y_train, dt, tr_ids_train,\n",
    "                                     add_const=True, l2=l2, cluster_se=False)\n",
    "        mu_val = predict_mu(res, X_val, dt)\n",
    "        mu_null = np.full_like(y_val, y_train.mean(), dtype=float)\n",
    "\n",
    "        fold_dev = poisson_deviance(y_val, mu_val)\n",
    "        fold_r2 = pseudo_R2(y_val, mu_val, mu_null)\n",
    "        rows.append({\"fold\": i, \"val_deviance\": fold_dev, \"val_pseudo_R2\": fold_r2, \"n_val_bins\": int(val_mask.sum())})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =====================\n",
    "# MultiFF adapter (features & end-to-end)\n",
    "# =====================\n",
    "\n",
    "def build_multiff_design(\n",
    "    *,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    cur_vis: np.ndarray,\n",
    "    nxt_vis: np.ndarray,\n",
    "    cur_dist: np.ndarray,\n",
    "    nxt_dist: np.ndarray,\n",
    "    cur_angle: np.ndarray,\n",
    "    nxt_angle: np.ndarray,\n",
    "    heading: Optional[np.ndarray] = None,\n",
    "    speed: Optional[np.ndarray] = None,\n",
    "    curvature: Optional[np.ndarray] = None,\n",
    "    spike_counts: Optional[np.ndarray] = None,\n",
    "    use_trial_FE: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Optional[np.ndarray], Dict[str, np.ndarray]]:\n",
    "    T = len(trial_ids)\n",
    "\n",
    "    # Bases\n",
    "    _, B_event = raised_cosine_basis(n_basis=6, t_max=0.60, dt=dt, t_min=0.0, log_spaced=True)\n",
    "    _, B_short = raised_cosine_basis(n_basis=5, t_max=0.30, dt=dt, t_min=0.0, log_spaced=True)\n",
    "    _, B_hist  = raised_cosine_basis(n_basis=5, t_max=0.20, dt=dt, t_min=dt,  log_spaced=True)\n",
    "\n",
    "    # Onsets and gated features\n",
    "    cur_on = onset_from_mask_trials(cur_vis, trial_ids)\n",
    "    nxt_on = onset_from_mask_trials(nxt_vis, trial_ids)\n",
    "\n",
    "    cur_dist_g = cur_dist * (cur_vis > 0)\n",
    "    nxt_dist_g = nxt_dist * (nxt_vis > 0)\n",
    "\n",
    "    cur_sin, cur_cos = angle_sin_cos(cur_angle)\n",
    "    nxt_sin, nxt_cos = angle_sin_cos(nxt_angle)\n",
    "    cur_sin *= (cur_vis > 0); cur_cos *= (cur_vis > 0)\n",
    "    nxt_sin *= (nxt_vis > 0); nxt_cos *= (nxt_vis > 0)\n",
    "\n",
    "    stimulus_dict: Dict[str, np.ndarray] = {\n",
    "        \"cur_on\": cur_on,\n",
    "        \"nxt_on\": nxt_on,\n",
    "        \"cur_dist\": cur_dist_g,\n",
    "        \"nxt_dist\": nxt_dist_g,\n",
    "        \"cur_angle_sin\": cur_sin,\n",
    "        \"cur_angle_cos\": cur_cos,\n",
    "        \"nxt_angle_sin\": nxt_sin,\n",
    "        \"nxt_angle_cos\": nxt_cos,\n",
    "    }\n",
    "\n",
    "    extra_covariates: Dict[str, np.ndarray] = {}\n",
    "    if heading is not None:\n",
    "        cur_align = np.cos(wrap_angle(heading - cur_angle)) * (cur_vis > 0)\n",
    "        nxt_align = np.cos(wrap_angle(heading - nxt_angle)) * (nxt_vis > 0)\n",
    "        stimulus_dict[\"cur_align\"] = cur_align\n",
    "        stimulus_dict[\"nxt_align\"] = nxt_align\n",
    "\n",
    "    if speed is not None:   extra_covariates[\"speed\"] = speed\n",
    "    if curvature is not None: extra_covariates[\"curvature\"] = curvature\n",
    "\n",
    "    stimulus_basis_dict: Dict[str, np.ndarray] = {\n",
    "        \"cur_on\": B_event, \"nxt_on\": B_event,\n",
    "        \"cur_dist\": B_short, \"nxt_dist\": B_short,\n",
    "        \"cur_angle_sin\": B_short, \"cur_angle_cos\": B_short,\n",
    "        \"nxt_angle_sin\": B_short, \"nxt_angle_cos\": B_short,\n",
    "        **({\"cur_align\": B_short, \"nxt_align\": B_short} if heading is not None else {}),\n",
    "    }\n",
    "\n",
    "    design_df, y = build_glm_design_with_trials(\n",
    "        dt=dt,\n",
    "        trial_ids=trial_ids,\n",
    "        stimulus_dict=stimulus_dict,\n",
    "        stimulus_basis_dict=stimulus_basis_dict,\n",
    "        spike_counts=spike_counts,\n",
    "        history_basis=B_hist,\n",
    "        extra_covariates=extra_covariates,\n",
    "        use_trial_FE=use_trial_FE,\n",
    "    )\n",
    "\n",
    "    meta = {\"B_event\": B_event, \"B_short\": B_short, \"B_hist\": B_hist}\n",
    "    return design_df, y, meta\n",
    "\n",
    "\n",
    "def fit_multiff_glm(\n",
    "    *,\n",
    "    dt: float,\n",
    "    trial_ids: np.ndarray,\n",
    "    cur_vis: np.ndarray,\n",
    "    nxt_vis: np.ndarray,\n",
    "    cur_dist: np.ndarray,\n",
    "    nxt_dist: np.ndarray,\n",
    "    cur_angle: np.ndarray,\n",
    "    nxt_angle: np.ndarray,\n",
    "    heading: Optional[np.ndarray] = None,\n",
    "    speed: Optional[np.ndarray] = None,\n",
    "    curvature: Optional[np.ndarray] = None,\n",
    "    spike_counts: np.ndarray,\n",
    "    l2: float = 0.0,\n",
    "    use_trial_FE: bool = True,\n",
    "    cluster_se: bool = False,\n",
    "):\n",
    "    design_df, y, meta = build_multiff_design(\n",
    "        dt=dt, trial_ids=trial_ids,\n",
    "        cur_vis=cur_vis, nxt_vis=nxt_vis,\n",
    "        cur_dist=cur_dist, nxt_dist=nxt_dist,\n",
    "        cur_angle=cur_angle, nxt_angle=nxt_angle,\n",
    "        heading=heading, speed=speed, curvature=curvature,\n",
    "        spike_counts=spike_counts, use_trial_FE=use_trial_FE,\n",
    "    )\n",
    "    res = fit_poisson_glm_trials(design_df, y, dt, trial_ids, add_const=True, l2=l2, cluster_se=False)\n",
    "    mu = predict_mu(res, design_df, dt)\n",
    "    mu_null = np.full_like(y, y.mean(), dtype=float)\n",
    "    metrics = {\n",
    "        \"deviance\": poisson_deviance(y, mu),\n",
    "        \"pseudo_R2\": pseudo_R2(y, mu, mu_null),\n",
    "        \"per_trial_deviance\": per_trial_deviance(y, mu, trial_ids),\n",
    "    }\n",
    "    return res, design_df, metrics, meta\n",
    "\n",
    "# =====================\n",
    "# Stable simulators\n",
    "# =====================\n",
    "\n",
    "def simulate_spikes_with_trials(\n",
    "    *, n_trials: int = 20, trial_len: int = 300, dt: float = 0.01, seed: int = 1,\n",
    "    max_rate_hz: float = 200.0\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = n_trials * trial_len\n",
    "    trial_ids = np.repeat(np.arange(n_trials), trial_len)\n",
    "\n",
    "    _, B_stim = raised_cosine_basis(n_basis=5, t_max=0.300, dt=dt, t_min=0.0, log_spaced=True)\n",
    "    _, B_hist = raised_cosine_basis(n_basis=4, t_max=0.200, dt=dt, t_min=dt, log_spaced=True)\n",
    "\n",
    "    event = np.concatenate([(rng.random(trial_len) < 0.02).astype(float) for _ in range(n_trials)])\n",
    "    speed = np.concatenate([signal.lfilter([1.0], [1.0, -0.7], rng.normal(0.0, 1.0, size=trial_len)) for _ in range(n_trials)])\n",
    "    speed = (speed - speed.mean()) / (speed.std() + 1e-12)\n",
    "\n",
    "    beta_event = np.array([0.8, 0.5, 0.2, -0.1, -0.2])\n",
    "    beta_speed = np.array([0.4, 0.2, 0.0, -0.1, -0.2])\n",
    "    beta_hist  = np.array([-1.2, -0.8, -0.5, -0.2])\n",
    "\n",
    "    X_event = lagged_design_from_signal_trials(event, B_stim, trial_ids)\n",
    "    X_speed = lagged_design_from_signal_trials(speed, B_stim, trial_ids)\n",
    "\n",
    "    h_hist = B_hist @ beta_hist\n",
    "    Lh = len(h_hist)\n",
    "\n",
    "    baseline_per_trial = -2.8 + 0.4 * rng.standard_normal(n_trials)\n",
    "\n",
    "    y = np.zeros(T, dtype=int)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        past = np.zeros(Lh)\n",
    "        b0 = baseline_per_trial[tr]\n",
    "        for t in idx:\n",
    "            stim_drive = X_event[t] @ beta_event + X_speed[t] @ beta_speed\n",
    "            hist_drive = np.dot(past, h_hist[::-1])\n",
    "            eta = b0 + stim_drive + hist_drive\n",
    "            lam = safe_poisson_lambda(eta, dt, max_rate_hz=max_rate_hz)\n",
    "            y[t] = np.random.poisson(lam)\n",
    "            past = np.roll(past, 1); past[0] = y[t]\n",
    "\n",
    "    stimulus_dict = {\"event\": event, \"speed\": speed}\n",
    "    stimulus_basis_dict = {\"event\": B_stim, \"speed\": B_stim}\n",
    "    return trial_ids, y, stimulus_dict, stimulus_basis_dict, B_hist\n",
    "\n",
    "\n",
    "def simulate_multiff_trials(\n",
    "    *, n_trials: int = 12, trial_len: int = 400, dt: float = 0.01, seed: int = 3,\n",
    "    max_rate_hz: float = 200.0\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = n_trials * trial_len\n",
    "    trial_ids = np.repeat(np.arange(n_trials), trial_len)\n",
    "\n",
    "    def random_vis_mask():\n",
    "        m = np.zeros(trial_len, dtype=int)\n",
    "        t = 0\n",
    "        while t < trial_len:\n",
    "            off = rng.integers(20, 80)\n",
    "            on  = rng.integers(30, 120)\n",
    "            t += off\n",
    "            if t >= trial_len: break\n",
    "            m[t: min(trial_len, t + on)] = 1\n",
    "            t += on\n",
    "        return m\n",
    "\n",
    "    cur_vis = np.concatenate([random_vis_mask() for _ in range(n_trials)])\n",
    "    nxt_vis = np.concatenate([np.r_[np.zeros(rng.integers(40, 120)), random_vis_mask()][:trial_len] for _ in range(n_trials)])\n",
    "\n",
    "    def distance_from_vis(m):\n",
    "        d = np.zeros_like(m, dtype=float)\n",
    "        run = 0.0\n",
    "        for i, v in enumerate(m):\n",
    "            if v:\n",
    "                run = 1.0 if run == 0 else max(0.0, run - 0.02)\n",
    "                d[i] = run\n",
    "            else:\n",
    "                run = 0.0; d[i] = 0.0\n",
    "        return d\n",
    "\n",
    "    cur_dist = np.concatenate([distance_from_vis(cur_vis[i*trial_len:(i+1)*trial_len]) for i in range(n_trials)])\n",
    "    nxt_dist = np.concatenate([distance_from_vis(nxt_vis[i*trial_len:(i+1)*trial_len]) for i in range(n_trials)])\n",
    "\n",
    "    def rand_angle_series(L):\n",
    "        a = np.cumsum(rng.normal(0, 0.05, size=L))\n",
    "        return ((a + np.pi) % (2*np.pi)) - np.pi\n",
    "\n",
    "    cur_angle = np.concatenate([rand_angle_series(trial_len) for _ in range(n_trials)])\n",
    "    nxt_angle = np.concatenate([rand_angle_series(trial_len) for _ in range(n_trials)])\n",
    "    heading   = np.concatenate([rand_angle_series(trial_len) for _ in range(n_trials)])\n",
    "\n",
    "    speed = np.concatenate([signal.lfilter([1.0], [1.0, -0.8], rng.normal(0, 1.0, size=trial_len)) for _ in range(n_trials)])\n",
    "    speed = (speed - speed.mean()) / (speed.std() + 1e-12)\n",
    "    curvature = np.concatenate([rng.normal(0.0, 0.4, size=trial_len) for _ in range(n_trials)])\n",
    "\n",
    "    # Build design (no history yet) to compute stim_drive for simulation\n",
    "    design_nohist, _, meta = build_multiff_design(\n",
    "        dt=dt, trial_ids=trial_ids,\n",
    "        cur_vis=cur_vis, nxt_vis=nxt_vis,\n",
    "        cur_dist=cur_dist, nxt_dist=nxt_dist,\n",
    "        cur_angle=cur_angle, nxt_angle=nxt_angle,\n",
    "        heading=heading, speed=speed, curvature=curvature,\n",
    "        spike_counts=None, use_trial_FE=True,\n",
    "    )\n",
    "\n",
    "    cols = list(design_nohist.columns)\n",
    "    def idxs(prefix):\n",
    "        return [i for i, c in enumerate(cols) if c.startswith(prefix)]\n",
    "\n",
    "    beta = np.zeros(design_nohist.shape[1])\n",
    "    for p, gain in [(\"cur_on\", 0.9), (\"nxt_on\", 0.5)]:\n",
    "        j = idxs(p); \n",
    "        if j: beta[j] = np.linspace(gain, 0.0, num=len(j))\n",
    "    for p, gain in [(\"cur_dist\", -0.7), (\"nxt_dist\", -0.5)]:\n",
    "        j = idxs(p); \n",
    "        if j: beta[j] = np.linspace(gain, 0.0, num=len(j))\n",
    "    for p, gain in [(\"cur_angle_sin\", 0.15), (\"cur_angle_cos\", 0.15), (\"nxt_angle_sin\", 0.10), (\"nxt_angle_cos\", 0.10)]:\n",
    "        j = idxs(p);\n",
    "        if j: beta[j] = gain / max(1, len(j))\n",
    "    for p, gain in [(\"cur_align\", 0.35), (\"nxt_align\", 0.20)]:\n",
    "        j = idxs(p);\n",
    "        if j: beta[j] = np.linspace(gain, 0.0, num=len(j))\n",
    "    if \"speed\" in cols:     beta[cols.index(\"speed\")] = 0.2\n",
    "    if \"curvature\" in cols: beta[cols.index(\"curvature\")] = -0.25\n",
    "\n",
    "    stim_drive = design_nohist.values @ beta\n",
    "\n",
    "    B_hist = meta[\"B_hist\"]\n",
    "    h_hist = B_hist @ np.array([-1.2, -0.8, -0.5, -0.3, -0.2])\n",
    "    Lh = len(h_hist)\n",
    "\n",
    "    baseline = -3.0 + 0.3 * rng.standard_normal(n_trials)\n",
    "\n",
    "    y = np.zeros(T, dtype=int)\n",
    "    for tr in _unique_trials(trial_ids):\n",
    "        idx = np.where(trial_ids == tr)[0]\n",
    "        past = np.zeros(Lh)\n",
    "        b0 = baseline[tr]\n",
    "        for t in idx:\n",
    "            eta = b0 + stim_drive[t] + np.dot(past, h_hist[::-1])\n",
    "            lam = safe_poisson_lambda(eta, dt, max_rate_hz=max_rate_hz)\n",
    "            y[t] = np.random.poisson(lam)\n",
    "            past = np.roll(past, 1); past[0] = y[t]\n",
    "\n",
    "    return {\n",
    "        \"trial_ids\": trial_ids,\n",
    "        \"cur_vis\": cur_vis,\n",
    "        \"nxt_vis\": nxt_vis,\n",
    "        \"cur_dist\": cur_dist,\n",
    "        \"nxt_dist\": nxt_dist,\n",
    "        \"cur_angle\": cur_angle,\n",
    "        \"nxt_angle\": nxt_angle,\n",
    "        \"heading\": heading,\n",
    "        \"speed\": speed,\n",
    "        \"curvature\": curvature,\n",
    "        \"spike_counts\": y,\n",
    "        \"dt\": dt,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# =====================\n",
    "# plotting helpers\n",
    "\n",
    "def _coef_series(result, design_df):\n",
    "    if hasattr(result, 'params'):\n",
    "        params = np.asarray(result.params).ravel()\n",
    "        names = getattr(result, 'exog_names', None)\n",
    "        if names is None and hasattr(result.model, 'exog_names'):\n",
    "            names = result.model.exog_names\n",
    "        if names and names[0] == 'const' and len(params) == len(names):\n",
    "            params = params[1:]\n",
    "            names = names[1:]\n",
    "        return pd.Series(params, index=names if names else list(design_df.columns))\n",
    "    return pd.Series(np.zeros(design_df.shape[1]), index=list(design_df.columns))\n",
    "\n",
    "\n",
    "def reconstruct_kernel(prefix: str, basis: np.ndarray, coef_s: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cols = [c for c in coef_s.index if c.startswith(prefix + \"_rc\")]\n",
    "    if not cols:\n",
    "        return np.arange(basis.shape[0]), np.zeros(basis.shape[0])\n",
    "    def rc_idx(c):\n",
    "        try:\n",
    "            return int(c.split('_rc')[-1])\n",
    "        except:\n",
    "            return 0\n",
    "    cols.sort(key=rc_idx)\n",
    "    w = coef_s.loc[cols].values\n",
    "    k = basis @ w\n",
    "    t = np.arange(basis.shape[0])\n",
    "    return t, k\n",
    "\n",
    "\n",
    "def reconstruct_history_kernel(B_hist: np.ndarray, coef_s: pd.Series) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cols = [c for c in coef_s.index if c.startswith('hist_rc')]\n",
    "    if not cols:\n",
    "        return np.arange(B_hist.shape[0]), np.zeros(B_hist.shape[0])\n",
    "    def rc_idx(c):\n",
    "        try:\n",
    "            return int(c.split('_rc')[-1])\n",
    "        except:\n",
    "            return 0\n",
    "    cols.sort(key=rc_idx)\n",
    "    w = coef_s.loc[cols].values\n",
    "    k = B_hist @ w\n",
    "    t = np.arange(B_hist.shape[0])\n",
    "    return t, k\n",
    "\n",
    "\n",
    "def plot_fitted_kernels(result, design_df, meta, dt, *, prefixes=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if prefixes is None:\n",
    "        prefixes = ['cur_on', 'nxt_on', 'cur_dist', 'nxt_dist', 'cur_angle_sin', 'cur_angle_cos', 'nxt_angle_sin', 'nxt_angle_cos']\n",
    "    coef_s = _coef_series(result, design_df)\n",
    "    B_event, B_short, B_hist = meta['B_event'], meta['B_short'], meta['B_hist']\n",
    "\n",
    "    def pick_basis(p):\n",
    "        if p in ['cur_on', 'nxt_on']:\n",
    "            return B_event\n",
    "        elif p.startswith(('cur_', 'nxt_')):\n",
    "            return B_short\n",
    "        else:\n",
    "            return B_short\n",
    "\n",
    "    for p in prefixes:\n",
    "        B = pick_basis(p)\n",
    "        t, k = reconstruct_kernel(p, B, coef_s)\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.plot(t * dt, k)\n",
    "        plt.xlabel('Time lag (s)'); plt.ylabel('Kernel weight')\n",
    "        plt.title(f'{p} kernel')\n",
    "        plt.show()\n",
    "\n",
    "    t_h, k_h = reconstruct_history_kernel(B_hist, coef_s)\n",
    "    plt.figure()\n",
    "    plt.plot(t_h * dt, k_h)\n",
    "    plt.xlabel('Time lag (s)'); plt.ylabel('History weight')\n",
    "    plt.title('Spike history kernel')\n",
    "    plt.show()\n",
    "# =====================\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glm_models\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools import glm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01  # 10 ms bins for modeling\n",
    "\n",
    "bases_cfg = {\n",
    "    'speed':   {'kind':'cont',  't_max':0.30, 'K':6, 'log':True},   # 0–300 ms filter\n",
    "    'go':      {'kind':'event', 't_max':0.50, 'K':6, 'log':True},   # 0–500 ms post-event\n",
    "    'hist':    {'kind':'hist',  't_max_short':0.025, 'K_short':3,   # 0–25 ms (fine)\n",
    "                              't_max_long':0.250,  'K_long':5},     # 25–250 ms (coarse)\n",
    "    'heading': {'kind':'angle'}                                      # instantaneous sin/cos\n",
    "}\n",
    "\n",
    "model_bundle = glm_models.fit_glm_cv(train_trials, dt, bases_cfg,\n",
    "                          alphas=np.logspace(-4, 1, 8),\n",
    "                          n_splits=5, shuffle_trials=True, random_state=0)\n",
    "print(\"Best alpha:\", model_bundle['best_alpha'])\n",
    "\n",
    "\n",
    "results = evaluate_on_test(train_trials, test_trials, dt, bases_cfg,\n",
    "                           model_bundle, smooth_sigma_ms=40)  # optional PSTH smoothing\n",
    "for k, v in results.items():\n",
    "    if not isinstance(v, np.ndarray):\n",
    "        print(k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.signal import fftconvolve\n",
    "# from scipy.special import gammaln\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import PoissonRegressor\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # -------------------- Bases & design helpers --------------------\n",
    "\n",
    "# def raised_cosine_basis(n_basis, t_max, dt, t_min=0.0, log_spaced=True, eps=1e-3):\n",
    "#     \"\"\"\n",
    "#     Causal raised-cosine basis functions that tile [t_min, t_max].\n",
    "#     Columns are normalized to unit area (sum * dt = 1).\n",
    "#     Returns: lags (L,), B (L x K)\n",
    "#     \"\"\"\n",
    "#     lags = np.arange(0, t_max + dt, dt)\n",
    "\n",
    "#     def warp(x):  # denser near 0 if log_spaced\n",
    "#         return np.log(x + eps) if log_spaced else x\n",
    "\n",
    "#     W = warp(lags)\n",
    "#     c = np.linspace(warp(t_min + eps), warp(t_max), n_basis)\n",
    "#     w = (c[1] - c[0]) if n_basis > 1 else (warp(t_max) - warp(t_min) + 1e-6)\n",
    "\n",
    "#     B = []\n",
    "#     for ci in c:\n",
    "#         arg = (W - ci) * np.pi / w\n",
    "#         b = 0.5 * (1 + np.cos(np.clip(arg, -np.pi, np.pi)))\n",
    "#         b[(W < ci - w) | (W > ci + w)] = 0.0\n",
    "#         B.append(b)\n",
    "#     B = np.stack(B, axis=1)\n",
    "#     B /= (B.sum(axis=0, keepdims=True) * dt + 1e-12)  # unit area\n",
    "#     return lags, B\n",
    "\n",
    "# def convolve_causal(x, k):\n",
    "#     \"\"\"\n",
    "#     Causal convolution of time series x with kernel k (defined on nonnegative lags).\n",
    "#     Output length matches x (truncate 'full' at T).\n",
    "#     \"\"\"\n",
    "#     return fftconvolve(x, k, mode='full')[:len(x)]\n",
    "\n",
    "# def build_design_for_trial(trial, dt, bases_cfg):\n",
    "#     \"\"\"\n",
    "#     trial: dict with keys:\n",
    "#       - 'y': counts per bin (T,)\n",
    "#       - optional continuous series: e.g., 'speed', 'dist', ... each (T,)\n",
    "#       - optional events as binary series: e.g., 'go', 'flash', ... each (T,)\n",
    "#       - optional angle series in radians: e.g., 'heading' (T,)\n",
    "#     bases_cfg: dict describing which features get which bases.\n",
    "#       Example:\n",
    "#         bases_cfg = {\n",
    "#            'speed':  {'kind':'cont',  't_max':0.30, 'K':6, 'log':True},\n",
    "#            'go':     {'kind':'event', 't_max':0.50, 'K':6, 'log':True},\n",
    "#            'hist':   {'kind':'hist',  't_max_short':0.025, 'K_short':3,\n",
    "#                                     't_max_long':0.250, 'K_long':5},\n",
    "#            'heading':{'kind':'angle'}  # instant sin/cos (no lags)\n",
    "#         }\n",
    "#     Returns: X_trial (T x P), colnames (list of str)\n",
    "#     \"\"\"\n",
    "#     y = trial['y']\n",
    "#     T = len(y)\n",
    "#     X_cols = []\n",
    "#     names = []\n",
    "\n",
    "#     # 1) Continuous covariates with temporal filters (project onto basis)\n",
    "#     for key, cfg in bases_cfg.items():\n",
    "#         if cfg.get('kind') == 'cont' and key in trial:\n",
    "#             lags, B = raised_cosine_basis(cfg['K'], cfg['t_max'], dt,\n",
    "#                                           t_min=0.0, log_spaced=cfg.get('log', True))\n",
    "#             for k in range(B.shape[1]):\n",
    "#                 X_cols.append(convolve_causal(trial[key], B[:, k]))\n",
    "#                 names.append(f\"{key}_rc{k+1}\")\n",
    "#     # 2) Event covariates (binary impulses convolved with basis)\n",
    "#     for key, cfg in bases_cfg.items():\n",
    "#         if cfg.get('kind') == 'event' and key in trial:\n",
    "#             lags, B = raised_cosine_basis(cfg['K'], cfg['t_max'], dt,\n",
    "#                                           t_min=0.0, log_spaced=cfg.get('log', True))\n",
    "#             for k in range(B.shape[1]):\n",
    "#                 X_cols.append(convolve_causal(trial[key], B[:, k]))\n",
    "#                 names.append(f\"{key}_rc{k+1}\")\n",
    "\n",
    "#     # 3) Spike history: fine 0–t_max_short and coarse t_max_short–t_max_long\n",
    "#     if 'hist' in bases_cfg:\n",
    "#         cfg = bases_cfg['hist']\n",
    "\n",
    "#         # Full lag grid up to the long window (for padding)\n",
    "#         l_full = np.arange(0, cfg['t_max_long'] + dt, dt)\n",
    "#         L_full = len(l_full)\n",
    "\n",
    "#         # Short window bases (0 .. t_max_short), linear spacing\n",
    "#         l1, B1 = raised_cosine_basis(\n",
    "#             n_basis=cfg['K_short'],\n",
    "#             t_max=cfg['t_max_short'],\n",
    "#             dt=dt,\n",
    "#             t_min=0.0,\n",
    "#             log_spaced=False\n",
    "#         )\n",
    "\n",
    "#         # Long window bases (t_max_short .. t_max_long), log spacing\n",
    "#         l2, B2 = raised_cosine_basis(\n",
    "#             n_basis=cfg['K_long'],\n",
    "#             t_max=cfg['t_max_long'],\n",
    "#             dt=dt,\n",
    "#             t_min=cfg['t_max_short'],\n",
    "#             log_spaced=True\n",
    "#         )\n",
    "#         # B2 is already defined on [0 .. t_max_long] (zeros before t_min),\n",
    "#         # so its number of rows should be L_full. B1 has fewer rows; pad it.\n",
    "\n",
    "#         # Zero-pad B1 to the full length\n",
    "#         pad_B1 = np.zeros((L_full, B1.shape[1]))\n",
    "#         pad_B1[:len(l1), :] = B1\n",
    "\n",
    "#         # Make sure B2 has the same number of rows (guard small rounding diffs)\n",
    "#         if len(l2) != L_full:\n",
    "#             pad_B2 = np.zeros((L_full, B2.shape[1]))\n",
    "#             L = min(L_full, len(l2))\n",
    "#             pad_B2[:L, :] = B2[:L, :]\n",
    "#             Bhist = np.hstack([pad_B1, pad_B2])  # columns = K_short + K_long\n",
    "#         else:\n",
    "#             Bhist = np.hstack([pad_B1, B2])\n",
    "\n",
    "#         # Convolve trial spikes with each history basis column (causal)\n",
    "#         for k in range(Bhist.shape[1]):\n",
    "#             X_cols.append(convolve_causal(y, Bhist[:, k]))\n",
    "#             names.append(f\"hist_rc{k+1}\")\n",
    "\n",
    "\n",
    "#     # 4) Instantaneous angle features (no lags): sin/cos for circular variables\n",
    "#     for key, cfg in bases_cfg.items():\n",
    "#         if cfg.get('kind') == 'angle' and key in trial:\n",
    "#             ang = trial[key]\n",
    "#             X_cols.append(np.sin(ang)); names.append(f\"{key}_sin\")\n",
    "#             X_cols.append(np.cos(ang)); names.append(f\"{key}_cos\")\n",
    "\n",
    "#     if not X_cols:\n",
    "#         X = np.zeros((T, 0))\n",
    "#     else:\n",
    "#         X = np.column_stack(X_cols)\n",
    "#     return X, names\n",
    "\n",
    "# # -------------------- Metrics --------------------\n",
    "\n",
    "# def loglik_poisson(y, mu):\n",
    "#     \"\"\"Sum log-likelihood for Poisson counts with mean mu (counts/bin).\"\"\"\n",
    "#     mu = np.clip(mu, 1e-12, None)\n",
    "#     return float(np.sum(y * np.log(mu) - mu - gammaln(y + 1)))\n",
    "\n",
    "# def bits_per_spike(y, mu, mu0=None):\n",
    "#     \"\"\"\n",
    "#     Bits/spike relative to a baseline (mu0 = homogeneous mean if None).\n",
    "#     y, mu, mu0 are in counts/bin.\n",
    "#     \"\"\"\n",
    "#     if mu0 is None:\n",
    "#         mu0 = np.full_like(y, y.sum() / len(y))\n",
    "#     LLm = loglik_poisson(y, mu)\n",
    "#     LL0 = loglik_poisson(y, mu0)\n",
    "#     return (LLm - LL0) / (y.sum() * np.log(2) + 1e-12)\n",
    "\n",
    "# def psth_from_trials(trials_counts, dt, smooth_kernel=None):\n",
    "#     \"\"\"\n",
    "#     Average counts across trials, optional smoothing on counts,\n",
    "#     then convert to Hz (divide by dt).\n",
    "#     \"\"\"\n",
    "#     M = np.mean(np.stack(trials_counts), axis=0)  # counts/bin\n",
    "#     if smooth_kernel is not None:\n",
    "#         M = np.convolve(M, smooth_kernel, mode='same')\n",
    "#     return M / dt  # Hz\n",
    "\n",
    "# def split_half_sb(test_trials_counts, dt, n_splits=200, rng=0, center=True, smooth_kernel=None):\n",
    "#     rng = np.random.default_rng(rng)\n",
    "#     counts = list(test_trials_counts)\n",
    "#     rsb = []\n",
    "#     for _ in range(n_splits):\n",
    "#         idx = rng.permutation(len(counts))\n",
    "#         A = [counts[i] for i in idx[::2]]\n",
    "#         B = [counts[i] for i in idx[1::2]]\n",
    "#         if len(A) == 0 or len(B) == 0:\n",
    "#             continue\n",
    "#         pA = psth_from_trials(A, dt, smooth_kernel)  # << same smoothing as eval\n",
    "#         pB = psth_from_trials(B, dt, smooth_kernel)\n",
    "#         if center:\n",
    "#             pA -= pA.mean(); pB -= pB.mean()\n",
    "#         r = np.dot(pA, pB) / (np.linalg.norm(pA)*np.linalg.norm(pB) + 1e-12)\n",
    "#         rsb.append(2*r / (1 + r))  # Spearman–Brown\n",
    "#     return float(np.mean(rsb)) if rsb else np.nan\n",
    "\n",
    "# # -------------------- Cross-validated GLM fit --------------------\n",
    "\n",
    "# def fit_glm_cv(trials, dt, bases_cfg, alphas=np.logspace(-4, 1, 8),\n",
    "#                n_splits=5, shuffle_trials=True, random_state=0):\n",
    "#     \"\"\"\n",
    "#     trials: list of per-trial dicts. Each must contain:\n",
    "#         'y'  (counts, shape T,)\n",
    "#       Optional per-trial arrays of same length T:\n",
    "#         e.g., 'speed', 'heading', 'go', ...\n",
    "#     Returns dict with model, scalers, column names, and CV metrics.\n",
    "#     \"\"\"\n",
    "#     # Build per-trial design matrices\n",
    "#     X_trials, y_trials = [], []\n",
    "#     for tr in trials:\n",
    "#         Xtr, _names = build_design_for_trial(tr, dt, bases_cfg)\n",
    "#         X_trials.append(Xtr); y_trials.append(tr['y'])\n",
    "\n",
    "#     # CV split by trials (NOT by time-bins)\n",
    "#     kf = KFold(n_splits=n_splits, shuffle=shuffle_trials, random_state=random_state)\n",
    "\n",
    "#     # Hyperparameter search (ridge strength)\n",
    "#     best_alpha, best_ll = None, -np.inf\n",
    "#     for a in alphas:\n",
    "#         ll_sum = 0.0\n",
    "#         for tr_idx, te_idx in kf.split(X_trials):\n",
    "#             # Assemble train arrays\n",
    "#             Xtr = np.vstack([X_trials[i] for i in tr_idx])\n",
    "#             ytr = np.concatenate([y_trials[i] for i in tr_idx])\n",
    "\n",
    "#             # Fit scaler on TRAIN ONLY\n",
    "#             scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "#             Xtrz = scaler.fit_transform(Xtr)\n",
    "\n",
    "#             # Fit Poisson GLM (ridge)\n",
    "#             model = PoissonRegressor(alpha=a, max_iter=5000, fit_intercept=True)\n",
    "#             model.fit(Xtrz, ytr)\n",
    "\n",
    "#             # Evaluate on TEST trials\n",
    "#             ll_fold = 0.0\n",
    "#             for i in te_idx:\n",
    "#                 Xte = X_trials[i]; yte = y_trials[i]\n",
    "#                 Xtez = scaler.transform(Xte)\n",
    "#                 mu = model.predict(Xtez)  # mean counts/bin\n",
    "#                 ll_fold += loglik_poisson(yte, mu)\n",
    "#             ll_sum += ll_fold\n",
    "#         if ll_sum > best_ll:\n",
    "#             best_ll, best_alpha = ll_sum, a\n",
    "\n",
    "#     # Refit on ALL trials with best alpha\n",
    "#     X_all = np.vstack(X_trials)\n",
    "#     y_all = np.concatenate(y_trials)\n",
    "#     scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "#     X_all_z = scaler.fit_transform(X_all)\n",
    "#     final_model = PoissonRegressor(alpha=best_alpha, max_iter=5000, fit_intercept=True)\n",
    "#     final_model.fit(X_all_z, y_all)\n",
    "\n",
    "#     return {\n",
    "#         \"model\": final_model,\n",
    "#         \"scaler\": scaler,\n",
    "#         \"X_trials\": X_trials,\n",
    "#         \"y_trials\": y_trials,\n",
    "#         \"colnames\": _names,  # from the last built trial (same across trials)\n",
    "#         \"dt\": dt,\n",
    "#         \"best_alpha\": best_alpha\n",
    "#     }\n",
    "\n",
    "# # -------------------- Evaluation on held-out trials --------------------\n",
    "\n",
    "# def evaluate_on_test(train_trials, test_trials, dt, bases_cfg, model_bundle,\n",
    "#                      smooth_sigma_ms=None):\n",
    "#     \"\"\"\n",
    "#     Build PSTHs & metrics on held-out TEST trials.\n",
    "#     Optionally smooth counts (both data & model) with Gaussian kernel (σ in ms).\n",
    "#     \"\"\"\n",
    "#     # Gaussian kernel (on counts) that preserves units (sum*dt = 1)\n",
    "#     smooth_kernel = None\n",
    "#     if smooth_sigma_ms is not None:\n",
    "#         sigma_s = smooth_sigma_ms / 1000.0\n",
    "#         sigma_bins = max(1, int(round(sigma_s / dt)))\n",
    "#         n = np.arange(-5*sigma_bins, 5*sigma_bins + 1)\n",
    "#         g = np.exp(-0.5 * (n / sigma_bins)**2)\n",
    "#         g /= (g.sum() * dt)\n",
    "#         smooth_kernel = g\n",
    "\n",
    "#     # Build per-trial designs for TEST\n",
    "#     X_te, y_te = [], []\n",
    "#     for tr in test_trials:\n",
    "#         X, _ = build_design_for_trial(tr, dt, bases_cfg)\n",
    "#         X_te.append(X); y_te.append(tr['y'])\n",
    "\n",
    "#     # Predict per test trial\n",
    "#     model = model_bundle['model']\n",
    "#     scaler = model_bundle['scaler']\n",
    "#     mu_te = []\n",
    "#     for X, y in zip(X_te, y_te):\n",
    "#         mu = model.predict(scaler.transform(X))  # counts/bin\n",
    "#         mu_te.append(mu)\n",
    "\n",
    "#     # Metrics: held-out LL and bits/spike (concatenated across test trials)\n",
    "#     y_cat  = np.concatenate(y_te)\n",
    "#     mu_cat = np.concatenate(mu_te)\n",
    "#     bps = bits_per_spike(y_cat, mu_cat)  # vs homogeneous baseline on test\n",
    "#     LL  = loglik_poisson(y_cat, mu_cat)\n",
    "\n",
    "#     # PSTH correlation and ceiling-normalized score on TEST\n",
    "#     psth_data  = psth_from_trials(y_te, dt, smooth_kernel)\n",
    "#     psth_model = psth_from_trials(mu_te, dt, smooth_kernel)\n",
    "#     # mean-center before correlation if you care about *shape*\n",
    "#     r_model = np.corrcoef(psth_model - psth_model.mean(),\n",
    "#                           psth_data  - psth_data.mean())[0, 1]\n",
    "#     r_sb = split_half_sb(y_te, dt, n_splits=200, rng=1, center=True)\n",
    "#     r_ceiling = np.sqrt(max(r_sb, 0.0))\n",
    "#     r_norm = np.clip(r_model / (r_ceiling + 1e-12), 0, 1) if np.isfinite(r_ceiling) else np.nan\n",
    "\n",
    "#     return {\n",
    "#         \"LL_test\": LL,\n",
    "#         \"bits_per_spike_test\": bps,\n",
    "#         \"psth_corr_test\": r_model,\n",
    "#         \"psth_ceiling_rsb\": r_sb,\n",
    "#         \"psth_ceiling_sqrt\": r_ceiling,\n",
    "#         \"psth_corr_normalized\": r_norm,\n",
    "#         \"psth_model_Hz\": psth_model,\n",
    "#         \"psth_data_Hz\": psth_data\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def _exp_kernel(tau_s, dt, L_mult=5, unit_area=True):\n",
    "#     \"\"\"One-sided exponential kernel k(t) ~ exp(-t/tau), t>=0.\"\"\"\n",
    "#     L = max(1, int(round(L_mult * tau_s / dt)))\n",
    "#     k = np.exp(-np.arange(L) * dt / tau_s)\n",
    "#     if unit_area:\n",
    "#         # Normalize so sum(k)*dt = 1 → unit area (stable across dt)\n",
    "#         k /= (k.sum() * dt)\n",
    "#     return k\n",
    "\n",
    "# def _causal_conv(x, k):\n",
    "#     \"\"\"Causal convolution (truncate to len(x)).\"\"\"\n",
    "#     from scipy.signal import fftconvolve\n",
    "#     return fftconvolve(x, k, mode='full')[:len(x)]\n",
    "\n",
    "# def make_synthetic_trials(\n",
    "#     n_conditions=3,\n",
    "#     train_repeats_per_cond=10,\n",
    "#     test_repeats_Cstar=4,\n",
    "#     C_star=0,\n",
    "#     T_s=2.0,       # trial length (s)\n",
    "#     dt=0.01,       # bin size (s) → 10 ms\n",
    "#     seed=0,\n",
    "#     # safety knobs:\n",
    "#     target_peak_hz=30.0,   # try to keep peak expected rate (no-history) ≤ this\n",
    "#     clip_peak_hz=80.0      # hard cap on expected rate during sampling\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#       train_trials: list of dicts (keys: 'y','speed','heading','go','condition','cond_angle')\n",
    "#       test_trials : same keys (held-out repeats of condition C_star only)\n",
    "#     \"\"\"\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     T = int(round(T_s / dt))\n",
    "#     t = np.arange(T) * dt\n",
    "\n",
    "#     # --- true underlying filters/weights (used to simulate spikes) ---\n",
    "#     # Kernels (unit-area so weights are gains)\n",
    "#     k_evt   = _exp_kernel(tau_s=0.12, dt=dt)   # 120 ms event kernel (unit area)\n",
    "#     k_speed = _exp_kernel(tau_s=0.15, dt=dt)   # 150 ms speed kernel (unit area)\n",
    "#     k_hist  = _exp_kernel(tau_s=0.02, dt=dt)   # history kernel (unit area)\n",
    "\n",
    "#     # Coefficients (log-link)\n",
    "#     # log μ = b0 + β_evt*(go⊛k_evt) + β_spd*(speed⊛k_speed)\n",
    "#     #         + β_sin*sin(heading) + β_cos*cos(heading) + hist_gain*(y⊛k_hist)\n",
    "#     b0         = np.log(0.03)   # ~3 Hz baseline at dt=0.01\n",
    "#     beta_evt   = 0.12           # event adds ≈ 1.0 to log-rate at onset (unit-area kernel)\n",
    "#     beta_speed = 0.01           # modest effect of smoothed speed\n",
    "#     beta_sin   = 0.20\n",
    "#     beta_cos   = -0.10\n",
    "#     hist_gain  = -1.0           # inhibitory history\n",
    "\n",
    "#     # Condition angles (e.g., target directions)\n",
    "#     cond_angles = np.linspace(0, 2*np.pi, n_conditions, endpoint=False)\n",
    "\n",
    "#     train_trials, test_trials = [], []\n",
    "\n",
    "#     def simulate_one_trial(cond_idx):\n",
    "#         phi = cond_angles[cond_idx]  # condition angle\n",
    "\n",
    "#         # Event binary series (aligned near t=0.2 s)\n",
    "#         go = np.zeros(T)\n",
    "#         go[int(0.2 / dt)] = 1.0\n",
    "\n",
    "#         # Speed: baseline + event-driven bump + colored noise\n",
    "#         speed_base = 10.0 + 2.0 * rng.normal()\n",
    "#         bump = 6.0 * _causal_conv(go, _exp_kernel(0.4, dt))               # slower bump\n",
    "#         noise = _causal_conv(rng.normal(0, 0.20, size=T), _exp_kernel(0.05, dt))\n",
    "#         speed = np.maximum(0.0, speed_base + bump + noise)                 # cm/s\n",
    "\n",
    "#         # Heading: around the condition angle with slow jitter (wrap to [-π, π])\n",
    "#         heading = phi + _causal_conv(rng.normal(0, 0.10, size=T), _exp_kernel(0.2, dt))\n",
    "#         heading = (heading + np.pi) % (2*np.pi) - np.pi\n",
    "\n",
    "#         # Covariate-driven part of the linear predictor (no history)\n",
    "#         x_evt   = _causal_conv(go,    k_evt)\n",
    "#         x_speed = _causal_conv(speed, k_speed)\n",
    "#         lin_nohist = (b0\n",
    "#                       + beta_evt   * x_evt\n",
    "#                       + beta_speed * x_speed\n",
    "#                       + beta_sin   * np.sin(heading)\n",
    "#                       + beta_cos   * np.cos(heading))\n",
    "\n",
    "#         # ---------- AUTO-CALIBRATE PEAK so expected counts/bin don't explode ----------\n",
    "#         # Target peak expected counts/bin (no-history)\n",
    "#         mu_target = max(1e-6, target_peak_hz * dt)   # e.g., 30 Hz @ 10ms → 0.30\n",
    "#         mu_peak_nh = float(np.exp(np.max(lin_nohist)))\n",
    "#         if mu_peak_nh > mu_target:\n",
    "#             shift = np.log(mu_target) - np.log(mu_peak_nh)\n",
    "#             lin_nohist = lin_nohist + shift   # equivalent to lowering b0 for this trial\n",
    "\n",
    "#         # ---------- Simulate spikes with inhibitory history ----------\n",
    "#         y = np.zeros(T, dtype=int)\n",
    "#         mu_clip = max(1e-6, clip_peak_hz * dt)       # final backstop, e.g., 80 Hz @ 10ms → 0.80\n",
    "\n",
    "#         for tt in range(T):\n",
    "#             # history term from past spikes only (exclude current bin)\n",
    "#             Lh = min(tt, len(k_hist) - 1)\n",
    "#             if Lh > 0:\n",
    "#                 hist_term = hist_gain * np.dot(y[tt-Lh:tt][::-1], k_hist[1:Lh+1])\n",
    "#             else:\n",
    "#                 hist_term = 0.0\n",
    "\n",
    "#             eta = lin_nohist[tt] + hist_term         # log(mean counts per bin)\n",
    "#             mu  = np.exp(eta)\n",
    "#             # Hard cap as a safety backstop (should rarely activate after the shift)\n",
    "#             if mu > mu_clip:\n",
    "#                 mu = mu_clip\n",
    "#             y[tt] = rng.poisson(mu)\n",
    "\n",
    "#         return {\n",
    "#             'y': y,\n",
    "#             'speed': speed,\n",
    "#             'heading': heading,\n",
    "#             'go': go,\n",
    "#             'condition': int(cond_idx),\n",
    "#             'cond_angle': float(phi)\n",
    "#         }\n",
    "\n",
    "#     # Build trials for each condition\n",
    "#     for c in range(n_conditions):\n",
    "#         for _ in range(train_repeats_per_cond):\n",
    "#             train_trials.append(simulate_one_trial(c))\n",
    "#         if c == C_star:\n",
    "#             for _ in range(test_repeats_Cstar):\n",
    "#                 test_trials.append(simulate_one_trial(c))\n",
    "\n",
    "#     rng.shuffle(train_trials)\n",
    "#     rng.shuffle(test_trials)\n",
    "#     return train_trials, test_trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data: 3 conditions; hold out 4 repeats of condition 0 for Test.\n",
    "train_trials, test_trials = make_synthetic_trials(\n",
    "    n_conditions=3, train_repeats_per_cond=12, test_repeats_Cstar=4,\n",
    "    C_star=0, T_s=2.0, dt=0.01, seed=1\n",
    ")\n",
    "\n",
    "# Inspect shapes quickly\n",
    "T = len(train_trials[0]['y'])\n",
    "print(f\"Train trials: {len(train_trials)} | Test trials: {len(test_trials)} | T bins per trial: {T}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trials[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases_cfg = {\n",
    "    'speed':   {'kind':'cont',  't_max':0.30, 'K':6, 'log':True},\n",
    "    'go':      {'kind':'event', 't_max':0.50, 'K':6, 'log':True},\n",
    "    'hist':    {'kind':'hist',  't_max_short':0.025, 'K_short':3,\n",
    "                                't_max_long':0.250,  'K_long':5},\n",
    "    'heading': {'kind':'angle'}\n",
    "}\n",
    "bundle = fit_glm_cv(train_trials, dt=0.01, bases_cfg=bases_cfg)\n",
    "results = evaluate_on_test(train_trials, test_trials, 0.01, bases_cfg, bundle, smooth_sigma_ms=40)\n",
    "print({k:v for k,v in results.items() if not hasattr(v, '__len__')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test trials:\", len(test_trials))\n",
    "print(\"Test spikes (sum y):\", int(np.sum(np.concatenate([tr['y'] for tr in test_trials]))))\n",
    "print(\"Bins per trial:\", len(test_trials[0]['y']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(pn.x_var, pn.y_var_reduced, pn.bin_width, pn.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.prepare_for_pgam(num_total_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal kernel\n",
    "\n",
    "modified from PGAM_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_temporal_features_to_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gdh.smooths_handler.add_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst._add_spatial_features_to_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.run_pgam(neural_cluster_number=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.post_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "# Degree of spline\n",
    "k = 3\n",
    "\n",
    "# Example 1: Not enough start knots\n",
    "knots1 = [0, 2, 4, 6, 8, 10, 10, 10, 10]\n",
    "coeffs = np.ones(len(knots1) - k - 1)\n",
    "spline1 = BSpline(knots1, coeffs, k)\n",
    "\n",
    "# Example 2: Proper repeated start knots\n",
    "knots2 = [0, 0, 0, 0, 2, 4, 6, 8, 10, 10, 10, 10]\n",
    "coeffs2 = np.ones(len(knots2) - k - 1)\n",
    "spline2 = BSpline(knots2, coeffs2, k)\n",
    "\n",
    "x = np.linspace(-1, 11, 400)\n",
    "\n",
    "plt.plot(x, spline1(x), label=\"Start knot once\")\n",
    "plt.plot(x, spline2(x), label=\"Start knot repeated\")\n",
    "plt.axvline(0, color='gray', linestyle='--', label=\"x = 0\")\n",
    "plt.legend()\n",
    "plt.title(\"Effect of Repeating Start Knot\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Spline Value\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterate through all neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgam_inst = pgam_class.PGAMclass(pn.x_var, pn.y_var, pn.bin_width, pn.processed_neural_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(pn.x_var.shape[1]):\n",
    "    print(f'neural_cluster_number: {i} out of {pn.x_var.shape[1]}')\n",
    "    pgam_inst.streamline_pgam(neural_cluster_number=i, num_total_trials=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
