{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils, combine_info_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural.pn_decoding import pn_decoding_utils, plot_pn_decoding, pn_decoding_model_specs\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "import quantities as pq\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specs and funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features = [\n",
    "    'new_bin', 'new_segment', 'whether_test',\n",
    "    'cur_eye_hor_l', 'cur_eye_ver_l', 'cur_eye_hor_r', 'cur_eye_ver_r',\n",
    "    'nxt_eye_hor_l', 'nxt_eye_ver_l', 'nxt_eye_hor_r', 'nxt_eye_ver_r',\n",
    "    'LDz', 'RDz', 'LDx', 'RDx',\n",
    "    'gaze_mky_view_x', 'gaze_mky_view_y', 'gaze_mky_view_angle',\n",
    "    'cur_opt_arc_dheading', 'cur_ff_distance',\n",
    "    'cur_ff_rel_x', 'cur_ff_rel_y', 'nxt_ff_rel_x', 'nxt_ff_rel_y', 'nxt_ff_distance',\n",
    "    'num_ff_visible', 'num_ff_in_memory',\n",
    "    'cur_ff_distance_at_ref', 'cur_ff_angle_boundary_at_ref', 'nxt_ff_distance_at_ref',\n",
    "    'speed', 'ang_speed', 'accel', 'ang_accel',\n",
    "    'monkey_speeddummy', 'curv_of_traj',\n",
    "    'angle_from_cur_ff_to_nxt_ff',\n",
    "    'time_since_last_capture', 'bin_mid_time_rel_to_event',\n",
    "    'time', 'target_index',\n",
    "    'cur_vis', 'nxt_vis', 'nxt_in_memory', 'any_ff_visible',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through run_segment_split_regression_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_raw_spike_data_instead=True\n",
    "apply_pca_on_raw_spike_data=False\n",
    "use_lagged_raw_spike_data=True\n",
    "\n",
    "use_lagged_rebinned_behav_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Bruno')\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print('='*100)\n",
    "    print('='*100)\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    \n",
    "    reduce_y_var_lags = False\n",
    "    planning_data_by_point_exists_ok = True\n",
    "    y_data_exists_ok = True\n",
    "    bin_width = 0.1\n",
    "    \n",
    "    try:\n",
    "        pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path, bin_width=bin_width)\n",
    "        pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "        pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "            pn.planning_data_by_point)\n",
    "        pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "        \n",
    "            \n",
    "        pn.prepare_seg_aligned_data()\n",
    "        if not use_raw_spike_data_instead:\n",
    "            pn.get_gpfa_traj(latent_dimensionality=7, exists_ok=True)\n",
    "\n",
    "        pn.get_concat_data_for_regression(use_raw_spike_data_instead=use_raw_spike_data_instead,\n",
    "                                        use_lagged_rebinned_behav_data=use_lagged_rebinned_behav_data,\n",
    "                                        apply_pca_on_raw_spike_data=apply_pca_on_raw_spike_data,\n",
    "                                        use_lagged_raw_spike_data=use_lagged_raw_spike_data,) \n",
    "\n",
    "\n",
    "        pn.concat_behav_trials, added_cols = pn_decoding_utils.prep_behav(pn.concat_behav_trials)\n",
    "        pn.rebinned_behav_data, _ = pn_decoding_utils.prep_behav(pn.rebinned_behav_data)\n",
    "        overall_key_feats = list(set(key_features + added_cols))\n",
    "        pn.concat_behav_trials = pn.concat_behav_trials[overall_key_feats].copy()\n",
    "        pn.rebinned_behav_data = pn.rebinned_behav_data[overall_key_feats].copy()\n",
    "        \n",
    "        pn.print_data_dimensions()\n",
    "        \n",
    "        mask = pn.concat_behav_trials['bin_mid_time_rel_to_event'] > 0\n",
    "        pn.concat_behav_trials = pn.concat_behav_trials[mask]\n",
    "        pn.concat_neural_trials = pn.concat_neural_trials[mask]\n",
    "        \n",
    "        \n",
    "\n",
    "        pn.separate_test_and_control_data()\n",
    "        # columns_of_interest = ['whether_test', 'cur_ff_distance', 'cur_ff_angle', 'nxt_ff_distance', 'nxt_ff_rel_y', 'nxt_opt_arc_dheading', 'nxt_ff_rel_x', 'nxt_ff_angle', 'nxt_ff_angle_at_ref']\n",
    "        columns_of_interest = pn.concat_behav_trials.columns\n",
    "        all_results = []\n",
    "        for test_or_control in ['both']: #['test', 'control', 'both']:\n",
    "            x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control=test_or_control)\n",
    "            \n",
    "            results_summary = ml_methods_utils.run_segment_split_regression_cv(\n",
    "                x_var, \n",
    "                y_var, \n",
    "                columns_of_interest, \n",
    "                num_folds=5, \n",
    "            )\n",
    "            results_summary['test_or_control'] = test_or_control\n",
    "            all_results.append(results_summary)\n",
    "\n",
    "        all_results = pd.concat(all_results)\n",
    "        all_results.head()\n",
    "\n",
    "        reg_results = all_results[all_results['Model'] == 'Linear Regression']\n",
    "        class_results = all_results[all_results['Model'] == 'Logistic Regression']\n",
    "\n",
    "        # first only plot key_features2 cur_ff_distance\n",
    "        key_features2 = ['cur_ff_distance', 'log1p_cur_ff_distance', 'speed', 'accel', 'time_since_last_capture']\n",
    "        for metric in ['test_r2']:\n",
    "            ml_methods_utils.make_barplot_to_compare_results(\n",
    "                    reg_results, \n",
    "                    metric=metric, \n",
    "                    features=key_features2,\n",
    "                )\n",
    "            print('='*100)\n",
    "            print('='*100)\n",
    "\n",
    "\n",
    "        rest_of_features = [c for c in reg_results['Feature'].unique() if c not in key_features2]\n",
    "\n",
    "        # regression results\n",
    "        for metric in ['test_r2']:\n",
    "            ml_methods_utils.make_barplot_to_compare_results(\n",
    "                    reg_results, \n",
    "                    metric=metric, \n",
    "                    features=rest_of_features,\n",
    "                )\n",
    "            print('='*100)\n",
    "            print('='*100)\n",
    "            \n",
    "        # classification results\n",
    "        for metric in ['test_roc_auc']:\n",
    "            ml_methods_utils.make_barplot_to_compare_results(\n",
    "                class_results, \n",
    "                metric=metric, \n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['data_name']}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Save the current state of the notebook\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY GPFA NEXT (for the above) !!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through 'decode_by_num_ff_visible_or_in_memory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note: i can try various combos of neural data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_visibility_col = 'num_ff_visible'\n",
    "\n",
    "\n",
    "use_raw_spike_data_instead=True\n",
    "apply_pca_on_raw_spike_data=False\n",
    "use_lagged_raw_spike_data=True\n",
    "\n",
    "use_lagged_rebinned_behav_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reload(pn_decoding_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Bruno')\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print('='*100)\n",
    "    print('='*100)\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    \n",
    "    reduce_y_var_lags = False\n",
    "    planning_data_by_point_exists_ok = True\n",
    "    y_data_exists_ok = True\n",
    "    bin_width = 0.1\n",
    "    \n",
    "    try:\n",
    "        pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path, bin_width=bin_width)\n",
    "        pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "        pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "            pn.planning_data_by_point)\n",
    "        pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "        \n",
    "            \n",
    "        pn.prepare_seg_aligned_data()\n",
    "        if not use_raw_spike_data_instead:\n",
    "            pn.get_gpfa_traj(latent_dimensionality=7, exists_ok=True)\n",
    "\n",
    "        # for regression later\n",
    "        pn.get_concat_data_for_regression(use_raw_spike_data_instead=use_raw_spike_data_instead,\n",
    "                                        use_lagged_rebinned_behav_data=use_lagged_rebinned_behav_data,\n",
    "                                        apply_pca_on_raw_spike_data=apply_pca_on_raw_spike_data,\n",
    "                                        use_lagged_raw_spike_data=use_lagged_raw_spike_data,) \n",
    "\n",
    "        # get key_features2\n",
    "        pn.concat_behav_trials, cols_added = pn_decoding_utils.add_interaction_terms_and_features(pn.concat_behav_trials)\n",
    "        key_features2 = (\n",
    "            ['cur_ff_distance', 'log1p_cur_ff_distance', 'speed',\n",
    "                'accel', 'time_since_last_capture']\n",
    "            + cols_added\n",
    "        )\n",
    "\n",
    "\n",
    "        x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control='both')\n",
    "        y_var, cols_added = pn_decoding_utils.add_interaction_terms_and_features(y_var)\n",
    "        y_var, added_cols = pn_decoding_utils.prep_behav(y_var)\n",
    "\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        save_path = os.path.join(pn.planning_and_neural_folder_path, 'pn_decoding', 'conditioned_on_ff_visibility')\n",
    "        for model_name, spec in pn_decoding_model_specs.MODEL_SPECS.items():\n",
    "\n",
    "            print(f'model_name: ', model_name)\n",
    "                        \n",
    "            config = pn_decoding_utils.DecodingRunConfig(\n",
    "                model_class=spec['model_class'],\n",
    "                model_kwargs=spec['model_kwargs'],\n",
    "                use_early_stopping=False,\n",
    "                make_plots=False,\n",
    "            )\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                results_df = pn_decoding_utils.decode_by_num_ff_visible_or_in_memory(\n",
    "                    x_var,\n",
    "                    y_var,\n",
    "                    key_features2,\n",
    "                    config=config,\n",
    "                    save_path=save_path,\n",
    "                    ff_visibility_col=ff_visibility_col,\n",
    "                \n",
    "            )\n",
    "\n",
    "            results_df['model_name'] = model_name\n",
    "            all_results.append(results_df)\n",
    "\n",
    "        all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "        plot_pn_decoding.plot_decoding_heatmaps_with_n(\n",
    "            all_results_df,\n",
    "            ff_visibility_col)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['data_name']}: {e}\")\n",
    "        continue\n",
    "\n",
    "    break\n",
    "    # Save the current state of the notebook\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffled control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "    raw_data_dir_name, 'monkey_Bruno')\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print('='*100)\n",
    "    print('='*100)\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    \n",
    "    reduce_y_var_lags = False\n",
    "    planning_data_by_point_exists_ok = True\n",
    "    y_data_exists_ok = True\n",
    "    bin_width = 0.1\n",
    "    \n",
    "    try:\n",
    "        pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path, bin_width=bin_width)\n",
    "        pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "        pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "            pn.planning_data_by_point)\n",
    "        pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "        \n",
    "            \n",
    "        pn.prepare_seg_aligned_data()\n",
    "        if not use_raw_spike_data_instead:\n",
    "            pn.get_gpfa_traj(latent_dimensionality=7, exists_ok=True)\n",
    "\n",
    "        # for regression later\n",
    "        pn.get_concat_data_for_regression(use_raw_spike_data_instead=use_raw_spike_data_instead,\n",
    "                                        use_lagged_rebinned_behav_data=use_lagged_rebinned_behav_data,\n",
    "                                        apply_pca_on_raw_spike_data=apply_pca_on_raw_spike_data,\n",
    "                                        use_lagged_raw_spike_data=use_lagged_raw_spike_data,) \n",
    "\n",
    "        # get key_features2\n",
    "        pn.concat_behav_trials, cols_added = pn_decoding_utils.add_interaction_terms_and_features(pn.concat_behav_trials)\n",
    "        key_features2 = (\n",
    "            ['cur_ff_distance', 'log1p_cur_ff_distance', 'speed',\n",
    "                'accel', 'time_since_last_capture']\n",
    "            + cols_added\n",
    "        )\n",
    "\n",
    "\n",
    "        x_var, y_var = pn.get_concat_x_and_y_var_for_lr(test_or_control='both')\n",
    "        y_var, cols_added = pn_decoding_utils.add_interaction_terms_and_features(y_var)\n",
    "        y_var, added_cols = pn_decoding_utils.prep_behav(y_var)\n",
    "\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        save_path = os.path.join(pn.planning_and_neural_folder_path, 'pn_decoding', 'conditioned_on_ff_visibility')\n",
    "        for model_name, spec in pn_decoding_model_specs.MODEL_SPECS.items():\n",
    "            \n",
    "            print(f'model_name: ', model_name)\n",
    "            \n",
    "            config = pn_decoding_utils.DecodingRunConfig(\n",
    "                model_class=spec['model_class'],\n",
    "                model_kwargs=spec['model_kwargs'],\n",
    "                use_early_stopping=False,\n",
    "                make_plots=False,\n",
    "            )\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                results_df = pn_decoding_utils.decode_by_num_ff_visible_or_in_memory(\n",
    "                    x_var,\n",
    "                    y_var,\n",
    "                    key_features2,\n",
    "                    config=config,\n",
    "                    save_path=save_path,\n",
    "                    ff_visibility_col=ff_visibility_col,\n",
    "                    shuffle_y=True\n",
    "            )\n",
    "\n",
    "            results_df['model_name'] = model_name\n",
    "            all_results.append(results_df)\n",
    "\n",
    "        all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "        plot_pn_decoding.plot_decoding_heatmaps_with_n(\n",
    "            all_results_df,\n",
    "            ff_visibility_col)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['data_name']}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    break\n",
    "    # Save the current state of the notebook\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first try for same session, use multiple combos of neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_features = [\n",
    "#     'new_bin', 'new_segment', 'whether_test',\n",
    "#     'cur_eye_hor_l', 'cur_eye_ver_l', 'cur_eye_hor_r', 'cur_eye_ver_r',\n",
    "#     'nxt_eye_hor_l', 'nxt_eye_ver_l', 'nxt_eye_hor_r', 'nxt_eye_ver_r',\n",
    "#     'LDz', 'RDz', 'LDx', 'RDx',\n",
    "#     'gaze_mky_view_x', 'gaze_mky_view_y', 'gaze_mky_view_angle',\n",
    "#     'cur_opt_arc_dheading', 'cur_ff_distance',\n",
    "#     'cur_ff_rel_x', 'cur_ff_rel_y', 'nxt_ff_rel_x', 'nxt_ff_rel_y', 'nxt_ff_distance',\n",
    "#     'num_ff_visible', 'num_ff_in_memory',\n",
    "#     'cur_ff_distance_at_ref', 'cur_ff_angle_boundary_at_ref', 'nxt_ff_distance_at_ref',\n",
    "#     'speed', 'ang_speed', 'accel', 'ang_accel',\n",
    "#     'monkey_speeddummy', 'curv_of_traj',\n",
    "#     'angle_from_cur_ff_to_nxt_ff',\n",
    "#     'time_since_last_capture', 'bin_mid_time_rel_to_event',\n",
    "#     'time', 'target_index',\n",
    "#     'cur_vis', 'nxt_vis', 'nxt_in_memory', 'any_ff_visible',\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "bin_width = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0316\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural.pn_decoding.interactions.band_conditioned.specified_pairs import CONTINUOUS_INTERACTIONS, DISCRETE_INTERACTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural.pn_decoding.interactions import add_interactions, discrete_decoders, interaction_decoding\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural.pn_decoding.interactions.band_conditioned import conditional_decoding_clf, conditional_decoding_reg, conditional_decoding_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path, bin_width=bin_width)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "    pn.planning_data_by_point)\n",
    "pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "\n",
    "flags = {\n",
    "    'use_raw_spike_data_instead': True,\n",
    "    'apply_pca_on_raw_spike_data': False,\n",
    "    'use_lagged_raw_spike_data': False,\n",
    "}\n",
    "pn.get_concat_data_for_regression(**flags) \n",
    "df = pn.concat_behav_trials.copy()\n",
    "df, added_cols = pn_decoding_utils.prep_behav(df)\n",
    "df = add_interactions.add_behavior_bands(df)\n",
    "\n",
    "\n",
    "key_features2 = (['cur_ff_distance', 'log1p_cur_ff_distance', 'speed', 'accel', 'time_since_last_capture'] + added_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
