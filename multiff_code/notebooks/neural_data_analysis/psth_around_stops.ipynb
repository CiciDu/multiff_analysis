{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.around_stops import psth_around_stops, stop_analysis_utils\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0316\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0321\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0329\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0403\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "# pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "#     pn.planning_data_by_point)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "if not hasattr(pn, 'spikes_df'):\n",
    "    pn.retrieve_or_make_monkey_data()\n",
    "    pn.spikes_df = neural_data_processing.make_spikes_df(pn.raw_data_folder_path, pn.ff_caught_T_sorted,\n",
    "                                                            sampling_rate=pn.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Get misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stops_based_on_distance_to_ff_capture(filtered_stops_df, monkey_information, ff_caught_T_new, min_cum_distance_to_ff_capture):\n",
    "    # eliminate the stops that are too close to a ff capture (within min_cum_distance_to_ff_capture)\n",
    "\n",
    "    # first find the corresponding point index of each time point in ff_caught_T_new\n",
    "    ff_caught_points_sorted = np.searchsorted(\n",
    "        monkey_information['time'].values, ff_caught_T_new)\n",
    "    ff_caught_points_df = monkey_information.iloc[ff_caught_points_sorted].copy(\n",
    "    )\n",
    "\n",
    "    # for each value in filtered_stops_df's cum_distance column, find the closest cum_distance in ff_caught_points\n",
    "    filtered_stops_df['distance_to_next_ff_capture'] = filtered_stops_df['cum_distance'].apply(\n",
    "        lambda x: np.abs(ff_caught_points_df['cum_distance'].values - x).min())\n",
    "    # then, eliminate the stops that are too close to a capture\n",
    "    filtered_stops_df = filtered_stops_df[filtered_stops_df['distance_to_next_ff_capture']\n",
    "                                          > min_cum_distance_to_ff_capture].copy()\n",
    "\n",
    "    return filtered_stops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.find_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_give_up_after_trying_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "there can be multiple stop_point_index for the same ff_index. This can happen when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_try_a_few_times_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_one_stop_w_ff_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.one_stop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.one_stop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.one_stop_df[['point_index']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.one_stop_w_ff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.one_stop_w_ff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = psth_around_stops.PSTHConfig(\n",
    "    pre_window=1.0,\n",
    "    post_window=1.0,\n",
    "    bin_width=0.02,\n",
    "    smoothing_sigma=0.05,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "an = psth_around_stops.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg)\n",
    "\n",
    "# Per-cluster plots with bands\n",
    "fig1 = an.plot_psth(cluster_idx=None, show_individual=False)\n",
    "\n",
    "# Overlay comparison\n",
    "fig2 = an.plot_comparison(cluster_idx=0)\n",
    "\n",
    "# Stats in early post-stop window\n",
    "stats_ = an.statistical_comparison(time_window=(0.0, 0.5))\n",
    "\n",
    "\n",
    "df = psth_around_stops.export_psth_to_df(an)              # all clusters\n",
    "df_c0 = psth_around_stops.export_psth_to_df(an, [0])      # just the first cluster\n",
    "\n",
    "\n",
    "windows = {\n",
    "    \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "    \"early_dip(0.0–0.3)\": (0.0, 0.3),\n",
    "    \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "}\n",
    "summary = psth_around_stops.compare_windows(an, windows, alpha=0.05)\n",
    "summary.sort_values([\"window\",\"p\"]).head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you already have `analyzer` built and run_full_analysis() done\n",
    "res_pre  = an.statistical_comparison(time_window=(-0.3, 0.0))\n",
    "res_early= an.statistical_comparison(time_window=(0.0, 0.3))\n",
    "res_late = an.statistical_comparison(time_window=(0.3, 0.8))\n",
    "\n",
    "# Access Cluster 0 by its original ID or inspect keys:\n",
    "print(list(res_pre.keys())[:5])     # cluster-id strings (e.g., '0','1','7',...)\n",
    "print(res_pre[str(an.clusters[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True/False per cluster\n",
    "sig_any = summary.groupby(\"cluster\")[\"sig_FDR\"].any()  # index = cluster\n",
    "\n",
    "# align to rows via map\n",
    "mask = summary[\"cluster\"].map(sig_any)                 # boolean per row\n",
    "summary_any = summary[mask]                            # rows whose cluster is sig in ANY window\n",
    "\n",
    "clusters_with_signal = sig_any[sig_any].index          # clusters with any True\n",
    "summary_any = summary[ summary[\"cluster\"].isin(clusters_with_signal) ]\n",
    "\n",
    "summary_any = summary.merge(sig_any.rename(\"sig_any\"), left_on=\"cluster\", right_index=True)\n",
    "summary_any = summary_any[ summary_any[\"sig_any\"] ]\n",
    "\n",
    "summary_any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where sig_FDR is True\n",
    "sig_rows = summary[summary[\"sig_FDR\"]]\n",
    "\n",
    "# plot effect sizes by epoch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=sig_rows, x=\"window\", y=\"cohens_d\", hue=\"cluster\", dodge=True)\n",
    "plt.axhline(0, color=\"k\", lw=1)\n",
    "plt.ylabel(\"Cohen's d (capture − miss)\")\n",
    "plt.title(\"Significant neurons across epochs\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## my method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_stop_duration = 0.02\n",
    "max_stop_duration = 1\n",
    "\n",
    "# get stop_id for pn.closest_stop_to_capture_df\n",
    "pn.closest_stop_to_capture_df['stop_id'] = pn.monkey_information.loc[pn.closest_stop_to_capture_df['point_index'], 'stop_id'].values\n",
    "captures_df = pn.closest_stop_to_capture_df[['cur_ff_index', 'stop_id', 'time', 'point_index', 'stop_time', 'distance_from_ff_to_stop']]\n",
    "\n",
    "stops_df = pn.monkey_information[pn.monkey_information['stop_id'].notna()].copy()\n",
    "\n",
    "# add stop duration\n",
    "stop_stats = stops_df.groupby(\"stop_id\")[\"time\"].agg(\n",
    "    stop_id_start_time=\"min\",\n",
    "    stop_id_end_time=\"max\"\n",
    ")\n",
    "stop_stats[\"stop_id_duration\"] = (\n",
    "    stop_stats[\"stop_id_end_time\"] - stop_stats[\"stop_id_start_time\"]\n",
    ")\n",
    "\n",
    "stops_df = stops_df.merge(stop_stats, on=\"stop_id\", how=\"left\").sort_values('point_index', ascending=True)\n",
    "\n",
    "stops_df = stops_df.groupby('stop_id').first().reset_index()\n",
    "no_capture_stops_df = stops_df[~stops_df['stop_id'].isin(captures_df['stop_id'])].reset_index(drop=True)\n",
    "\n",
    "# filter by min_stop_duration\n",
    "no_capture_stops_df = no_capture_stops_df[no_capture_stops_df['stop_id_duration'] >= min_stop_duration].reset_index(drop=True)\n",
    "\n",
    "# filter by max_stop_duration\n",
    "no_capture_stops_df = no_capture_stops_df[no_capture_stops_df['stop_id_duration'] <= max_stop_duration].reset_index(drop=True)\n",
    "\n",
    "# now, drop the captures where distance_from_ff_to_stop > 25\n",
    "captures_df_cleaned = captures_df[captures_df['distance_from_ff_to_stop'] <= 25].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_match_window = 0.3\n",
    "no_capture_stops_df_filtered = stop_analysis_utils.filter_no_capture_stops_vectorized(no_capture_stops_df, pn.ff_caught_T_new, capture_match_window)\n",
    "no_capture_stops_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## run class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = psth_around_stops.PSTHConfig(\n",
    "    pre_window=1.0,\n",
    "    post_window=1.0,\n",
    "    bin_width=0.02,\n",
    "    smoothing_sigma=0.05,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "\n",
    "an = psth_around_stops.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "                                                 captures_df=captures_df_cleaned,\n",
    "                                                 no_capture_stops_df=no_capture_stops_df_filtered)\n",
    "\n",
    "an.identify_stop_events()\n",
    "\n",
    "# Per-cluster plots with bands\n",
    "fig1 = an.plot_psth(cluster_idx=None, show_individual=False)\n",
    "\n",
    "# Overlay comparison\n",
    "fig2 = an.plot_comparison(cluster_idx=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Stats in early post-stop window\n",
    "stats_ = an.statistical_comparison(time_window=(0.0, 0.5))\n",
    "\n",
    "\n",
    "df = psth_around_stops.export_psth_to_df(an)              # all clusters\n",
    "df_c0 = psth_around_stops.export_psth_to_df(an, [0])      # just the first cluster\n",
    "\n",
    "\n",
    "windows = {\n",
    "    \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "    \"early_dip(0.0–0.3)\": (0.0, 0.3),\n",
    "    \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "}\n",
    "summary = psth_around_stops.compare_windows(an, windows, alpha=0.05)\n",
    "summary.sort_values([\"window\",\"p\"]).head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = psth_around_stops.compare_windows(an, windows, alpha=0.05)\n",
    "summary.sort_values([\"window\",\"p\"]).head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = psth_around_stops.PSTHConfig(\n",
    "    pre_window=1.0,\n",
    "    post_window=1.0,\n",
    "    bin_width=0.02,\n",
    "    smoothing_sigma=0.05,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "# Make sure your monkey_information has the stop_id system\n",
    "from data_wrangling.process_monkey_information import add_more_columns_to_monkey_information\n",
    "\n",
    "# Add stop_id system to your monkey_information\n",
    "pn.monkey_information = add_more_columns_to_monkey_information(pn.monkey_information)\n",
    "\n",
    "# # Now use PSTH analysis - it will use your stop_id system\n",
    "# an = psth_around_stops.PSTHAnalyzer(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "#                                           captures_df=captures_df_cleaned,\n",
    "#                                           no_capture_stops_df=no_capture_stops_df_filtered)\n",
    "\n",
    "\n",
    "an = psth_around_stops.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "                                                 captures_df=captures_df_cleaned,\n",
    "                                                 no_capture_stops_df=no_capture_stops_df_filtered)\n",
    "\n",
    "an.identify_stop_events()\n",
    "\n",
    "# Per-cluster plots with bands\n",
    "fig1 = an.plot_psth(cluster_idx=None, show_individual=False)\n",
    "\n",
    "# Overlay comparison\n",
    "fig2 = an.plot_comparison(cluster_idx=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Stats in early post-stop window\n",
    "stats_ = an.statistical_comparison(time_window=(0.0, 0.5))\n",
    "\n",
    "\n",
    "df = psth_around_stops.export_psth_to_df(an)              # all clusters\n",
    "df_c0 = psth_around_stops.export_psth_to_df(an, [0])      # just the first cluster\n",
    "\n",
    "\n",
    "windows = {\n",
    "    \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "    \"early_dip(0.0–0.3)\": (0.0, 0.3),\n",
    "    \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "}\n",
    "summary = psth_around_stops.compare_windows(an, windows, alpha=0.05)\n",
    "summary.sort_values([\"window\",\"p\"]).head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiff_code/notebooks/neural_data_analysis/selection_comparison_chart.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_data_analysis.selection_comparison_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = create_selection_comparison_chart()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Create and display the comparison table\n",
    "    comparison_df = create_comparison_table()\n",
    "    print(\"\\nDetailed Selection Criteria Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the chart functions\n",
    "from multiff_analysis.selection_comparison_chart import (\n",
    "    create_selection_comparison_chart, create_comparison_table\n",
    ")\n",
    "\n",
    "# Create and display the chart\n",
    "fig = create_selection_comparison_chart()\n",
    "plt.show()\n",
    "\n",
    "# Create and display the comparison table\n",
    "comparison_df = create_comparison_table()\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "from neural_data_analysis.selection_comparison_chart import (\n",
    "    create_selection_comparison_chart, create_comparison_table\n",
    ")\n",
    "\n",
    "# Create and display the visual chart\n",
    "fig = create_selection_comparison_chart()\n",
    "plt.show()\n",
    "\n",
    "# Create and display the comparison table\n",
    "comparison_df = create_comparison_table()\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running your PSTH analysis\n",
    "analyzer = psth_around_stops.PSTHAnalyzer(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg)\n",
    "analyzer.identify_stop_events()\n",
    "\n",
    "# Print detailed diagnostic report\n",
    "analyzer.print_capture_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer._find_stops_from_speed_fallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [9, 18, 19, 27, 30, 33, 36, 45, 61, 64, 72, 87, 109, 125, 126, 131, 141, 199, 201, 204, 209, 227, 238, 264, 287, 317, 322, 336, 357, 361, 372, 376, 395, 406, 414, 416, 430, 432, 447, 454, 459, 465, 475, 494, 509, 514, 544, 547, 550, 572, 577, 585, 588, 599, 629, 631, 651, 657, 675, 680, 699, 700, 702, 712, 715, 741, 743, 757, 760, 768, 776, 784, 798, 837, 840, 845, 847, 853, 856, 874, 876, 888, 896, 901, 915, 919, 956, 961, 963, 967, 969, 975, 983, 986, 987, 1003, 1012, 1024, 1032, 1036, 1045, 1061, 1069, 1106, 1116, 1131, 1153, 1170, 1206, 1212, 1214, 1217, 1231, 1242, 1243, 1247, 1252, 1258, 1259, 1268, 1275, 1277, 1281, 1289, 1291, 1295, 1308, 1336, 1337]\n",
    "pn.closest_stop_to_capture_df[pn.closest_stop_to_capture_df['cur_ff_index'].isin(missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_or_retrieve_closest_stop_to_capture_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.closest_stop_to_capture_df[pn.closest_stop_to_capture_df['distance_from_ff_to_stop']> 26].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(pn.closest_stop_to_capture_df2.loc[pn.closest_stop_to_capture_df2['distance_from_ff_to_stop']> 25, 'distance_from_ff_to_stop'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(pn.closest_stop_to_capture_df['diff_from_caught_time'], bins=100)\n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.closest_stop_to_capture_df2 = nxt_ff_utils.get_closest_stop_to_all_capture_position(pn.ff_caught_T_sorted, pn.monkey_information, pn.ff_real_position_sorted,\n",
    "                                                                                       cur_ff_index_array=np.arange(len(pn.ff_caught_T_sorted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_caught_T_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.closest_stop_to_capture_df['diff_from_caught_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pn.closest_stop_to_capture_df['distance_from_ff_to_stop'] > 30).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.stop_events[analyzer.stop_events['stop_time'] >= 58.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = pn.monkey_information.copy()\n",
    "m_sub = mdf[mdf['whether_new_distinct_stop']].copy()\n",
    "m_sub['dt'] = m_sub['time'].diff()\n",
    "len(m_sub[m_sub['dt'] < 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "133/2838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sub[m_sub['dt'] < 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# 2nd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your monkey_information has the stop_id system\n",
    "from data_wrangling.process_monkey_information import add_more_columns_to_monkey_information\n",
    "\n",
    "# Add stop_id system to your monkey_information\n",
    "pn.monkey_information = add_more_columns_to_monkey_information(pn.monkey_information)\n",
    "\n",
    "# Now use PSTH analysis - it will use your stop_id system\n",
    "analyzer = psth_around_stops.PSTHAnalyzer(pn.spikes_df, monkey_information, pn.ff_caught_T_new, cfg)\n",
    "analyzer.identify_stop_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have the stop_id system, it will automatically fall back\n",
    "analyzer = psth_around_stops.PSTHAnalyzer(pn.spikes_df, monkey_information, pn.ff_caught_T_new, cfg)\n",
    "analyzer.identify_stop_events()  # Uses fallback method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "# More plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Heatmap of effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sig_heatmap(summary: pd.DataFrame, title=\"Significant effects (Cohen's d)\"):\n",
    "    # keep only FDR-significant rows\n",
    "    sig = summary[summary[\"sig_FDR\"]].copy()\n",
    "    if sig.empty:\n",
    "        print(\"No significant results to plot.\")\n",
    "        return\n",
    "\n",
    "    # pivot to clusters × windows, values = d\n",
    "    pivot = sig.pivot_table(index=\"cluster\", columns=\"window\", values=\"cohens_d\", aggfunc=\"mean\")\n",
    "\n",
    "    # optional: sort clusters by strongest absolute effect\n",
    "    order = np.argsort(-pivot.abs().max(axis=1).values)\n",
    "    pivot = pivot.iloc[order]\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, 0.35 * len(pivot))))\n",
    "    im = ax.imshow(pivot.values, aspect=\"auto\", cmap=\"coolwarm\", vmin=-np.nanmax(abs(pivot.values)), vmax=np.nanmax(abs(pivot.values)))\n",
    "    ax.set_xticks(range(pivot.shape[1])); ax.set_xticklabels(pivot.columns, rotation=30, ha=\"right\")\n",
    "    ax.set_yticks(range(pivot.shape[0])); ax.set_yticklabels(pivot.index)\n",
    "    ax.set_title(title)\n",
    "    cbar = plt.colorbar(im, ax=ax); cbar.set_label(\"Cohen's d (capture − miss)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# usage\n",
    "plot_sig_heatmap(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## Bar chart of significant effects per epoch (one bar per cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sig_bars(summary: pd.DataFrame, epoch: str):\n",
    "    g = summary[(summary[\"window\"] == epoch) & (summary[\"sig_FDR\"])].copy()\n",
    "    if g.empty:\n",
    "        print(f\"No significant clusters for {epoch}.\"); return\n",
    "    g = g.sort_values(\"cohens_d\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(3, 0.35 * len(g))))\n",
    "    ax.barh(g[\"cluster\"], g[\"cohens_d\"])\n",
    "    ax.axvline(0, color=\"k\", lw=1, alpha=0.5)\n",
    "    ax.set_xlabel(\"Cohen's d (capture − miss)\")\n",
    "    ax.set_ylabel(\"Cluster\")\n",
    "    ax.set_title(f\"Significant clusters in {epoch}\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# usage\n",
    "plot_sig_bars(summary, \"pre_bump(-0.3–0.0)\")\n",
    "plot_sig_bars(summary, \"early_dip(0.0–0.3)\")\n",
    "plot_sig_bars(summary, \"late_rebound(0.3–0.8)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Quickly plot PSTHs for the top significant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_top_psths(analyzer, summary: pd.DataFrame, epoch: str, top_k=6):\n",
    "    # pick significant clusters in the epoch, ranked by |d|\n",
    "    g = summary[(summary[\"window\"] == epoch) & (summary[\"sig_FDR\"])].copy()\n",
    "    if g.empty:\n",
    "        print(f\"No significant clusters for {epoch}.\"); return\n",
    "    g = g.sort_values(\"cohens_d\", key=lambda s: s.abs(), ascending=False).head(top_k)\n",
    "\n",
    "    # map string cluster ids back to analyzer cluster indices\n",
    "    plotted = 0\n",
    "    for cl_str in g[\"cluster\"]:\n",
    "        # analyzer.clusters holds original IDs (numeric or str)\n",
    "        # coerce both sides to string for robust matching\n",
    "        matches = np.where(np.array(list(map(str, analyzer.clusters))) == str(cl_str))[0]\n",
    "        if len(matches) == 0: \n",
    "            continue\n",
    "        ci = int(matches[0])\n",
    "        analyzer.plot_comparison(cluster_idx=ci)  # your existing method\n",
    "        plotted += 1\n",
    "    if plotted == 0:\n",
    "        print(\"Nothing plotted (no matches).\")\n",
    "\n",
    "# usage\n",
    "plot_top_psths(an, summary, \"early_dip(0.0–0.3)\", top_k=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## heatmap of effect sizes (Cohen’s d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sig_heatmap(summary: pd.DataFrame, title=\"Significant effects (Cohen's d)\"):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of Cohen's d for significant cluster×epoch combinations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary : pd.DataFrame\n",
    "        Output from summarize_epochs() / compare_windows().\n",
    "        Must include columns: ['cluster','window','cohens_d','sig_FDR'].\n",
    "    title : str\n",
    "        Title for the plot.\n",
    "    \"\"\"\n",
    "    # keep only significant rows\n",
    "    sig = summary[summary[\"sig_FDR\"]].copy()\n",
    "    if sig.empty:\n",
    "        print(\"No significant results to plot.\")\n",
    "        return\n",
    "\n",
    "    # pivot into matrix: clusters (rows) × windows (columns)\n",
    "    pivot = sig.pivot_table(\n",
    "        index=\"cluster\", columns=\"window\", values=\"cohens_d\", aggfunc=\"mean\"\n",
    "    )\n",
    "\n",
    "    # optional: sort clusters by strongest absolute effect\n",
    "    order = np.argsort(-pivot.abs().max(axis=1).values)\n",
    "    pivot = pivot.iloc[order]\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, 0.4 * len(pivot))))\n",
    "    vmax = np.nanmax(abs(pivot.values))\n",
    "    im = ax.imshow(pivot.values, aspect=\"auto\", cmap=\"coolwarm\",\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "\n",
    "    ax.set_xticks(range(pivot.shape[1]))\n",
    "    ax.set_xticklabels(pivot.columns, rotation=30, ha=\"right\")\n",
    "    ax.set_yticks(range(pivot.shape[0]))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"Cohen's d (capture − miss)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "summary = psth_around_stops.summarize_epochs(an, alpha=0.05)\n",
    "plot_sig_heatmap(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## heatmaps including all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = psth_around_stops.summarize_epochs(an, alpha=0.05)\n",
    "psth_around_stops.plot_effect_heatmap_all(summary)                       # sort by strongest effect\n",
    "# or:\n",
    "psth_around_stops.plot_effect_heatmap_all(summary, order=\"cluster\")      # keep cluster order\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
