{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, get_stops_utils, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0321\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0329\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0403\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0316\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "# pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "#     pn.planning_data_by_point)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "if not hasattr(pn, 'spikes_df'):\n",
    "    pn.retrieve_or_make_monkey_data()\n",
    "    pn.spikes_df = neural_data_processing.make_spikes_df(pn.raw_data_folder_path, pn.ff_caught_T_sorted,\n",
    "                                                            sampling_rate=pn.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# NEXT: try stop end time instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Get captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(get_stops_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example wiring (mirrors your original usage)\n",
    "valid_captures_df, filtered_no_capture_stops_df, stops_with_stats = get_stops_utils.prepare_no_capture_and_captures(\n",
    "    monkey_information=pn.monkey_information,\n",
    "    closest_stop_to_capture_df=pn.closest_stop_to_capture_df,\n",
    "    ff_caught_T_new=pn.ff_caught_T_new,\n",
    "    capture_match_window=0.3,\n",
    "    distance_thresh=25.0,\n",
    "    distance_col=\"distance_from_ff_to_stop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Get misses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##  one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = [\"stop_id\", \"stop_id_duration\", \"stop_id_start_time\", \"stop_id_end_time\"]\n",
    "\n",
    "pn.make_one_stop_w_ff_df()\n",
    "one_stop_miss_df = pn.one_stop_w_ff_df[['first_stop_point_index', 'first_stop_time', 'latest_visible_ff', 'ff_distance', 'min_distance_from_adjacent_stops']].copy()\n",
    "one_stop_miss_df.rename(columns={'first_stop_point_index': 'stop_point_index', 'first_stop_time': 'stop_time'}, inplace=True)\n",
    "one_stop_miss_df[columns_to_add] = pn.monkey_information.loc[one_stop_miss_df['stop_point_index'], columns_to_add].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_try_a_few_times_info()\n",
    "pn.get_give_up_after_trying_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = [\"stop_id\", \"stop_id_duration\", \"stop_id_start_time\", \"stop_id_end_time\"]\n",
    "shared_columns = [\"stop_point_index\", \"stop_time\"] + columns_to_add\n",
    "\n",
    "# --- Build expanded + ordered tables for GUAT / TAFT ---\n",
    "GUAT_expanded = get_stops_utils._expand_trials(pn.GUAT_trials_df, pn.monkey_information)\n",
    "TAFT_expanded = get_stops_utils._expand_trials(pn.TAFT_trials_df, pn.monkey_information)\n",
    "\n",
    "# add stop_id to GUAT_trials_df and TAFT_trials_df\n",
    "GUAT_expanded[columns_to_add] = pn.monkey_information.loc[GUAT_expanded['stop_point_index'], columns_to_add].values\n",
    "TAFT_expanded[columns_to_add] = pn.monkey_information.loc[TAFT_expanded['stop_point_index'], columns_to_add].values\n",
    "\n",
    "\n",
    "GUAT = get_stops_utils._add_cluster_ordering(GUAT_expanded)\n",
    "TAFT = get_stops_utils._add_cluster_ordering(TAFT_expanded)\n",
    "\n",
    "# --- Per-cluster slices (consistent, vectorized) ---\n",
    "# First stop in each cluster\n",
    "GUAT_first = GUAT[GUAT[\"is_first\"]].reset_index(drop=True)\n",
    "TAFT_first = TAFT[TAFT[\"is_first\"]].reset_index(drop=True)\n",
    "\n",
    "# Last stop in each cluster\n",
    "giveup_GUAT_last = GUAT[GUAT[\"is_last\"]].reset_index(drop=True)\n",
    "capture_TAFT_last = TAFT[TAFT[\"is_last\"]].reset_index(drop=True)\n",
    "\n",
    "# Middle stops (exclude first and last)\n",
    "GUAT_middle = GUAT[GUAT[\"is_middle\"]].reset_index(drop=True)\n",
    "TAFT_middle = TAFT[TAFT[\"is_middle\"]].reset_index(drop=True)\n",
    "\n",
    "# “First several” = all but the last stop in each cluster\n",
    "persist_GUAT_nonfinal = GUAT[GUAT[\"order_in_cluster\"] < GUAT[\"cluster_size\"] - 1].reset_index(drop=True)\n",
    "persist_TAFT_nonfinal = TAFT[TAFT[\"order_in_cluster\"] < TAFT[\"cluster_size\"] - 1].reset_index(drop=True)\n",
    "\n",
    "# Combine the “first several” from both, keep only columns you care about, then sort by index\n",
    "both_nonfinal = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            persist_GUAT_nonfinal[shared_columns],\n",
    "            persist_TAFT_nonfinal[shared_columns],\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    .sort_values(\"stop_point_index\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "persist_both_first = pd.concat([GUAT_first[shared_columns], \n",
    "                         TAFT_first[shared_columns]])\n",
    "\n",
    "both_middle = pd.concat([GUAT_middle[shared_columns], \n",
    "                         TAFT_middle[shared_columns]])\n",
    "\n",
    "# Optional: if you also want “last several” (all but the first), it’s symmetrical:\n",
    "# giveup_GUAT_last_several = GUAT[GUAT[\"order_in_cluster\"] > 0].reset_index(drop=True)\n",
    "# capture_TAFT_last_several = TAFT[TAFT[\"order_in_cluster\"] > 0].reset_index(drop=True)\n",
    "\n",
    "giveup_GUAT_last_plus_single_miss = pd.concat([giveup_GUAT_last[shared_columns], \n",
    "                                         one_stop_miss_df[shared_columns]])\n",
    "\n",
    "all_misses = pd.concat([one_stop_miss_df[shared_columns], \n",
    "                                         GUAT_expanded[shared_columns],\n",
    "                                         persist_TAFT_nonfinal[shared_columns]\n",
    "                                         ])\n",
    "\n",
    "all_first_misses = pd.concat(\n",
    "    [one_stop_miss_df[shared_columns], GUAT_first[shared_columns], TAFT_first[shared_columns]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# captures not in TAFT last (assuming capture_TAFT_last is a subset of captures)\n",
    "captures_minus_TAFT_last = compare_events.diff_by(valid_captures_df, capture_TAFT_last, key='stop_id')\n",
    "\n",
    "# non-captures excluding those flagged as 'all_misses'\n",
    "non_captures_minus_all_misses = compare_events.diff_by(filtered_no_capture_stops_df, all_misses, key='stop_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# ===COMPARE EVENTS==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- dataset registry (canonical) ----------\n",
    "datasets_raw = {\n",
    "    'captures': valid_captures_df.copy(),\n",
    "    'no_capture': filtered_no_capture_stops_df.copy(),\n",
    "    'persist_nonfinal': both_nonfinal.copy(),\n",
    "    'persist_middle': both_middle.copy(),\n",
    "    'giveup_GUAT_last': giveup_GUAT_last.copy(),\n",
    "    'capture_TAFT_last': capture_TAFT_last.copy(),\n",
    "    'giveup_single_miss': one_stop_miss_df.copy(),\n",
    "    'persist_both_first': persist_both_first.copy(),\n",
    "    'persist_GUAT_nonfinal': persist_GUAT_nonfinal.copy(),\n",
    "    'persist_TAFT_nonfinal': persist_TAFT_nonfinal.copy(),\n",
    "    'giveup_GUAT_last_plus_single_miss': giveup_GUAT_last_plus_single_miss.copy(),\n",
    "    'captures_minus_TAFT_last': captures_minus_TAFT_last.copy(),\n",
    "    'all_misses': all_misses.copy(),\n",
    "    'non_captures_minus_all_misses': non_captures_minus_all_misses.copy(),\n",
    "    'all_first_misses': all_first_misses.copy(),\n",
    "}\n",
    "\n",
    "# normalize schema + dedupe within each dataset\n",
    "datasets = {k: compare_events.dedupe_within(compare_events.ensure_event_schema(v)) for k, v in datasets_raw.items()}\n",
    "\n",
    "comparisons = compare_events.build_comparisons([\n",
    "    {'a': 'captures', 'b': 'no_capture', 'key': 'captures_vs_no_capture'},\n",
    "\n",
    "    {'a': 'persist_nonfinal', 'b': 'giveup_GUAT_last'},\n",
    "    {'a': 'persist_middle',  'b': 'giveup_GUAT_last'},\n",
    "\n",
    "    {'a': 'persist_nonfinal', 'b': 'giveup_GUAT_last_plus_single_miss'},\n",
    "    {'a': 'persist_middle',   'b': 'giveup_GUAT_last_plus_single_miss'},\n",
    "\n",
    "    {'a': 'giveup_single_miss', 'b': 'giveup_GUAT_last'},\n",
    "    {'a': 'giveup_single_miss', 'b': 'persist_both_first'},\n",
    "\n",
    "    {'a': 'persist_GUAT_nonfinal', 'b': 'persist_TAFT_nonfinal'},\n",
    "\n",
    "    {'a': 'giveup_GUAT_last', 'b': 'capture_TAFT_last'},\n",
    "\n",
    "    {'a': 'captures_minus_TAFT_last', 'b': 'capture_TAFT_last'},\n",
    "\n",
    "    {'a': 'captures', 'b': 'all_misses'},\n",
    "\n",
    "    {'a': 'non_captures_minus_all_misses', 'b': 'all_misses'},\n",
    "    {'a': 'non_captures_minus_all_misses', 'b': 'all_first_misses'},\n",
    "])\n",
    "\n",
    "\n",
    "compare_events.validate(datasets, comparisons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(compare_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = core_stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.05,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "runs = compare_events.run_all_comparisons(\n",
    "    comparisons, datasets, pn.spikes_df, pn.monkey_information, cfg,\n",
    "    align_by_stop_end=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture vs no capture\n",
    "event_a_df=valid_captures_df\n",
    "event_b_df=filtered_no_capture_stops_df\n",
    "\n",
    "event_a_label = 'valid_captures_df'\n",
    "event_b_label = 'filtered_no_capture_stops_df'\n",
    "\n",
    "\n",
    "# persist vs give up\n",
    "event_a_df=both_nonfinal\n",
    "event_b_df=giveup_GUAT_last\n",
    "\n",
    "event_a_label = 'both_nonfinal'\n",
    "event_b_label = 'giveup_GUAT_last'\n",
    "\n",
    "\n",
    "# persist (shrunken) vs give up\n",
    "event_a_df=both_middle\n",
    "event_b_df=giveup_GUAT_last\n",
    "\n",
    "event_a_label = 'both_middle'\n",
    "event_b_label = 'giveup_GUAT_last'\n",
    "\n",
    "\n",
    "# persist vs give up (expanded)\n",
    "event_a_df=both_nonfinal\n",
    "event_b_df=giveup_GUAT_last_plus_single_miss\n",
    "\n",
    "event_a_label = 'both_nonfinal'\n",
    "event_b_label = 'giveup_GUAT_last_plus_single_miss'\n",
    "\n",
    "\n",
    "# persist (shrunken) vs give up (expanded)\n",
    "event_a_df=both_nonfinal\n",
    "event_b_df=giveup_GUAT_last_plus_single_miss\n",
    "\n",
    "event_a_label = 'both_nonfinal'\n",
    "event_b_label = 'giveup_GUAT_last_plus_single_miss'\n",
    "\n",
    "# give up in single miss and give up in GUAT\n",
    "event_a_df=one_stop_miss_df\n",
    "event_b_df=giveup_GUAT_last\n",
    "\n",
    "event_a_label = 'one_stop_miss_df'\n",
    "event_b_label = 'giveup_GUAT_last'\n",
    "\n",
    "\n",
    "# give up in single miss and not give up in persist_both_first\n",
    "event_a_df=one_stop_miss_df\n",
    "event_b_df=persist_both_first\n",
    "\n",
    "event_a_label = 'one_stop_miss_df'\n",
    "event_b_label = 'persist_both_first'\n",
    "\n",
    "# GUAT first several vs TAFT first several\n",
    "event_a_df=persist_GUAT_nonfinal\n",
    "event_b_df=persist_TAFT_nonfinal\n",
    "\n",
    "event_a_label = 'persist_GUAT_nonfinal'\n",
    "event_b_label = 'persist_TAFT_nonfinal'\n",
    "\n",
    "\n",
    "# GUAT last vs TAFT last\n",
    "event_a_df=giveup_GUAT_last\n",
    "event_b_df=capture_TAFT_last\n",
    "\n",
    "event_a_label = 'giveup_GUAT_last'\n",
    "event_b_label = 'capture_TAFT_last'\n",
    "\n",
    "# capture (not in TAFT) vs TAFT last\n",
    "valid_captures_df_subset = valid_captures_df[~valid_captures_df['stop_id'].isin(capture_TAFT_last['stop_id'])].copy()\n",
    "\n",
    "event_a_df=valid_captures_df_subset\n",
    "event_b_df=capture_TAFT_last\n",
    "\n",
    "event_a_label = 'valid_captures_df_subset'\n",
    "event_b_label = 'capture_TAFT_last'\n",
    "\n",
    "\n",
    "# all captures vs all misses\n",
    "event_a_df=valid_captures_df\n",
    "event_b_df=all_misses\n",
    "\n",
    "event_a_label = 'valid_captures_df'\n",
    "event_b_label = 'all_misses'\n",
    "\n",
    "\n",
    "# all misses vs non-capture stops\n",
    "non_captures_minus_all_misses = filtered_no_capture_stops_df[~filtered_no_capture_stops_df['stop_id'].isin(all_misses['stop_id'])].copy()\n",
    "\n",
    "event_a_df=non_captures_minus_all_misses\n",
    "event_b_df=all_misses\n",
    "\n",
    "event_a_label = 'non_captures_minus_all_misses'\n",
    "event_b_label = 'all_misses'\n",
    "\n",
    "\n",
    "# all first misses vs non-capture stops\n",
    "non_captures_minus_all_misses = filtered_no_capture_stops_df[~filtered_no_capture_stops_df['stop_id'].isin(all_misses['stop_id'])].copy()\n",
    "\n",
    "all_first_misses = pd.concat([one_stop_miss_df[shared_columns], \n",
    "                                         GUAT_first[shared_columns],\n",
    "                                         TAFT_first[shared_columns]\n",
    "                                         ])\n",
    "\n",
    "\n",
    "event_a_df=non_captures_minus_all_misses\n",
    "event_b_df=all_first_misses\n",
    "\n",
    "event_a_label = 'non_captures_minus_all_misses'\n",
    "event_b_label = 'all_first_misses'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# run class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture vs no capture\n",
    "event_a_df=valid_captures_df\n",
    "event_b_df=filtered_no_capture_stops_df\n",
    "\n",
    "event_a_label = 'valid_captures_df'\n",
    "event_b_label = 'filtered_no_capture_stops_df'\n",
    "\n",
    "print(f'{event_a_label} vs {event_b_label}')\n",
    "print('=====================================================')\n",
    "\n",
    "\n",
    "event_a_df['stop_time'] = event_a_df['stop_id_start_time']\n",
    "event_b_df['stop_time'] = event_b_df['stop_id_start_time']\n",
    "\n",
    "\n",
    "cfg = core_stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.05,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "an = core_stops_psth.create_psth_around_stops(pn.spikes_df, pn.monkey_information,\n",
    "                                                 event_a_df=event_a_df,\n",
    "                                                 event_b_df=event_b_df,\n",
    "                                                 event_a_label=event_a_label,\n",
    "                                                 event_b_label=event_b_label,\n",
    "                                                 config=cfg,\n",
    "                                                 )\n",
    "\n",
    "fig2 = an.plot_comparison(cluster_idx=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "windows = {\n",
    "    \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "    \"early_dip(0.0–0.3)\": (0.0, 0.3),\n",
    "    \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "}\n",
    "    \n",
    "summary = psth_postprocessing.compare_windows(an, windows, alpha=0.05)\n",
    "summary_sub = summary.loc[summary['sig_FDR'], ['cluster', 'window', 'p', 'cohens_d', 'sig_FDR']].copy()\n",
    "summary_sub = summary_sub.sort_values([\"sig_FDR\", \"window\", \"p\"], ascending=[False, True, True])\n",
    "print(summary_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "windows = {\n",
    "    \"pre_baseline(-0.5–-0.2)\": (-0.5, -0.2),\n",
    "    \"pre_bump(-0.2–-0.1)\": (-0.2, -0.1),\n",
    "    \"pre_bump2(-0.1–0.0)\": (-0.1, 0.0),\n",
    "    # \"pre_bump3(-0.2--0.0)\": (-0.2, -0.0),\n",
    "    \"early_dip(0.0–0.1)\": (0.0, 0.1),\n",
    "    \"early_rebound(0.1–0.2)\": (0.1, 0.2),\n",
    "    \"early_rebound2(0.2–0.3)\": (0.2, 0.3),\n",
    "    \"early_rebound3(0.3–0.4)\": (0.3, 0.4),\n",
    "    \"early_rebound4(0.4–0.5)\": (0.4, 0.5),\n",
    "    \"late_rebound(0.5–0.8)\": (0.5, 0.8),\n",
    "    \"post_tail(0.8–1.2)\": (0.8, 1.2),\n",
    "}\n",
    "\n",
    "summary = psth_postprocessing.compare_windows(an, windows, alpha=0.05)\n",
    "summary_sub = summary[summary['sig_FDR']].copy()\n",
    "df = summary_sub.sort_values([\"sig_FDR\", \"window\", \"p\"], ascending=[False, True, True])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values([\"sig_FDR\", \"p\"], ascending=[False, True]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Other analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## AUC: just pre or post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Iterable, Sequence, Tuple, Optional\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- core: build X,y with optional unit selection ---\n",
    "def build_Xy(an, t0: float, t1: float, units: Optional[Sequence] = None, standardize: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    seg = an.psth_data['segments']; time = an.psth_data['psth']['time_axis']; bw = an.config.bin_width\n",
    "    i0 = int(np.searchsorted(time, t0, side='left')); i1 = int(np.searchsorted(time, t1, side='right'))\n",
    "    Xa = seg['event_a'][:, i0:i1, :].mean(axis=1) / bw   # nA x U\n",
    "    Xb = seg['event_b'][:, i0:i1, :].mean(axis=1) / bw   # nB x U\n",
    "    X = np.vstack([Xa, Xb])                              # (nA+nB) x U\n",
    "    y = np.r_[np.ones(len(Xa), int), np.zeros(len(Xb), int)]\n",
    "\n",
    "    # select units if requested\n",
    "    unit_ids = an.clusters  # original cluster labels\n",
    "    if units is not None:\n",
    "        # 'units' can be indices into columns OR original cluster labels\n",
    "        if all(isinstance(u, (int, np.integer)) for u in units) and set(units).issubset(set(range(X.shape[1]))):\n",
    "            cols = np.array(list(units), int)\n",
    "        else:\n",
    "            # map labels -> column indices\n",
    "            col_map = {lab: j for j, lab in enumerate(unit_ids)}\n",
    "            cols = np.array([col_map[u] for u in units], int)\n",
    "        X = X[:, cols]\n",
    "        unit_ids = unit_ids[cols]\n",
    "\n",
    "    if standardize:\n",
    "        sc = StandardScaler().fit(X)\n",
    "        X = sc.transform(X)\n",
    "\n",
    "    return X, y, unit_ids\n",
    "\n",
    "def decode_auc_cv(X: np.ndarray, y: np.ndarray, k: int = 5, seed: int = 0, C: float = 1.0, class_weight=None) -> Tuple[float, float]:\n",
    "    cv = StratifiedKFold(k, shuffle=True, random_state=seed)\n",
    "    aucs = []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        clf = LogisticRegression(max_iter=1000, C=C, class_weight=class_weight, solver='lbfgs')\n",
    "        clf.fit(X[tr], y[tr])\n",
    "        p = clf.predict_proba(X[te])[:, 1]\n",
    "        aucs.append(roc_auc_score(y[te], p))\n",
    "    aucs = np.asarray(aucs, float)\n",
    "    return float(np.mean(aucs)), float(np.std(aucs))\n",
    "\n",
    "# --- 1) per-unit (single-cluster) AUCs ---\n",
    "def per_unit_auc_df(an, window=(-0.3, 0.0), k=5, seed=0, standardize=False, include_sd=True) -> pd.DataFrame:\n",
    "    X, y, unit_ids = build_Xy(an, *window, standardize=standardize)\n",
    "    rows = []\n",
    "    for j, cid in enumerate(unit_ids):\n",
    "        m, s = decode_auc_cv(X[:, [j]], y, k=k, seed=seed)\n",
    "        rows.append({'cluster': int(cid) if str(cid).isdigit() else cid, 'auc': float(m), 'sd_cv': float(s)})\n",
    "    df = pd.DataFrame(rows).sort_values('auc', ascending=False).reset_index(drop=True)\n",
    "    df['delta'] = df['auc'] - 0.5\n",
    "    df['rank'] = np.arange(1, len(df) + 1)\n",
    "    cols = ['rank', 'cluster', 'auc', 'delta'] + (['sd_cv'] if include_sd else [])\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "# --- 2) subset decoding (choose a list of units) ---\n",
    "def subset_auc(an, units: Sequence, window=(-0.3, 0.0), k=5, seed=0, standardize=True) -> Tuple[float, float]:\n",
    "    X, y, _ = build_Xy(an, *window, units=units, standardize=standardize)\n",
    "    return decode_auc_cv(X, y, k=k, seed=seed)\n",
    "\n",
    "# --- 3) neuron-drop / top-K curve ---\n",
    "def neuron_drop_curve(an, window=(-0.3, 0.0), k=5, seed=0, rank_by='univariate', standardize=True, max_k=None):\n",
    "    \"\"\"\n",
    "    rank_by: 'univariate' (per-unit AUC vs labels), or 'coef' (coef magnitude from a full model)\n",
    "    returns dict with 'order' (cluster IDs in rank order) and 'curve' (list of AUCs for K=1..max_k)\n",
    "    \"\"\"\n",
    "    X, y, unit_ids = build_Xy(an, *window, standardize=standardize)\n",
    "\n",
    "    # ranking\n",
    "    if rank_by == 'univariate':\n",
    "        scores = np.full(X.shape[1], -np.inf)\n",
    "        for j in range(X.shape[1]):\n",
    "            try:\n",
    "                scores[j] = roc_auc_score(y, X[:, j])\n",
    "                scores[j] = max(scores[j], 1 - scores[j])  # make it >= 0.5\n",
    "            except Exception:\n",
    "                scores[j] = 0.5\n",
    "        order = np.argsort(-scores)  # high to low\n",
    "    elif rank_by == 'coef':\n",
    "        clf = LogisticRegression(max_iter=1000, C=1.0, solver='lbfgs').fit(X, y)\n",
    "        scores = np.abs(clf.coef_.ravel())\n",
    "        order = np.argsort(-scores)\n",
    "    else:\n",
    "        raise ValueError(\"rank_by must be 'univariate' or 'coef'\")\n",
    "\n",
    "    order_ids = unit_ids[order]\n",
    "    Kmax = X.shape[1] if max_k is None else min(max_k, X.shape[1])\n",
    "    curve = []\n",
    "    for K in range(1, Kmax + 1):\n",
    "        cols = order[:K]\n",
    "        auc_mean, _ = decode_auc_cv(X[:, cols], y, k=k, seed=seed)\n",
    "        curve.append(auc_mean)\n",
    "    return {'order_ids': order_ids, 'curve': np.array(curve), 'rank_scores': scores[order]}\n",
    "\n",
    "# --- 4) convenience wrappers you can call directly ---\n",
    "def auc_all_units(an, window=(-0.3, 0.0), **cvkw):\n",
    "    X, y, _ = build_Xy(an, *window, standardize=True)\n",
    "    return decode_auc_cv(X, y, **cvkw)\n",
    "\n",
    "def auc_single_unit(an, unit, window=(-0.3, 0.0), **cvkw):\n",
    "    return subset_auc(an, units=[unit], window=window, **cvkw)\n",
    "\n",
    "def auc_topK_units(an, K, window=(-0.3, 0.0), rank_by='univariate', **cvkw):\n",
    "    rank = neuron_drop_curve(an, window=window, rank_by=rank_by, **{k:v for k,v in cvkw.items() if k in ('k','seed','standardize')})\n",
    "    return float(rank['curve'][K-1]), rank\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_unit_auc_bar(df, top=None):\n",
    "    dd = df.copy()\n",
    "    if top is not None:\n",
    "        dd = dd.head(top)\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, 0.35*len(dd))))\n",
    "    ax.barh(dd['cluster'].astype(str), dd['auc'])\n",
    "    ax.axvline(0.5, linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('AUC')\n",
    "    ax.set_ylabel('Cluster')\n",
    "    ax.set_title('Single-unit AUC')\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All clusters\n",
    "\n",
    "m, s = auc_all_units(an, window=(-0.3, 0.0), k=5, seed=0)\n",
    "print('pre-stop AUC (all units):', round(m,3))\n",
    "\n",
    "m, s = auc_all_units(an, window=(0.05, 0.35), k=5, seed=0)\n",
    "print('post-stop AUC (all units):', round(m,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per cluster\n",
    "df = per_unit_auc_df(an, window=(-0.3, 0.0), k=5, seed=0, standardize=True)\n",
    "print(df.to_string(index=False, formatters={'auc': '{:.3f}'.format,\n",
    "                                            'delta': '{:+.3f}'.format,\n",
    "                                            'sd_cv': '{:.3f}'.format}))\n",
    "\n",
    "# plot all units\n",
    "fig, ax = plot_unit_auc_bar(df, top=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k curve\n",
    "\n",
    "res = neuron_drop_curve(an, window=(-0.3, 0.0), k=5, seed=0, rank_by='univariate', standardize=True)\n",
    "print('best K=5 AUC:', float(res['curve'][4]))\n",
    "# res['order_ids'] gives the unit order; res['curve'] is AUC for K=1..Kmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick subset\n",
    "m, s = subset_auc(an, units=[2, 5], window=(-0.3, 0.0), k=5, seed=0)  # if your labels are 12,27,44\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## AUC: both pre and post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- reuseable bits ---\n",
    "\n",
    "def build_Xy(an, t0, t1, standardize=False):\n",
    "    seg = an.psth_data['segments']; time = an.psth_data['psth']['time_axis']; bw = an.config.bin_width\n",
    "    i0 = int(np.searchsorted(time, t0, side='left')); i1 = int(np.searchsorted(time, t1, side='right'))\n",
    "    Xa = seg['event_a'][:, i0:i1, :].mean(axis=1) / bw\n",
    "    Xb = seg['event_b'][:, i0:i1, :].mean(axis=1) / bw\n",
    "    X = np.vstack([Xa, Xb])\n",
    "    y = np.r_[np.ones(len(Xa), int), np.zeros(len(Xb), int)]\n",
    "    if standardize:\n",
    "        sc = StandardScaler().fit(X)\n",
    "        X = sc.transform(X)\n",
    "    return X, y\n",
    "\n",
    "def decode_auc_cv(X, y, k=5, seed=0, C=1.0, class_weight=None):\n",
    "    cv = StratifiedKFold(k, shuffle=True, random_state=seed)\n",
    "    aucs = []\n",
    "    for tr, te in cv.split(X, y):\n",
    "        clf = LogisticRegression(max_iter=1000, C=C, class_weight=class_weight, solver='lbfgs')\n",
    "        clf.fit(X[tr], y[tr])\n",
    "        p = clf.predict_proba(X[te])[:, 1]\n",
    "        aucs.append(roc_auc_score(y[te], p))\n",
    "    return float(np.mean(aucs)), float(np.std(aucs))\n",
    "\n",
    "def per_unit_auc_df(an, window=(-0.3, 0.0), k=5, seed=0, standardize=True) -> pd.DataFrame:\n",
    "    seg = an.psth_data['segments']; time = an.psth_data['psth']['time_axis']; bw = an.config.bin_width\n",
    "    i0 = int(np.searchsorted(time, window[0], side='left')); i1 = int(np.searchsorted(time, window[1], side='right'))\n",
    "    Xa = seg['event_a'][:, i0:i1, :].mean(axis=1) / bw\n",
    "    Xb = seg['event_b'][:, i0:i1, :].mean(axis=1) / bw\n",
    "    X = np.vstack([Xa, Xb]); y = np.r_[np.ones(len(Xa), int), np.zeros(len(Xb), int)]\n",
    "    unit_ids = an.clusters\n",
    "\n",
    "    if standardize:\n",
    "        sc = StandardScaler().fit(X)\n",
    "        X = sc.transform(X)\n",
    "\n",
    "    rows = []\n",
    "    for j, cid in enumerate(unit_ids):\n",
    "        m, s = decode_auc_cv(X[:, [j]], y, k=k, seed=seed)\n",
    "        rows.append({'cluster': int(cid) if str(cid).isdigit() else cid, 'auc': float(m), 'sd_cv': float(s)})\n",
    "    df = pd.DataFrame(rows).sort_values('auc', ascending=False).reset_index(drop=True)\n",
    "    df['delta'] = df['auc'] - 0.5\n",
    "    df['rank'] = np.arange(1, len(df) + 1)\n",
    "    return df[['rank', 'cluster', 'auc', 'delta', 'sd_cv']]\n",
    "\n",
    "# --- pre+post in one go ---\n",
    "\n",
    "def per_unit_auc_pre_post(an,\n",
    "                          pre=(-0.3, 0.0),\n",
    "                          post=(0.05, 0.35),\n",
    "                          k=5, seed=0, standardize=True):\n",
    "    df_pre  = per_unit_auc_df(an, window=pre,  k=k, seed=seed, standardize=standardize).rename(columns={'auc':'auc_pre','delta':'delta_pre','sd_cv':'sd_pre'})\n",
    "    df_post = per_unit_auc_df(an, window=post, k=k, seed=seed, standardize=standardize).rename(columns={'auc':'auc_post','delta':'delta_post','sd_cv':'sd_post'})\n",
    "    wide = pd.merge(df_pre[['cluster','auc_pre','sd_pre']], df_post[['cluster','auc_post','sd_post']],\n",
    "                    on='cluster', how='inner')\n",
    "    wide['delta'] = wide['auc_post'] - wide['auc_pre']\n",
    "    wide = wide.sort_values(['auc_post'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    long = pd.concat([\n",
    "        pd.DataFrame({'cluster': df_pre['cluster'],  'window': 'pre',  'auc': df_pre['auc_pre']  if 'auc_pre' in df_pre else df_pre['auc']}),\n",
    "        pd.DataFrame({'cluster': df_post['cluster'], 'window': 'post', 'auc': df_post['auc_post'] if 'auc_post' in df_post else df_post['auc']}),\n",
    "    ], ignore_index=True)\n",
    "    return {'wide': wide, 'pre': df_pre, 'post': df_post, 'long': long}\n",
    "\n",
    "# --- plots ---\n",
    "\n",
    "def plot_unit_auc_bar(df, top=None, title='Single-unit AUC', xlabel='AUC'):\n",
    "    dd = df.copy()\n",
    "    if top is not None:\n",
    "        dd = dd.head(top)\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, 0.35*len(dd))))\n",
    "    ax.barh(dd['cluster'].astype(str), dd['auc'])\n",
    "    ax.axvline(0.5, linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Cluster')\n",
    "    ax.set_title(title)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_unit_auc_pre_post_bars(res, top=None, title='Single-unit AUC: pre vs post'):\n",
    "    pre = res['pre'][['cluster','auc_pre']].rename(columns={'auc_pre':'auc'})\n",
    "    post = res['post'][['cluster','auc_post']].rename(columns={'auc_post':'auc'})\n",
    "    # common order by max(pre, post)\n",
    "    order = (pd.merge(pre, post, on='cluster', how='inner')\n",
    "               .assign(mx=lambda d: d[['auc_x','auc_y']].max(axis=1))\n",
    "               .sort_values('mx', ascending=False)['cluster'])\n",
    "    pre = pre.set_index('cluster').loc[order].reset_index()\n",
    "    post = post.set_index('cluster').loc[order].reset_index()\n",
    "    if top is not None:\n",
    "        pre = pre.head(top); post = post.head(top)\n",
    "    H = max(3, 0.35*len(pre))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, H), sharey=True)\n",
    "    for ax, df_, lab in zip(axes, [pre, post], ['pre', 'post']):\n",
    "        ax.barh(df_['cluster'].astype(str), df_['auc'])\n",
    "        ax.axvline(0.5, linestyle='--', linewidth=1)\n",
    "        ax.set_xlabel('AUC')\n",
    "        ax.set_title(lab)\n",
    "        ax.invert_yaxis()\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout(rect=[0,0,1,0.95])\n",
    "    return fig, axes\n",
    "\n",
    "def plot_unit_auc_scatter(wide, title='Pre vs Post single-unit AUC'):\n",
    "    fig, ax = plt.subplots(figsize=(5.5,5.5))\n",
    "    ax.scatter(wide['auc_pre'], wide['auc_post'], s=20, alpha=0.8)\n",
    "    ax.plot([0,1],[0,1], linestyle='--', linewidth=1)\n",
    "    ax.axvline(0.5, linestyle=':', linewidth=1); ax.axhline(0.5, linestyle=':', linewidth=1)\n",
    "    ax.set_xlabel('AUC (pre)')\n",
    "    ax.set_ylabel('AUC (post)')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0.3, 1.0); ax.set_ylim(0.3, 1.0)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_auc_delta_bar(wide, top=None, title='Δ AUC (post − pre) by unit'):\n",
    "    dd = wide.copy().sort_values('delta', ascending=False)\n",
    "    if top is not None:\n",
    "        dd = dd.head(top)\n",
    "    H = max(3, 0.35*len(dd))\n",
    "    fig, ax = plt.subplots(figsize=(8, H))\n",
    "    ax.barh(dd['cluster'].astype(str), dd['delta'])\n",
    "    ax.axvline(0.0, linestyle='--', linewidth=1)\n",
    "    ax.set_xlabel('Δ AUC (post − pre)')\n",
    "    ax.set_ylabel('Cluster')\n",
    "    ax.set_title(title)\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute both\n",
    "res = per_unit_auc_pre_post(an, pre=(-0.30, 0.00), post=(0.05, 0.35), k=5, seed=0, standardize=True)\n",
    "\n",
    "# bar charts for each window\n",
    "plot_unit_auc_pre_post_bars(res, top=15)\n",
    "\n",
    "# pre vs post scatter\n",
    "plot_unit_auc_scatter(res['wide'])\n",
    "\n",
    "# delta bar (who improves after the stop?)\n",
    "plot_auc_delta_bar(res['wide'], top=15)\n",
    "\n",
    "# if you only want a post-stop bar:\n",
    "post_only = res['post'].rename(columns={'auc_post':'auc'})\n",
    "plot_unit_auc_bar(post_only, top=15, title='Single-unit AUC (post-stop)', xlabel='AUC (post)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## dPCA (demixed PCA): time vs outcome\n",
    "\n",
    "Note: it’s across all neurons together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming you have dpca_time_outcome(an) defined:\n",
    "rep = dpca_utils.run_dpca_report(an, dpca_fn=dpca_utils.dpca_time_outcome, pre_window=(-0.3, 0.0), post_window=(0.05, 0.35),\n",
    "                      top_k=20, n_perm=1000, seed=0, show=True)\n",
    "\n",
    "print('EVR1 obs:', rep['evr_perm']['evr1_obs'], 'p=', rep['evr_perm']['p_value'])\n",
    "print('R^2 (reconstruction):', rep['reconstruction']['R2'])\n",
    "print('AUC all units pre/post:', rep['auc_all_units'])\n",
    "\n",
    "# top outcome neurons\n",
    "print(rep['top_outcome_units'].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rep['out']\n",
    "\n",
    "# Global vs within-block normalization (helps intuition):\n",
    "evr_global = out['outcome']['explained_var']           # what you plotted\n",
    "evr_within = evr_global / np.sum(evr_global) if np.sum(evr_global)>0 else evr_global\n",
    "print('Outcome EVR (global):', evr_global[:5])\n",
    "print('Outcome EVR (within-outcome):', evr_within[:5])  # sums to 1 over outcome comps\n",
    "\n",
    "# Do the blocks add up? (time + outcome ≤ 1 because interaction/residual isn’t modeled):\n",
    "print('sum(time EVR) + sum(outcome EVR) =',\n",
    "      np.sum(out['time']['explained_var']) + np.sum(out['outcome']['explained_var']))\n",
    "\n",
    "# Is it significant?\n",
    "print('Outcome EVR1:', rep['evr_perm']['evr1_obs'], '  p=', rep['evr_perm']['p_value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Targeted dim-red (‘outcome axis’)\n",
    "\n",
    "Idea. Learn a single weight vector w that best predicts event_a vs event_b from pre-stop activity (linear/ridge). Then project full trajectories onto w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "\n",
    "def outcome_axis(an, train_window=(-0.3, 0.0), alpha=1.0,\n",
    "                 model='ridge', standardize=True, return_trials=True):\n",
    "    \"\"\"\n",
    "    Learn a 1D 'outcome axis' on a training window, then project peri-stop trajectories.\n",
    "\n",
    "    model: 'ridge' (your original) or 'logreg'\n",
    "    standardize: z-score units using training-window data\n",
    "    return_trials: also return per-trial projections (for CIs / AUC)\n",
    "    \"\"\"\n",
    "    if not getattr(an, 'psth_data', None):\n",
    "        an.run_full_analysis()\n",
    "\n",
    "    seg  = an.psth_data['segments']\n",
    "    time = an.psth_data['psth']['time_axis']\n",
    "    bw   = an.config.bin_width\n",
    "    i0   = int(np.searchsorted(time, train_window[0], side='left'))\n",
    "    i1   = int(np.searchsorted(time, train_window[1], side='right'))\n",
    "\n",
    "    # training features (mean rate in window)\n",
    "    Xa = seg['event_a'][:, i0:i1, :].mean(axis=1) / bw   # nA x U\n",
    "    Xb = seg['event_b'][:, i0:i1, :].mean(axis=1) / bw   # nB x U\n",
    "    X  = np.vstack([Xa, Xb])                             # N x U\n",
    "    y  = np.r_[np.ones(len(Xa), int), np.zeros(len(Xb), int)]\n",
    "\n",
    "    scaler = None\n",
    "    if standardize:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "    # fit linear readout\n",
    "    if model == 'ridge':\n",
    "        clf = Ridge(alpha=alpha).fit(X, y)\n",
    "        w = clf.coef_.astype(float)                      # U\n",
    "        b = float(getattr(clf, 'intercept_', 0.0))\n",
    "    elif model == 'logreg':\n",
    "        clf = LogisticRegression(penalty='l2', C=1.0/alpha, solver='lbfgs', max_iter=1000)\n",
    "        clf.fit(X, y)\n",
    "        w = clf.coef_.ravel().astype(float)\n",
    "        b = float(clf.intercept_[0])\n",
    "    else:\n",
    "        raise ValueError(\"model must be 'ridge' or 'logreg'\")\n",
    "\n",
    "    # mean trajectories over time (T x U)\n",
    "    traj_a = (seg['event_a'] / bw).mean(axis=0)   # T x U\n",
    "    traj_b = (seg['event_b'] / bw).mean(axis=0)   # T x U\n",
    "    if standardize:\n",
    "        # apply the same scaler column-wise to trajectories\n",
    "        traj = np.vstack([traj_a, traj_b])        # (2T) x U\n",
    "        traj = scaler.transform(traj)\n",
    "        traj_a, traj_b = traj[:len(time)], traj[len(time):]\n",
    "\n",
    "    # project means (time courses)\n",
    "    proj_a = traj_a @ w + b\n",
    "    proj_b = traj_b @ w + b\n",
    "\n",
    "    out = {\n",
    "        'w': w, 'b': b, 'proj_time': time,\n",
    "        'proj_a': proj_a, 'proj_b': proj_b,\n",
    "        'clusters': an.clusters,\n",
    "        'model': model, 'train_window': train_window,\n",
    "        'standardize': standardize\n",
    "    }\n",
    "\n",
    "    if return_trials:\n",
    "        # per-trial projections across time (for CIs/AUC without refit)\n",
    "        A = seg['event_a'] / bw   # nA x T x U\n",
    "        B = seg['event_b'] / bw   # nB x T x U\n",
    "        if standardize:\n",
    "            # flatten trials×time, transform, then reshape back\n",
    "            AT = A.reshape(-1, A.shape[2])\n",
    "            BT = B.reshape(-1, B.shape[2])\n",
    "            AT = scaler.transform(AT); BT = scaler.transform(BT)\n",
    "            A = AT.reshape(A.shape);  B = BT.reshape(B.shape)\n",
    "        out['scores_a'] = A @ w + b   # nA x T\n",
    "        out['scores_b'] = B @ w + b   # nB x T\n",
    "\n",
    "    return out\n",
    "\n",
    "def cosine_with_dpca(outcome_axis_w, dpca_out):\n",
    "    \"\"\"Cosine similarity between your outcome axis and dPCA outcome comp-1.\"\"\"\n",
    "    w1 = outcome_axis_w.ravel()\n",
    "    w2 = dpca_out['outcome']['components'][0].ravel()\n",
    "    return float((w1 @ w2) / (np.linalg.norm(w1) * np.linalg.norm(w2) + 1e-12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the axis (train on pre-stop), get time courses\n",
    "\n",
    "out = outcome_axis(\n",
    "    an,\n",
    "    train_window=(-0.3, 0.0),   # train the readout here\n",
    "    alpha=1.0,                  # ridge strength (or 1/C if model='logreg')\n",
    "    model='ridge',              # or 'logreg'\n",
    "    standardize=True,\n",
    "    return_trials=True\n",
    ")\n",
    "\n",
    "# quick plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(out['proj_time'], out['proj_a'], label=getattr(an, 'event_a_label', 'event_a'))\n",
    "plt.plot(out['proj_time'], out['proj_b'], label=getattr(an, 'event_b_label', 'event_b'))\n",
    "plt.axvline(0, ls='--', lw=1)\n",
    "plt.xlabel('Time (s)'); plt.ylabel('Projection score'); plt.legend(); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC in any window using the fixed axis (no refit)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def window_auc_from_scores(out, window):\n",
    "    t = out['proj_time']\n",
    "    i0 = int(np.searchsorted(t, window[0], side='left'))\n",
    "    i1 = int(np.searchsorted(t, window[1], side='right'))\n",
    "    sa = out['scores_a'][:, i0:i1].mean(axis=1)   # per-trial scores in window\n",
    "    sb = out['scores_b'][:, i0:i1].mean(axis=1)\n",
    "    y  = np.r_[np.ones(len(sa), int), np.zeros(len(sb), int)]\n",
    "    s  = np.r_[sa, sb]\n",
    "    return roc_auc_score(y, s)\n",
    "\n",
    "auc_pre  = window_auc_from_scores(out, (-0.3, 0.0))\n",
    "auc_post = window_auc_from_scores(out, (0.05, 0.35))\n",
    "print('AUC pre:', round(auc_pre,3), '  AUC post:', round(auc_post,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def outcome_axis(an, train_window=(-0.3, 0.0), alpha=1.0):\n",
    "    if not getattr(an, 'psth_data', None):\n",
    "        an.run_full_analysis()\n",
    "    seg = an.psth_data['segments']\n",
    "    time = an.psth_data['psth']['time_axis']\n",
    "    bw = an.config.bin_width\n",
    "\n",
    "    i0 = int(np.searchsorted(time, train_window[0], side='left'))\n",
    "    i1 = int(np.searchsorted(time, train_window[1], side='right'))\n",
    "    Xa = seg['event_a'][:, i0:i1, :].mean(axis=1) / bw    # nA x U\n",
    "    Xb = seg['event_b'][:, i0:i1, :].mean(axis=1) / bw    # nB x U\n",
    "    X = np.vstack([Xa, Xb])\n",
    "    y = np.r_[np.ones(len(Xa), int), np.zeros(len(Xb), int)]\n",
    "\n",
    "    model = Ridge(alpha=alpha).fit(X, y)\n",
    "    w = model.coef_.astype(float)                         # U\n",
    "\n",
    "    # project mean trajectories over time\n",
    "    traj_a = (seg['event_a'] / bw).mean(axis=0)          # T x U\n",
    "    traj_b = (seg['event_b'] / bw).mean(axis=0)          # T x U\n",
    "    proj_a = traj_a @ w                                   # T\n",
    "    proj_b = traj_b @ w                                   # T\n",
    "    return {'w': w, 'proj_time': an.psth_data['psth']['time_axis'],\n",
    "            'proj_a': proj_a, 'proj_b': proj_b, 'clusters': an.clusters}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: bootstrap CI bands on the time courses\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mean_ci_band(scores, alpha=0.05, iters=1000, seed=0):\n",
    "    # scores: trials x time\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, T = scores.shape\n",
    "    boots = np.empty((iters, T))\n",
    "    for i in range(iters):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        boots[i] = scores[idx].mean(0)\n",
    "    lo, hi = np.percentile(boots, [100*alpha/2, 100*(1-alpha/2)], axis=0)\n",
    "    return scores.mean(0), lo, hi\n",
    "\n",
    "ma, lo_a, hi_a = mean_ci_band(out['scores_a'])\n",
    "mb, lo_b, hi_b = mean_ci_band(out['scores_b'])\n",
    "\n",
    "t = out['proj_time']\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(t, ma, label=getattr(an, 'event_a_label','event_a'))\n",
    "plt.fill_between(t, lo_a, hi_a, alpha=0.2, linewidth=0)\n",
    "plt.plot(t, mb, label=getattr(an, 'event_b_label','event_b'))\n",
    "plt.fill_between(t, lo_b, hi_b, alpha=0.2, linewidth=0)\n",
    "plt.axvline(0, ls='--', lw=1); plt.xlabel('Time (s)'); plt.ylabel('Projection score'); plt.legend(); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to dPCA outcome axis (alignment)\n",
    "dp = dpca_time_outcome(an)\n",
    "cos = cosine_with_dpca(out['w'], dp)   # uses the helper I gave earlier\n",
    "print('cosine(outcome_axis, dPCA outcome comp1) =', round(cos,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect which neurons drive the axis\n",
    "\n",
    "import pandas as pd\n",
    "top = (pd.DataFrame({'cluster': an.clusters, 'w': out['w']})\n",
    "         .assign(abs_w=lambda d: d['w'].abs())\n",
    "         .sort_values('abs_w', ascending=False)\n",
    "         .head(15))\n",
    "print(top.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on a different window or use logistic\n",
    "\n",
    "out = outcome_axis(an, train_window=(0.05, 0.35), model='logreg', alpha=1.0)\n",
    "\n",
    "\n",
    "# quick plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(out['proj_time'], out['proj_a'], label=getattr(an, 'event_a_label', 'event_a'))\n",
    "plt.plot(out['proj_time'], out['proj_b'], label=getattr(an, 'event_b_label', 'event_b'))\n",
    "plt.axvline(0, ls='--', lw=1)\n",
    "plt.xlabel('Time (s)'); plt.ylabel('Projection score'); plt.legend(); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### ridgeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_hedges_g(x, y, eps=1e-3, trim=0.0):\n",
    "    import numpy as np\n",
    "    x, y = np.asarray(x, float), np.asarray(y, float)\n",
    "    if trim > 0:\n",
    "        qx = np.quantile(x, [trim/2, 1-trim/2]); x = np.clip(x, qx[0], qx[1])\n",
    "        qy = np.quantile(y, [trim/2, 1-trim/2]); y = np.clip(y, qy[0], qy[1])\n",
    "    n1, n2 = len(x), len(y)\n",
    "    if n1 < 2 or n2 < 2: return 0.0\n",
    "    s1, s2 = np.var(x, ddof=1), np.var(y, ddof=1)\n",
    "    sp = np.sqrt(((n1-1)*s1 + (n2-1)*s2) / max(n1+n2-2, 1))\n",
    "    sp = max(sp, eps)\n",
    "    d = (np.mean(x) - np.mean(y)) / sp\n",
    "    J = 1 - 3/(4*(n1+n2)-9) if (n1+n2) > 2 else 1.0\n",
    "    return float(J*d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def _kde_safe(x, grid, bw=None):\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size < 3:\n",
    "        return np.zeros_like(grid)\n",
    "    kde = gaussian_kde(x, bw_method=bw)\n",
    "    return kde(grid)\n",
    "\n",
    "def plot_ridge_effect_by_window(summary_df, order='effect', bw=None):\n",
    "    \"\"\"\n",
    "    Ridgeline of Cohen's d distributions across windows (aggregated over clusters).\n",
    "    summary_df: output from summarize_epochs()/compare_windows()\n",
    "                needs ['window','cohens_d'] (and optionally 'sig_FDR').\n",
    "    order: 'effect' (windows ordered by |median d|) or 'as_is' (given order).\n",
    "    \"\"\"\n",
    "    df = summary_df.copy()\n",
    "    df = df[np.isfinite(df['cohens_d'])]\n",
    "    if df.empty:\n",
    "        raise ValueError('no finite effect sizes in summary_df')\n",
    "\n",
    "    # choose window order\n",
    "    windows = list(df['window'].unique())\n",
    "    if order == 'effect':\n",
    "        med = df.groupby('window', as_index=False)['cohens_d'].median()\n",
    "        windows = med.reindex(med['cohens_d'].abs().sort_values(ascending=False).index)['window'].tolist()\n",
    "\n",
    "    # common x-range\n",
    "    lo = np.nanpercentile(df['cohens_d'], 1)\n",
    "    hi = np.nanpercentile(df['cohens_d'], 99)\n",
    "    pad = 0.1 * (hi - lo + 1e-9)\n",
    "    xgrid = np.linspace(lo - pad, hi + pad, 500)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 0.7 * max(3, len(windows))))\n",
    "    offset = 0.0\n",
    "    gap = 1.2\n",
    "\n",
    "    for w in windows:\n",
    "        x = df.loc[df['window'] == w, 'cohens_d'].values\n",
    "        y = _kde_safe(x, xgrid, bw=bw)\n",
    "        if y.max() > 0:\n",
    "            y = y / y.max()  # normalize height\n",
    "        ax.fill_between(xgrid, offset, offset + y, alpha=0.6, linewidth=0)\n",
    "        ax.plot(xgrid, offset + y, linewidth=1.5)\n",
    "        ax.text(xgrid[0], offset + 0.05, str(w), va='bottom', fontsize=9)\n",
    "        offset += gap\n",
    "\n",
    "    ax.axvline(0, linestyle='--', linewidth=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Cohen's d (event_a − event_b)\")\n",
    "    ax.set_title('Effect ridgeline across windows (distribution over clusters)')\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = psth_postprocessing.summarize_epochs(an)   # or compare_windows(...)\n",
    "plot_ridge_effect_by_window(summary)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## volcano plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_volcano(summary_df, p_col='p', effect_col='cohens_d', by_window=True, sig_col='sig_FDR'):\n",
    "    \"\"\"\n",
    "    Volcano: effect size vs -log10(p).\n",
    "    summary_df: from summarize_epochs()/compare_windows()/sliding_window_stats\n",
    "    p_col: 'p' or 'p_value' (whatever your table uses)\n",
    "    by_window: if True, one panel per window; else single panel with colors by window\n",
    "    \"\"\"\n",
    "    df = summary_df.copy()\n",
    "    if p_col not in df.columns:\n",
    "        # try common names\n",
    "        p_col = 'p_value' if 'p_value' in df.columns else p_col\n",
    "    df = df[np.isfinite(df[effect_col]) & np.isfinite(df[p_col])]\n",
    "    if df.empty:\n",
    "        raise ValueError('no finite rows to plot')\n",
    "\n",
    "    df['_mlog10p'] = -np.log10(df[p_col].clip(lower=1e-300))\n",
    "\n",
    "    if by_window:\n",
    "        windows = list(df['window'].unique())\n",
    "        n = len(windows)\n",
    "        fig, axes = plt.subplots(n, 1, figsize=(7, max(3, 2.2*n)), sharex=True)\n",
    "        if n == 1:\n",
    "            axes = [axes]\n",
    "        for ax, w in zip(axes, windows):\n",
    "            sub = df[df['window'] == w]\n",
    "            if sig_col in sub.columns:\n",
    "                sig = sub[sig_col].astype(bool).values\n",
    "            else:\n",
    "                sig = np.zeros(len(sub), bool)\n",
    "            ax.scatter(sub[effect_col][~sig], sub['_mlog10p'][~sig], s=14, alpha=0.6)\n",
    "            ax.scatter(sub[effect_col][ sig], sub['_mlog10p'][ sig], s=20, alpha=0.9)\n",
    "            ax.axvline(0, linestyle='--', linewidth=1)\n",
    "            ax.set_ylabel('-log10(p)')\n",
    "            ax.set_title(str(w))\n",
    "        axes[-1].set_xlabel(\"Effect size (e.g., Cohen's d)\")\n",
    "        plt.tight_layout()\n",
    "        return fig, axes\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        for w in df['window'].unique():\n",
    "            sub = df[df['window'] == w]\n",
    "            ax.scatter(sub[effect_col], sub['_mlog10p'], s=14, alpha=0.7, label=str(w))\n",
    "        ax.axvline(0, linestyle='--', linewidth=1)\n",
    "        ax.set_xlabel(\"Effect size (e.g., Cohen's d)\")\n",
    "        ax.set_ylabel('-log10(p)')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_volcano(summary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## Noise correlations (pre-stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def noise_correlations(an, window=(-0.3, 0.0)):\n",
    "    if not getattr(an, 'psth_data', None):\n",
    "        an.run_full_analysis()\n",
    "    seg = an.psth_data['segments']; time = an.psth_data['psth']['time_axis']\n",
    "    bw = an.config.bin_width\n",
    "    i0 = int(np.searchsorted(time, window[0], side='left'))\n",
    "    i1 = int(np.searchsorted(time, window[1], side='right'))\n",
    "\n",
    "    def corr_offdiag(arr):  # arr: trials x time x units\n",
    "        if arr.shape[0] < an.config.min_trials:\n",
    "            return np.array([]), None\n",
    "        X = arr[:, i0:i1, :].mean(axis=1) / bw            # trials x units\n",
    "        X = X - X.mean(axis=0, keepdims=True)             # residuals\n",
    "        C = np.corrcoef(X, rowvar=False)                  # units x units\n",
    "        iu = np.triu_indices_from(C, k=1)\n",
    "        return C[iu], C\n",
    "\n",
    "    rA, CA = corr_offdiag(seg['event_a'])\n",
    "    rB, CB = corr_offdiag(seg['event_b'])\n",
    "\n",
    "    stat, p = (np.nan, np.nan)\n",
    "    if len(rA) > 0 and len(rB) > 0:\n",
    "        stat, p = mannwhitneyu(rA, rB, alternative='two-sided')\n",
    "\n",
    "    return {\n",
    "        'offdiag_event_a': rA, 'offdiag_event_b': rB,\n",
    "        'matrix_event_a': CA, 'matrix_event_b': CB,\n",
    "        'mw_stat': stat, 'mw_p': p\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def _iqr(x):\n",
    "    x = np.asarray(x, float)\n",
    "    q1, q3 = np.nanpercentile(x, [25, 75])\n",
    "    return float(q3 - q1)\n",
    "\n",
    "def _sym_lims(*arrays, q=99.0, min_lim=0.2):\n",
    "    a = np.concatenate([np.ravel(np.abs(z[np.isfinite(z)])) for z in arrays if z is not None and np.size(z)])\n",
    "    if a.size == 0:\n",
    "        return (-1, 1)\n",
    "    m = np.nanpercentile(a, q)\n",
    "    m = max(m, min_lim)\n",
    "    return (-m, m)\n",
    "\n",
    "# ---------- 1) distributions + MWU ----------\n",
    "def plot_noise_corr_distributions(res, title='Noise correlations (off-diagonal)', bins=40):\n",
    "    rA, rB = res['offdiag_event_a'], res['offdiag_event_b']\n",
    "    if rA.size == 0 or rB.size == 0:\n",
    "        print('Not enough trials for at least one condition.')\n",
    "        return None, None\n",
    "\n",
    "    lims = _sym_lims(rA, rB, q=99.0, min_lim=0.2)\n",
    "    edges = np.linspace(lims[0], lims[1], bins+1)\n",
    "\n",
    "    mA, mB = np.nanmedian(rA), np.nanmedian(rB)\n",
    "    iA, iB = _iqr(rA), _iqr(rB)\n",
    "    p  = res.get('mw_p', np.nan)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 3.2))\n",
    "    ax.hist(rA, bins=edges, alpha=0.45, density=True, label='event_a', edgecolor='none')\n",
    "    ax.hist(rB, bins=edges, alpha=0.45, density=True, label='event_b', edgecolor='none')\n",
    "    ax.axvline(0, ls='--', c='k', lw=1)\n",
    "    ax.set_xlim(lims); ax.set_xlabel('Noise correlation r'); ax.set_ylabel('Density')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(title=f'mWU p={p:.3g}\\nmedian/IQR  a={mA:.3f}/{iA:.3f},  b={mB:.3f}/{iB:.3f}')\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# ---------- 2) matrices (upper triangles) ----------\n",
    "def plot_noise_corr_matrices(res, clusters=None, vmax_q=99.0, title='Noise correlation matrices'):\n",
    "    CA, CB = res['matrix_event_a'], res['matrix_event_b']\n",
    "    if CA is None or CB is None:\n",
    "        print('Correlation matrices not available.')\n",
    "        return None, None, None\n",
    "\n",
    "    M = CA.shape[0]\n",
    "    mask = np.tril_indices(M, k=0)\n",
    "    A = CA.copy(); B = CB.copy()\n",
    "    A[mask] = np.nan; B[mask] = np.nan\n",
    "\n",
    "    vmin, vmax = _sym_lims(A, B, q=vmax_q)\n",
    "    cmap = plt.get_cmap('coolwarm').copy(); cmap.set_bad('white')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 3.4), constrained_layout=True)\n",
    "    im0 = axes[0].imshow(A, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    axes[0].set_title('event_a')\n",
    "    im1 = axes[1].imshow(B, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    axes[1].set_title('event_b')\n",
    "    D = CB - CA\n",
    "    D[mask] = np.nan\n",
    "    dvmin, dvmax = _sym_lims(D, q=vmax_q)\n",
    "    im2 = axes[2].imshow(D, vmin=dvmin, vmax=dvmax, cmap=cmap)\n",
    "    axes[2].set_title('event_b − event_a')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "    cbar0 = fig.colorbar(im0, ax=axes[:2], fraction=0.046, pad=0.04)\n",
    "    cbar0.set_label('r')\n",
    "    cbar1 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "    cbar1.set_label('Δr')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    return fig, axes, (vmin, vmax, dvmin, dvmax)\n",
    "\n",
    "# ---------- 3) paired scatter (per unit-pair) ----------\n",
    "def plot_noise_corr_scatter(res, title='Pairwise noise corr: event_a vs event_b'):\n",
    "    rA, rB = res['offdiag_event_a'], res['offdiag_event_b']\n",
    "    if rA.size == 0 or rB.size == 0 or rA.size != rB.size:\n",
    "        print('Off-diagonal vectors missing or mismatched sizes.')\n",
    "        return None, None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.2, 4.2))\n",
    "    ax.scatter(rA, rB, s=10, alpha=0.5)\n",
    "    lims = _sym_lims(rA, rB, q=99.0)\n",
    "    ax.plot(lims, lims, ls='--', c='k', lw=1)\n",
    "    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    ax.set_xlabel('r (event_a)'); ax.set_ylabel('r (event_b)')\n",
    "    ax.set_title(title)\n",
    "    # simple summaries\n",
    "    delta = np.nanmean(rB - rA)\n",
    "    ax.text(0.05, 0.95, f'Δ mean = {delta:.3f}', transform=ax.transAxes, va='top')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "# ---------- 4) one-call convenience ----------\n",
    "def noise_corr_report(an, window=(-0.3, 0.0), show=True):\n",
    "    res = noise_correlations(an, window=window)\n",
    "    title = f'Noise correlations  window=({window[0]:.2f},{window[1]:.2f})'\n",
    "    fig1, _ = plot_noise_corr_distributions(res, title=title)\n",
    "    fig2, _, _ = plot_noise_corr_matrices(res, title=title)\n",
    "    fig3, _ = plot_noise_corr_scatter(res, title=title)\n",
    "    if show:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.show()\n",
    "    return {'res': res, 'fig_dist': fig1, 'fig_mats': fig2, 'fig_scatter': fig3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute + visualize (pre-stop)\n",
    "out_pre = noise_corr_report(an, window=(-0.30, 0.00))\n",
    "\n",
    "# post-stop too\n",
    "out_post = noise_corr_report(an, window=(0.05, 0.35))\n",
    "\n",
    "# If you only want a specific view:\n",
    "\n",
    "# res = noise_correlations(an, window=(0.05, 0.35))\n",
    "res = noise_correlations(an, window=(-0.30, 0.00))\n",
    "plot_noise_corr_distributions(res)\n",
    "plot_noise_corr_matrices(res)\n",
    "plot_noise_corr_scatter(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Poisson GLM with elastic-net (regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "look it up again\n",
    "https://chatgpt.com/c/68b22a5f-d034-8331-8a9d-7f36b25a802b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# More plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Quickly plot PSTHs for the top significant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_top_psths(analyzer, summary: pd.DataFrame, epoch: str, top_k=6):\n",
    "    # pick significant clusters in the epoch, ranked by |d|\n",
    "    g = summary[(summary[\"window\"] == epoch) & (summary[\"sig_FDR\"])].copy()\n",
    "    if g.empty:\n",
    "        print(f\"No significant clusters for {epoch}.\"); return\n",
    "    g = g.sort_values(\"cohens_d\", key=lambda s: s.abs(), ascending=False).head(top_k)\n",
    "\n",
    "    # map string cluster ids back to analyzer cluster indices\n",
    "    plotted = 0\n",
    "    for cl_str in g[\"cluster\"]:\n",
    "        # analyzer.clusters holds original IDs (numeric or str)\n",
    "        # coerce both sides to string for robust matching\n",
    "        matches = np.where(np.array(list(map(str, analyzer.clusters))) == str(cl_str))[0]\n",
    "        if len(matches) == 0: \n",
    "            continue\n",
    "        ci = int(matches[0])\n",
    "        analyzer.plot_comparison(cluster_idx=ci)  # your existing method\n",
    "        plotted += 1\n",
    "    if plotted == 0:\n",
    "        print(\"Nothing plotted (no matches).\")\n",
    "\n",
    "# usage\n",
    "plot_top_psths(an, summary, \"early_dip(0.0–0.3)\", top_k=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Heatmap of effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# usage\n",
    "psth_postprocessing.plot_sig_heatmap(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where sig_FDR is True\n",
    "sig_rows = summary[summary[\"sig_FDR\"]]\n",
    "\n",
    "# plot effect sizes by epoch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(data=sig_rows, x=\"window\", y=\"cohens_d\", hue=\"cluster\", dodge=True)\n",
    "plt.axhline(0, color=\"k\", lw=1)\n",
    "plt.ylabel(\"Cohen's d (capture − miss)\")\n",
    "# change angle of xticks\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Significant neurons across epochs\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Bar chart of significant effects per epoch (one bar per cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sig_bars(summary: pd.DataFrame, epoch: str):\n",
    "    g = summary[(summary[\"window\"] == epoch) & (summary[\"sig_FDR\"])].copy()\n",
    "    if g.empty:\n",
    "        print(f\"No significant clusters for {epoch}.\"); return\n",
    "    g = g.sort_values(\"cohens_d\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(3, 0.35 * len(g))))\n",
    "    ax.barh(g[\"cluster\"], g[\"cohens_d\"])\n",
    "    ax.axvline(0, color=\"k\", lw=1, alpha=0.5)\n",
    "    ax.set_xlabel(\"Cohen's d (capture − miss)\")\n",
    "    ax.set_ylabel(\"Cluster\")\n",
    "    ax.set_title(f\"Significant clusters in {epoch}\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# usage\n",
    "plot_sig_bars(summary, \"pre_bump(-0.3–0.0)\")\n",
    "plot_sig_bars(summary, \"early_dip(0.0–0.3)\")\n",
    "plot_sig_bars(summary, \"late_rebound(0.3–0.8)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## Other anchors for stops\n",
    "https://docs.google.com/document/d/1nJOQlqsyq7eOVtUlL8TAAacC25vXeQvlGfDI-SbQrKE/edit?tab=t.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _finite_diff(y, t):\n",
    "    dy = np.gradient(y, t)  # robust to uneven dt\n",
    "    return dy\n",
    "\n",
    "def anchor_peak_decel(df, pre_window_s=0.5):\n",
    "    # peak negative acceleration in a window ending at stop onset\n",
    "    # requires per-stop onset; derive from first time of each stop_id\n",
    "    onset = df.groupby(\"stop_id\")[\"time\"].min()\n",
    "    t = df[\"time\"].to_numpy()\n",
    "    a = _finite_diff(df[\"speed\"].to_numpy(), t)  # acceleration\n",
    "    df2 = df.assign(accel=a)\n",
    "    rows = []\n",
    "    for sid, t0 in onset.items():\n",
    "        m = (df2[\"stop_id\"]==sid) & (df2[\"time\"].between(t0-pre_window_s, t0, inclusive=\"left\"))\n",
    "        if not m.any(): \n",
    "            continue\n",
    "        i = df2.loc[m, \"accel\"].idxmin()\n",
    "        rows.append((sid, df2.at[i, \"time\"]))\n",
    "    out = pd.DataFrame(rows, columns=[\"stop_id\",\"anchor_time\"])\n",
    "    out[\"anchor_kind\"] = \"peak_decel\"\n",
    "    return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## other processing/windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Per-cluster plots with bands\n",
    "# fig1 = an.plot_psth(cluster_idx=None, show_individual=False)\n",
    "\n",
    "\n",
    "# df = psth_postprocessing.export_psth_to_df(an)              # all clusters\n",
    "# df_c0 = psth_postprocessing.export_psth_to_df(an, [0])      # just the first cluster\n",
    "\n",
    "\n",
    "# windows = {\n",
    "#     \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "#     \"early_dip(0.0–0.3)\": (0.0, 0.3),\n",
    "#     \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "# }\n",
    "\n",
    "\n",
    "# windows = {\n",
    "#     \"pre_baseline(-0.6–-0.3)\": (-0.6, -0.3),\n",
    "#     \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "#     \"early_dip(0.0–0.1)\": (0.0, 0.1),\n",
    "#     \"early_rebound(0.1–0.3)\": (0.1, 0.3),\n",
    "#     \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "#     \"post_tail(0.8–1.5)\": (0.8, 1.5),\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## Validate near-miss single stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether \"near-miss\" stops (one_stop_w_ff_df) are truly not part of a stop cluster.  \n",
    "# A stop cluster is defined as ≥ 2 stops where each consecutive stop is within 50 cm (cumulative distance).\n",
    "\n",
    "# --- Step 1: Create one-stop dataframe and assign cluster IDs\n",
    "pn.make_one_stop_w_ff_df()\n",
    "pn.monkey_information = find_GUAT_or_TAFT_trials.add_stop_cluster_id(pn.monkey_information)\n",
    "\n",
    "# --- Step 2: Build stop-cluster summary\n",
    "stop_cluster_df = (\n",
    "    pn.monkey_information.loc[pn.monkey_information['whether_new_distinct_stop'], ['point_index', 'stop_cluster_id']]\n",
    "    .copy()\n",
    ")\n",
    "stop_cluster_df['num_stops_in_cluster'] = (\n",
    "    stop_cluster_df.groupby('stop_cluster_id')['point_index'].transform('count')\n",
    ")\n",
    "\n",
    "# --- Step 3: Merge cluster info into one-stop dataframe (if not already present)\n",
    "if 'stop_cluster_id' not in pn.one_stop_w_ff_df.columns:\n",
    "    pn.one_stop_w_ff_df = pn.one_stop_w_ff_df.merge(\n",
    "        stop_cluster_df.rename(columns={'point_index': 'first_stop_point_index'}),\n",
    "        on='first_stop_point_index',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# --- Step 4: Inspect any one-stop rows that actually fall in a multi-stop cluster\n",
    "pn.one_stop_w_ff_df[pn.one_stop_w_ff_df['num_stops_in_cluster'] > 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Check dt between stops in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Expand so each stop_index gets its own row\n",
    "GUAT_expanded = pn.GUAT_trials_df.explode(\"stop_indices\").reset_index(drop=True)\n",
    "\n",
    "# Optionally rename column\n",
    "GUAT_expanded = GUAT_expanded.rename(columns={\"stop_indices\": \"stop_point_index\"})\n",
    "GUAT_expanded['stop_time'] = pn.monkey_information['time'].loc[GUAT_expanded['stop_point_index']].values\n",
    "\n",
    "TAFT_expanded = pn.TAFT_trials_df.explode(\"stop_indices\").reset_index(drop=True)\n",
    "\n",
    "# Optionally rename column\n",
    "TAFT_expanded = TAFT_expanded.rename(columns={\"stop_indices\": \"stop_point_index\"})\n",
    "TAFT_expanded['stop_time'] = pn.monkey_information['time'].loc[TAFT_expanded['stop_point_index']].values\n",
    "\n",
    "\n",
    "# group TAFT_expanded by stop_cluster_id and drop the last row of each group\n",
    "TAFT_expanded.sort_values('stop_point_index', inplace=True)\n",
    "persist_TAFT_nonfinal = TAFT_expanded.groupby('stop_cluster_id').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_expanded['dt'] = GUAT_expanded['stop_time'].diff()\n",
    "TAFT_expanded['dt'] = TAFT_expanded['stop_time'].diff()\n",
    "GUAT_sub = GUAT_expanded[GUAT_expanded['dt'] < 0.5]\n",
    "TAFT_sub = TAFT_expanded[TAFT_expanded['dt'] < 0.5]\n",
    "GUAT_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(GUAT_sub[['dt']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(TAFT_sub[['dt']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_sub[['dt']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAFT_sub[['dt']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "## check inter-stop intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(process_monkey_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.monkey_information = process_monkey_information.add_whether_new_distinct_stop_and_stop_id(pn.monkey_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.monkey_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_stops_df = (\n",
    "    pn.monkey_information[['stop_id', 'point_index', 'time', 'stop_id_start_time', 'stop_id_end_time', 'stop_id_duration']].groupby(\"stop_id\", as_index=False, sort=False)\n",
    "    .first()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "unique_stops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stops_df = get_stops_utils.extract_unique_stops(pn.monkey_information)\n",
    "unique_stops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stops_df = get_stops_utils.extract_unique_stops(pn.monkey_information)\n",
    "onsets = unique_stops_df['time'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned = get_stops_utils.plot_inter_stop_intervals(onsets)\n",
    "ax1 = returned['ax1']\n",
    "\n",
    "# add an additional vertical line to the linear plot\n",
    "additional_vline = 0.3\n",
    "ax1.axvline(additional_vline, linestyle=\"--\", color=\"b\", alpha=0.8, label=f\"x = {additional_vline}s\")\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## check stop durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.histplot(unique_stops_df['stop_id_duration'], bins=100)\n",
    "ax.set_xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## add stop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.monkey_information = process_monkey_information.add_whether_new_distinct_stop_and_stop_id(pn.monkey_information)\n",
    "pn.monkey_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## stop clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.monkey_information = find_GUAT_or_TAFT_trials.add_stop_cluster_id(pn.monkey_information, max_cluster_distance=50, use_ff_caught_time_new_to_separate_clusters=False,\n",
    "                                                                        col_exists_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_cluster_distance in [50, 45, 40, 35, 30, 20, 50]:\n",
    "    print(f'max_cluster_distance: {max_cluster_distance}')\n",
    "    pn.monkey_information = find_GUAT_or_TAFT_trials.add_stop_cluster_id(pn.monkey_information, max_cluster_distance=max_cluster_distance, use_ff_caught_time_new_to_separate_clusters=False,\n",
    "                                                                        col_exists_ok=False)\n",
    "\n",
    "    stop_sub = pn.monkey_information[pn.monkey_information['stop_id'].notna()].copy()\n",
    "    stop_sub = stop_sub[['stop_id', 'stop_cluster_id']].drop_duplicates()\n",
    "    # Compute cluster sizes properly\n",
    "    stop_sub['cluster_size'] = stop_sub.groupby('stop_cluster_id')['stop_id'].transform('size')\n",
    "\n",
    "    # Show how many rows belong to clusters of size > 1\n",
    "    print('number of clusters: ', stop_sub['stop_cluster_id'].unique().shape[0])\n",
    "    print('number of stops: ', stop_sub['stop_id'].unique().shape[0])\n",
    "    print('number of clusters with more than 1 stop: ', stop_sub[stop_sub['cluster_size'] > 1].shape[0])\n",
    "    print('--------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stops = pn.monkey_information['stop_id'].unique().shape[0]\n",
    "num_clusters = pn.monkey_information['stop_cluster_id'].unique().shape[0]\n",
    "num_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "## censor_mask_for_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "censor_mdef censor_mask_for_event(t0, all_stops, time_axis, pad=0.15):\n",
    "    abs_times = t0 + time_axis\n",
    "    dmin = np.min(np.abs(abs_times[:,None] - all_stops[None,:]), axis=1)\n",
    "    keep = (dmin >= pad) | np.isclose(abs_times, t0)\n",
    "    return keep  # bool, shape (len(time_axis),)\n",
    "ask_for_event"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
