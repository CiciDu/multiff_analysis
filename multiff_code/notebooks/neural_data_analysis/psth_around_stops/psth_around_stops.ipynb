{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from decision_making_analysis.compare_add_features_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis import stops_psth, extract_stops_utils, psth_postprocessing\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0321\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0329\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0403\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0312\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0330\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0316\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_y_var_lags = False\n",
    "planning_data_by_point_exists_ok = True\n",
    "y_data_exists_ok = True\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(raw_data_folder_path=raw_data_folder_path)\n",
    "pn.prep_data_to_analyze_planning(planning_data_by_point_exists_ok=planning_data_by_point_exists_ok)\n",
    "# pn.planning_data_by_point, cols_to_drop = general_utils.drop_columns_with_many_nans(\n",
    "#     pn.planning_data_by_point)\n",
    "#pn.get_x_and_y_data_for_modeling(exists_ok=y_data_exists_ok, reduce_y_var_lags=reduce_y_var_lags)\n",
    "\n",
    "if not hasattr(pn, 'spikes_df'):\n",
    "    pn.retrieve_or_make_monkey_data()\n",
    "    pn.spikes_df = neural_data_processing.make_spikes_df(pn.raw_data_folder_path, pn.ff_caught_T_sorted,\n",
    "                                                            sampling_rate=pn.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Get captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example wiring (mirrors your original usage)\n",
    "valid_captures_df, filtered_no_capture_stops_df, stops_with_stats = extract_stops_utils.prepare_no_capture_and_captures(\n",
    "    monkey_information=pn.monkey_information,\n",
    "    closest_stop_to_capture_df=pn.closest_stop_to_capture_df,\n",
    "    ff_caught_T_new=pn.ff_caught_T_new,\n",
    "    min_stop_duration=0.02,\n",
    "    max_stop_duration=1.0,\n",
    "    capture_match_window=0.3,\n",
    "    distance_thresh=25.0,\n",
    "    distance_col=\"distance_from_ff_to_stop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_captures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_no_capture_stops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Get misses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##  one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_one_stop_w_ff_df()\n",
    "one_stop_miss_df = pn.one_stop_w_ff_df[['first_stop_point_index', 'first_stop_time', 'latest_visible_ff', 'ff_distance', 'min_distance_from_adjacent_stops']].copy()\n",
    "one_stop_miss_df.rename(columns={'first_stop_point_index': 'stop_point_index', 'first_stop_time': 'stop_time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.get_try_a_few_times_info()\n",
    "pn.get_give_up_after_trying_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Expand so each stop_index gets its own row\n",
    "GUAT_expanded = pn.GUAT_trials_df.explode(\"stop_indices\").reset_index(drop=True)\n",
    "\n",
    "# Optionally rename column\n",
    "GUAT_expanded = GUAT_expanded.rename(columns={\"stop_indices\": \"stop_point_index\"})\n",
    "GUAT_expanded['stop_time'] = pn.monkey_information['time'].loc[GUAT_expanded['stop_point_index']].values\n",
    "\n",
    "TAFT_expanded = pn.TAFT_trials_df.explode(\"stop_indices\").reset_index(drop=True)\n",
    "\n",
    "# Optionally rename column\n",
    "TAFT_expanded = TAFT_expanded.rename(columns={\"stop_indices\": \"stop_point_index\"})\n",
    "TAFT_expanded['stop_time'] = pn.monkey_information['time'].loc[TAFT_expanded['stop_point_index']].values\n",
    "\n",
    "\n",
    "# group TAFT_expanded by stop_cluster_id and drop the last row of each group\n",
    "TAFT_expanded.sort_values('stop_point_index', inplace=True)\n",
    "TAFT_expanded2 = TAFT_expanded.groupby('stop_cluster_id').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only preserve the first stop of each stop cluster\n",
    "GUAT_expanded3 = GUAT_expanded.groupby('stop_cluster_id').first().reset_index()\n",
    "TAFT_expanded3 = TAFT_expanded.groupby('stop_cluster_id').first().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GUAT, separate the last stop of each stop cluster\n",
    "GUAT_expanded4_first_several = GUAT_expanded.groupby('stop_cluster_id').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n",
    "GUAT_expanded4_last = GUAT_expanded.groupby('stop_cluster_id').last().reset_index()\n",
    "\n",
    "TAFT_expanded4_first_several = TAFT_expanded.groupby('stop_cluster_id').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n",
    "both_first_several = pd.concat([GUAT_expanded4_first_several[['stop_point_index', 'stop_time']], TAFT_expanded4_first_several[['stop_point_index', 'stop_time']]])\n",
    "both_first_several.sort_values('stop_point_index', inplace=True)\n",
    "both_first_several.reset_index(drop=True, inplace=True)\n",
    "both_first_several\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# run class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(stops_psth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = stops_psth.PSTHConfig(\n",
    "#     pre_window=1.0,\n",
    "#     post_window=1.0,\n",
    "#     bin_width=0.02,\n",
    "#     smoothing_sigma=0.05,\n",
    "#     min_trials=5,\n",
    "#     normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    "# )\n",
    "\n",
    "cfg = stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.05,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "# an = stops_psth.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "#                                                  captures_df=valid_captures_df,\n",
    "#                                                  #no_capture_stops_df=filtered_no_capture_stops_df,\n",
    "#                                                  no_capture_stops_df=one_stop_miss_df\n",
    "#                                                  )\n",
    "\n",
    "# an = stops_psth.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "#                                                  captures_df=TAFT_expanded2,\n",
    "#                                                  no_capture_stops_df=GUAT_expanded\n",
    "#                                                  )\n",
    "\n",
    "# an = stops_psth.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "#                                                  captures_df=TAFT_expanded2,\n",
    "#                                                  no_capture_stops_df=one_stop_miss_df\n",
    "#                                                  )\n",
    "\n",
    "an = stops_psth.create_psth_around_stops(pn.spikes_df, pn.monkey_information, pn.ff_caught_T_new, cfg,\n",
    "                                                 captures_df=both_first_several,\n",
    "                                                 no_capture_stops_df=GUAT_expanded4_last\n",
    "                                                 )\n",
    "\n",
    "# Per-cluster plots with bands\n",
    "fig1 = an.plot_psth(cluster_idx=None, show_individual=False)\n",
    "\n",
    "# Overlay comparison\n",
    "fig2 = an.plot_comparison(cluster_idx=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Stats in early post-stop window\n",
    "stats_ = an.statistical_comparison(time_window=(0.0, 0.5))\n",
    "\n",
    "\n",
    "df = psth_postprocessing.export_psth_to_df(an)              # all clusters\n",
    "df_c0 = psth_postprocessing.export_psth_to_df(an, [0])      # just the first cluster\n",
    "\n",
    "\n",
    "windows = {\n",
    "    \"pre_bump(-0.3–0.0)\": (-0.3, 0.0),\n",
    "    \"early_dip(0.0–0.3)\": (0.0, 0.3),\n",
    "    \"late_rebound(0.3–0.8)\": (0.3, 0.8),\n",
    "}\n",
    "summary = psth_postprocessing.compare_windows(an, windows, alpha=0.05)\n",
    "summary.sort_values([\"window\",\"p\"]).head(12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where sig_FDR is True\n",
    "sig_rows = summary[summary[\"sig_FDR\"]]\n",
    "\n",
    "# plot effect sizes by epoch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=sig_rows, x=\"window\", y=\"cohens_d\", hue=\"cluster\", dodge=True)\n",
    "plt.axhline(0, color=\"k\", lw=1)\n",
    "plt.ylabel(\"Cohen's d (capture − miss)\")\n",
    "plt.title(\"Significant neurons across epochs\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# More plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Quickly plot PSTHs for the top significant neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_top_psths(analyzer, summary: pd.DataFrame, epoch: str, top_k=6):\n",
    "    # pick significant clusters in the epoch, ranked by |d|\n",
    "    g = summary[(summary[\"window\"] == epoch) & (summary[\"sig_FDR\"])].copy()\n",
    "    if g.empty:\n",
    "        print(f\"No significant clusters for {epoch}.\"); return\n",
    "    g = g.sort_values(\"cohens_d\", key=lambda s: s.abs(), ascending=False).head(top_k)\n",
    "\n",
    "    # map string cluster ids back to analyzer cluster indices\n",
    "    plotted = 0\n",
    "    for cl_str in g[\"cluster\"]:\n",
    "        # analyzer.clusters holds original IDs (numeric or str)\n",
    "        # coerce both sides to string for robust matching\n",
    "        matches = np.where(np.array(list(map(str, analyzer.clusters))) == str(cl_str))[0]\n",
    "        if len(matches) == 0: \n",
    "            continue\n",
    "        ci = int(matches[0])\n",
    "        analyzer.plot_comparison(cluster_idx=ci)  # your existing method\n",
    "        plotted += 1\n",
    "    if plotted == 0:\n",
    "        print(\"Nothing plotted (no matches).\")\n",
    "\n",
    "# usage\n",
    "plot_top_psths(an, summary, \"early_dip(0.0–0.3)\", top_k=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Heatmap of effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sig_heatmap(summary: pd.DataFrame, title=\"Significant effects (Cohen's d)\"):\n",
    "    # keep only FDR-significant rows\n",
    "    sig = summary[summary[\"sig_FDR\"]].copy()\n",
    "    if sig.empty:\n",
    "        print(\"No significant results to plot.\")\n",
    "        return\n",
    "\n",
    "    # pivot to clusters × windows, values = d\n",
    "    pivot = sig.pivot_table(index=\"cluster\", columns=\"window\", values=\"cohens_d\", aggfunc=\"mean\")\n",
    "\n",
    "    # optional: sort clusters by strongest absolute effect\n",
    "    order = np.argsort(-pivot.abs().max(axis=1).values)\n",
    "    pivot = pivot.iloc[order]\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(8, max(3, 0.35 * len(pivot))))\n",
    "    im = ax.imshow(pivot.values, aspect=\"auto\", cmap=\"coolwarm\", vmin=-np.nanmax(abs(pivot.values)), vmax=np.nanmax(abs(pivot.values)))\n",
    "    ax.set_xticks(range(pivot.shape[1])); ax.set_xticklabels(pivot.columns, rotation=30, ha=\"right\")\n",
    "    ax.set_yticks(range(pivot.shape[0])); ax.set_yticklabels(pivot.index)\n",
    "    ax.set_title(title)\n",
    "    cbar = plt.colorbar(im, ax=ax); cbar.set_label(\"Cohen's d (capture − miss)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# usage\n",
    "plot_sig_heatmap(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Bar chart of significant effects per epoch (one bar per cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sig_bars(summary: pd.DataFrame, epoch: str):\n",
    "    g = summary[(summary[\"window\"] == epoch) & (summary[\"sig_FDR\"])].copy()\n",
    "    if g.empty:\n",
    "        print(f\"No significant clusters for {epoch}.\"); return\n",
    "    g = g.sort_values(\"cohens_d\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(3, 0.35 * len(g))))\n",
    "    ax.barh(g[\"cluster\"], g[\"cohens_d\"])\n",
    "    ax.axvline(0, color=\"k\", lw=1, alpha=0.5)\n",
    "    ax.set_xlabel(\"Cohen's d (capture − miss)\")\n",
    "    ax.set_ylabel(\"Cluster\")\n",
    "    ax.set_title(f\"Significant clusters in {epoch}\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# usage\n",
    "plot_sig_bars(summary, \"pre_bump(-0.3–0.0)\")\n",
    "plot_sig_bars(summary, \"early_dip(0.0–0.3)\")\n",
    "plot_sig_bars(summary, \"late_rebound(0.3–0.8)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Validate near-miss single stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether \"near-miss\" stops (one_stop_w_ff_df) are truly not part of a stop cluster.  \n",
    "# A stop cluster is defined as ≥ 2 stops where each consecutive stop is within 50 cm (cumulative distance).\n",
    "\n",
    "# --- Step 1: Create one-stop dataframe and assign cluster IDs\n",
    "pn.make_one_stop_w_ff_df()\n",
    "pn.monkey_information = find_GUAT_or_TAFT_trials.add_stop_cluster_id(pn.monkey_information)\n",
    "\n",
    "# --- Step 2: Build stop-cluster summary\n",
    "stop_cluster_df = (\n",
    "    pn.monkey_information.loc[pn.monkey_information['whether_new_distinct_stop'], ['point_index', 'stop_cluster_id']]\n",
    "    .copy()\n",
    ")\n",
    "stop_cluster_df['num_stops_in_cluster'] = (\n",
    "    stop_cluster_df.groupby('stop_cluster_id')['point_index'].transform('count')\n",
    ")\n",
    "\n",
    "# --- Step 3: Merge cluster info into one-stop dataframe (if not already present)\n",
    "if 'stop_cluster_id' not in pn.one_stop_w_ff_df.columns:\n",
    "    pn.one_stop_w_ff_df = pn.one_stop_w_ff_df.merge(\n",
    "        stop_cluster_df.rename(columns={'point_index': 'first_stop_point_index'}),\n",
    "        on='first_stop_point_index',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# --- Step 4: Inspect any one-stop rows that actually fall in a multi-stop cluster\n",
    "pn.one_stop_w_ff_df[pn.one_stop_w_ff_df['num_stops_in_cluster'] > 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Check dt between stops in clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Expand so each stop_index gets its own row\n",
    "GUAT_expanded = pn.GUAT_trials_df.explode(\"stop_indices\").reset_index(drop=True)\n",
    "\n",
    "# Optionally rename column\n",
    "GUAT_expanded = GUAT_expanded.rename(columns={\"stop_indices\": \"stop_point_index\"})\n",
    "GUAT_expanded['stop_time'] = pn.monkey_information['time'].loc[GUAT_expanded['stop_point_index']].values\n",
    "\n",
    "TAFT_expanded = pn.TAFT_trials_df.explode(\"stop_indices\").reset_index(drop=True)\n",
    "\n",
    "# Optionally rename column\n",
    "TAFT_expanded = TAFT_expanded.rename(columns={\"stop_indices\": \"stop_point_index\"})\n",
    "TAFT_expanded['stop_time'] = pn.monkey_information['time'].loc[TAFT_expanded['stop_point_index']].values\n",
    "\n",
    "\n",
    "# group TAFT_expanded by stop_cluster_id and drop the last row of each group\n",
    "TAFT_expanded.sort_values('stop_point_index', inplace=True)\n",
    "TAFT_expanded2 = TAFT_expanded.groupby('stop_cluster_id').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_expanded['dt'] = GUAT_expanded['stop_time'].diff()\n",
    "TAFT_expanded['dt'] = TAFT_expanded['stop_time'].diff()\n",
    "GUAT_sub = GUAT_expanded[GUAT_expanded['dt'] < 0.5]\n",
    "TAFT_sub = TAFT_expanded[TAFT_expanded['dt'] < 0.5]\n",
    "GUAT_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(GUAT_sub[['dt']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(TAFT_sub[['dt']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_sub[['dt']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAFT_sub[['dt']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## check inter-stop intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stops_df = extract_stops_utils.extract_unique_stops(pn.monkey_information)\n",
    "onsets = unique_stops_df['time'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned = extract_stops_utils.plot_inter_stop_intervals(onsets)\n",
    "ax1 = returned['ax1']\n",
    "\n",
    "# add an additional vertical line to the linear plot\n",
    "additional_vline = 0.2\n",
    "ax1.axvline(additional_vline, linestyle=\"--\", color=\"b\", alpha=0.8, label=f\"x = {additional_vline}s\")\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## make stop clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_stop_clusters(\n",
    "    stops_df: pd.DataFrame,\n",
    "    ff_caught_T_new: np.ndarray | None = None,\n",
    "    capture_match_window: float = 0.3,\n",
    "    isi_threshold: float = 0.5,\n",
    "    time_col: str = \"stop_time\",\n",
    "    event_type_col: str | None = \"event_type\",  # set to None if not available\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Group temporally close stops into clusters and label cluster outcome.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stops_df : DataFrame\n",
    "        Must contain column `time_col` (seconds). Optionally `event_type_col`\n",
    "        with values {\"capture\",\"miss\"} at the stop level.\n",
    "    ff_caught_T_new : array-like or None\n",
    "        Sorted capture times (s). Required if `event_type_col` is None.\n",
    "    capture_match_window : float\n",
    "        A stop is 'capture' if within this |Δt| to a capture time (when inferring).\n",
    "    isi_threshold : float\n",
    "        Two consecutive stops belong to the same cluster if their time gap <= threshold.\n",
    "    time_col : str\n",
    "        Column name for stop times.\n",
    "    event_type_col : str | None\n",
    "        Column name with stop labels. If None, labels will be inferred.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clusters_df : DataFrame\n",
    "        One row per cluster:\n",
    "          ['cluster_id','start','end','duration','n_stops','outcome',\n",
    "           'stop_indices','stop_times']\n",
    "        outcome ∈ {\"success\",\"giveup\"} where \"success\" = any capture stop in cluster.\n",
    "    stops_with_cluster : DataFrame\n",
    "        Original stops annotated with:\n",
    "          ['cluster_id','pos_in_cluster','is_capture','cluster_outcome']\n",
    "    \"\"\"\n",
    "    if time_col not in stops_df.columns:\n",
    "        raise ValueError(f\"`stops_df` must contain '{time_col}'\")\n",
    "\n",
    "    df = stops_df.sort_values(time_col).reset_index(drop=False).rename(columns={\"index\":\"orig_index\"}).copy()\n",
    "    t = df[time_col].to_numpy().astype(float)\n",
    "    n = len(df)\n",
    "\n",
    "    # --- determine per-stop capture flag ---\n",
    "    if event_type_col is not None and event_type_col in df.columns:\n",
    "        is_capture = (df[event_type_col].to_numpy() == \"capture\")\n",
    "    else:\n",
    "        if ff_caught_T_new is None:\n",
    "            raise ValueError(\"Provide ff_caught_T_new or an event_type_col with 'capture'/'miss'.\")\n",
    "        cap = np.asarray(ff_caught_T_new, dtype=float)\n",
    "        # vectorized nearest-neighbor distance in time\n",
    "        idx = np.searchsorted(cap, t, side=\"left\")\n",
    "        left_dt  = np.where(idx > 0,            np.abs(t - cap[np.clip(idx-1,0,len(cap)-1)]), np.inf)\n",
    "        right_dt = np.where(idx < cap.size,     np.abs(cap[idx] - t),                          np.inf)\n",
    "        min_dt = np.minimum(left_dt, right_dt)\n",
    "        is_capture = (min_dt <= capture_match_window)\n",
    "\n",
    "    # --- build clusters by ISI threshold ---\n",
    "    cluster_id = np.empty(n, dtype=int)\n",
    "    pos_in_cluster = np.empty(n, dtype=int)\n",
    "    cid = 0\n",
    "    start_idx = 0\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            cluster_id[i] = cid\n",
    "            pos_in_cluster[i] = 1\n",
    "            continue\n",
    "        gap = t[i] - t[i-1]\n",
    "        if gap <= isi_threshold:\n",
    "            # same cluster\n",
    "            cluster_id[i] = cid\n",
    "            pos_in_cluster[i] = pos_in_cluster[i-1] + 1\n",
    "        else:\n",
    "            # new cluster\n",
    "            cid += 1\n",
    "            cluster_id[i] = cid\n",
    "            pos_in_cluster[i] = 1\n",
    "\n",
    "    df[\"cluster_id\"] = cluster_id\n",
    "    df[\"pos_in_cluster\"] = pos_in_cluster\n",
    "    df[\"is_capture\"] = is_capture\n",
    "\n",
    "    # --- cluster-level aggregation ---\n",
    "    grp = df.groupby(\"cluster_id\", sort=True)\n",
    "    start = grp[time_col].min()\n",
    "    end = grp[time_col].max()\n",
    "    duration = end - start\n",
    "    n_stops = grp.size()\n",
    "    any_capture = grp[\"is_capture\"].any()\n",
    "    outcome = np.where(any_capture, \"success\", \"giveup\")\n",
    "\n",
    "    # collect member indices/times for reference\n",
    "    members_idx = grp[\"orig_index\"].apply(lambda x: x.to_list())\n",
    "    members_times = grp[time_col].apply(lambda x: x.to_list())\n",
    "\n",
    "    clusters_df = pd.DataFrame({\n",
    "        \"cluster_id\": start.index,\n",
    "        \"start\": start.values.astype(float),\n",
    "        \"end\": end.values.astype(float),\n",
    "        \"duration\": duration.values.astype(float),\n",
    "        \"n_stops\": n_stops.values.astype(int),\n",
    "        \"outcome\": outcome.astype(str),\n",
    "        \"stop_indices\": members_idx.values,\n",
    "        \"stop_times\": members_times.values,\n",
    "    }).sort_values(\"start\").reset_index(drop=True)\n",
    "\n",
    "    # annotate stops with cluster outcome\n",
    "    outcome_map = clusters_df.set_index(\"cluster_id\")[\"outcome\"].to_dict()\n",
    "    df[\"cluster_outcome\"] = df[\"cluster_id\"].map(outcome_map)\n",
    "\n",
    "    # return stops with original order preserved (plus annotations)\n",
    "    stops_with_cluster = df.sort_values(\"orig_index\").reset_index(drop=True)\n",
    "\n",
    "    return clusters_df, stops_with_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df, stops_annot = make_stop_clusters(\n",
    "    stops_df=my_stops_df,                  # must have 'stop_time'\n",
    "    ff_caught_T_new=ff_caught_T_new,       # if you don't already have event_type\n",
    "    capture_match_window=0.3,\n",
    "    isi_threshold=0.5,                      # tune: 0.3–0.6s works well\n",
    "    time_col=\"stop_time\",\n",
    "    event_type_col=None                     # or \"event_type\" if present\n",
    ")\n",
    "\n",
    "print(clusters_df.head())\n",
    "# cluster_id | start | end | duration | n_stops | outcome | stop_indices | stop_times\n",
    "\n",
    "print(stops_annot.head())\n",
    "# ... original columns ... + cluster_id | pos_in_cluster | is_capture | cluster_outcome\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## censor_mask_for_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "censor_mdef censor_mask_for_event(t0, all_stops, time_axis, pad=0.15):\n",
    "    abs_times = t0 + time_axis\n",
    "    dmin = np.min(np.abs(abs_times[:,None] - all_stops[None,:]), axis=1)\n",
    "    keep = (dmin >= pad) | np.isclose(abs_times, t0)\n",
    "    return keep  # bool, shape (len(time_axis),)\n",
    "ask_for_event"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
