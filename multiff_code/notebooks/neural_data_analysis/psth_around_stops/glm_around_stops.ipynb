{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import get_stops_utils, collect_stop_data\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_fit import stop_glm_fit, cv_stop_glm, glm_fit_utils, variance_explained\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_plotting import plot_spikes, plot_glm_fit, plot_tuning_func\n",
    "from neural_data_analysis.design_kits.design_around_event import event_binning, stop_design, cluster_design, design_checks\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_hyperparams import compare_glm_configs, glm_hyperparams_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.ff_visibility import ff_vis_epochs, vis_design\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_decoding_tools import glm_decoding_llr\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import assemble_stop_design\n",
    "\n",
    "# import decoding\n",
    "from neural_data_analysis.neural_analysis_tools.decoding_tools import decoding_utils, decoding_analysis, plot_decoding, cmp_decode\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "import json\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0301\"\n",
    "\n",
    "pn, datasets, comparisons = collect_stop_data.collect_stop_data_func(\n",
    "    raw_data_folder_path)\n",
    "\n",
    "globals().update(datasets)\n",
    "\n",
    "captures_df, valid_captures_df, filtered_no_capture_stops_df, stops_with_stats = get_stops_utils.prepare_no_capture_and_captures(\n",
    "    monkey_information=pn.monkey_information,\n",
    "    closest_stop_to_capture_df=pn.closest_stop_to_capture_df,\n",
    "    ff_caught_T_new=pn.ff_caught_T_new,\n",
    "    distance_col=\"distance_from_ff_to_stop\",\n",
    ")\n",
    "\n",
    "keys = ['guat_first_vs_taft_first', 'guat_middle_vs_taft_middle', 'first_giveup_vs_first_persist', 'switch_vs_retry_after_miss', 'switch_vs_retry_after_retry']\n",
    "\n",
    "cfg = core_stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.05,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_caught_T_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save all combos\n",
    "\n",
    "import json, os\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "with open(\"multiff_analysis/configs/comparisons.json\", \"w\") as f:\n",
    "    json.dump(comparisons, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only save a subset of comparisons\n",
    "\n",
    "keys = [\n",
    "    'guat_first_vs_taft_first',\n",
    "    'guat_middle_vs_taft_middle',\n",
    "    'first_giveup_vs_first_persist',\n",
    "    'switch_vs_retry_after_miss',\n",
    "    'switch_vs_retry_after_retry'\n",
    "]\n",
    "\n",
    "# Filter the comparisons list by key\n",
    "comparisons_sub = [c for c in comparisons if c['key'] in keys]\n",
    "\n",
    "\n",
    "import json, os\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "with open(\"multiff_analysis/configs/comparisons.json\", \"w\") as f:\n",
    "    json.dump(comparisons_sub, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"multiff_analysis/configs/comparisons.json\", \"r\") as f:\n",
    "    comparisons_sub = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comparisons_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# ITERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys, json, hashlib\n",
    "# from pathlib import Path\n",
    "\n",
    "# def list_sessions(monkey_dir: Path):\n",
    "#     return sorted([p.name for p in monkey_dir.iterdir() if p.is_dir() and p.name.startswith('data_')])\n",
    "\n",
    "# monkey_dir = Path('all_monkey_data/raw_monkey_data/monkey_Bruno')\n",
    "# all_sessions = list_sessions(monkey_dir)\n",
    "\n",
    "# for session in all_sessions:\n",
    "#     print(session)\n",
    "#     try:\n",
    "#         raw_data_folder_path = f\"all_monkey_data/raw_monkey_data/monkey_Bruno/{session}\"\n",
    "#         cmp_decode.summarize_and_plot_decoding(raw_data_folder_path)   \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in {session}: {e}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Non-cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import combine_info_utils, specific_utils\n",
    "\n",
    "monkey_name = 'monkey_Bruno'\n",
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "                raw_data_dir_name, monkey_name)\n",
    "\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    print(raw_data_folder_path)\n",
    "    \n",
    "    try:\n",
    "        cmp_decode.summarize_and_plot_decoding(raw_data_folder_path, cumulative=False)   \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {raw_data_folder_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    sessions_df_for_one_monkey.loc[index, 'finished'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import combine_info_utils, specific_utils\n",
    "\n",
    "monkey_name = 'monkey_Bruno'\n",
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "                raw_data_dir_name, monkey_name)\n",
    "\n",
    "\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    print(raw_data_folder_path)\n",
    "    \n",
    "    try:\n",
    "        cmp_decode.summarize_and_plot_decoding(raw_data_folder_path, cumulative=True)   \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {raw_data_folder_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    sessions_df_for_one_monkey.loc[index, 'finished'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## see best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_decode.plot_best_params(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.DataFrame()\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    print(raw_data_folder_path)\n",
    "    \n",
    "    try:\n",
    "        pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(\n",
    "                raw_data_folder_path=raw_data_folder_path)\n",
    "        job_result_dir = Path(pn.retry_decoder_folder_path) / \"runs\"\n",
    "        df_all = cmp_decode._load_all_results(job_result_dir, None)\n",
    "        df_sub = df_all[df_all['model_name']=='logreg_elasticnet']\n",
    "        df_combined = pd.concat([df_combined, df_sub], ignore_index=True)\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {raw_data_folder_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    sessions_df_for_one_monkey.loc[index, 'finished'] = True\n",
    "cmp_decode.plot_best_params(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or, to see each session separately\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    print(raw_data_folder_path)\n",
    "    \n",
    "    try:\n",
    "        pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(\n",
    "                raw_data_folder_path=raw_data_folder_path)\n",
    "        job_result_dir = Path(pn.retry_decoder_folder_path) / \"runs\"\n",
    "        df_all = cmp_decode._load_all_results(job_result_dir, None)\n",
    "        cmp_decode.plot_best_params(df_all)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {raw_data_folder_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    sessions_df_for_one_monkey.loc[index, 'finished'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "job_result_dir = Path(pn.retry_decoder_folder_path) / \"runs\"\n",
    "df_all = cmp_decode._load_all_results(job_result_dir, None)\n",
    "cmp_decode.plot_best_params(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# COMPARE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0301\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cmp_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmp_decode.summarize_and_plot_decoding(raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['sample_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['model_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-3, 3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = df_all[df_all['model_name']=='logreg_elasticnet']\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0301\"\n",
    "\n",
    "\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "\n",
    "#job_result_dir = Path(pn.retry_decoder_folder_path) / \"per_model\"\n",
    "job_result_dir = Path(pn.retry_decoder_folder_path) / \"runs\"\n",
    "summary_csv = Path(pn.retry_decoder_folder_path) / \"cross_model\" / \"sum.csv\"\n",
    "\n",
    "cmp_decode._ensure_methods_on_path()\n",
    "\n",
    "df_all = cmp_decode._load_all_results(job_result_dir, None)\n",
    "# df_all = cmp_decode._apply_filters(df_all, key=None, align=None)\n",
    "\n",
    "if df_all.empty:\n",
    "    raise RuntimeError(\"No rows matched the provided filters.\")\n",
    "\n",
    "summary = cmp_decode._summarize(df_all)\n",
    "# print(\"\\n=== Peak AUC by model and comparison ===\")\n",
    "# print(summary.to_string(index=False))\n",
    "\n",
    "if summary_csv:\n",
    "    out_path = Path(summary_csv)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    summary.to_csv(out_path, index=False)\n",
    "    #print(f\"[write] Summary CSV → {out_path}\")\n",
    "\n",
    "# Overlay timecourses with a line per model, subplots per comparison\n",
    "plot_decoding.plot_decoding_timecourse(\n",
    "    df_all,\n",
    "    groupby_cols=(\"model_name\",),\n",
    "    split_by=(\"a_label\", \"b_label\"),\n",
    "    align_col=\"align_by_stop_end\",\n",
    "    value_col=\"mean_auc\",\n",
    "    sig_col=\"sig_ttest\" if \"sig_ttest\" in df_all.columns else \"sig_FDR\",\n",
    "    err_col=\"sd_auc\" if \"sd_auc\" in df_all.columns else None,\n",
    "    title_prefix=\"Decoding timecourse by model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the uniqueness of entries in df_all\n",
    "df_all.groupby(['key', 'window_start', 'model_name', 'align_by_stop_end']).count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_decode._plot_summary_bars(summary, title_prefix=\"Peak AUC by model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# See best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# assume df is your DataFrame and 'best_params' is the column name\n",
    "df = df_all[df_all['model_name']=='logreg_elasticnet'].copy()\n",
    "\n",
    "# safely parse the string dicts\n",
    "df['best_params'] = df['best_params'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# extract C and l1_ratio into separate columns\n",
    "df['C'] = df['best_params'].apply(lambda d: d.get('C'))\n",
    "df['l1_ratio'] = df['best_params'].apply(lambda d: d.get('l1_ratio'))\n",
    "\n",
    "# count frequency of each (C, l1_ratio) pair\n",
    "count_df = (\n",
    "    df.groupby(['C', 'l1_ratio'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "# make heatmap\n",
    "pivot = count_df.pivot(index='l1_ratio', columns='C', values='count').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(pivot, annot=True, fmt='.0f', cmap='viridis')\n",
    "plt.title('Frequency of Best Params (C vs l1_ratio)')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('l1_ratio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['align_by_stop_end'] == False] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['model']=='df_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['model_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['key'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.sort_values(by=['key', 'window_start', 'model_name', 'align_by_stop_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_sub = df_all[df_all['model_name']=='logreg_elasticnet']\n",
    "df_all_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(\n",
    "        raw_data_folder_path=raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, glob\n",
    "\n",
    "model_name = 'mlp'\n",
    "\n",
    "files = glob.glob(str(Path(pn.retry_decoder_folder_path) / \"runs\" / model_name / \"*.csv\"))\n",
    "df_results = pd.concat(pd.read_csv(f) for f in files)\n",
    "\n",
    "combined_results_path = Path(pn.retry_decoder_folder_path) / \"per_model\" / model_name\n",
    "os.makedirs(combined_results_path, exist_ok=True)\n",
    "df_results.to_csv(combined_results_path / \"combined.csv\", index=False)\n",
    "df_results['p_perm_sig'] = df_results['p_perm'] < 0.05\n",
    "\n",
    "# df_results = df_results[df_results['n_perm'] >= 1000]\n",
    "df_results = df_results.sort_values(by=['key', 'a_label', 'b_label', 'window_start', 'align_by_stop_end', 'n_perm']).groupby(['key', 'a_label', 'b_label', 'window_start', 'align_by_stop_end']).last().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['model_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results['model_name']=='mlp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Reset progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(progress_path, \"r\") as f:\n",
    "#     progress = json.load(f)\n",
    "# progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "monkey_dir = Path('all_monkey_data/raw_monkey_data/monkey_Bruno')\n",
    "retry_monkey_dir = Path('all_monkey_data/retry_decoder/monkey_Bruno')\n",
    "retry_monkey_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "progress_filename = f\"_decoding_progress_all.json\"\n",
    "progress_path = retry_monkey_dir / progress_filename\n",
    "\n",
    "all_sessions = []\n",
    "if monkey_dir.exists():\n",
    "    all_sessions = sorted([p.name for p in monkey_dir.iterdir() if p.is_dir() and p.name.startswith('data_')])\n",
    "\n",
    "with open(progress_path, \"r\") as f:\n",
    "    progress = json.load(f)\n",
    "    \n",
    "models = progress['models']\n",
    "    \n",
    "progress = {\n",
    "    'monkey': monkey_dir.name if monkey_dir else None,\n",
    "    'all': all_sessions,\n",
    "    'done': [],\n",
    "    'pending': all_sessions,\n",
    "    'last_updated': datetime.now().isoformat(timespec='seconds'),\n",
    "    'keys': progress['keys'],\n",
    "    'models': progress['models'],\n",
    "    'per_model_done': {m: [] for m in models} if models else {},\n",
    "}\n",
    "\n",
    "with open(progress_path, 'w') as f:\n",
    "    json.dump(progress, f, indent=2)\n",
    "\n",
    "print(f\"[SLURM] Progress reset: {progress_path} | done_all=0 | pending_all={len(progress['pending'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [(round(t, 1), round(t + 0.1, 1)) for t in np.arange(-0.2, 0.2, 0.1)]\n",
    "df_results = decoding_analysis.run_all_decoding_comparisons_cumulative(\n",
    "    comparisons=comparisons, keys=keys, datasets=datasets, pn=pn, cfg=cfg,\n",
    "    model_name='logreg_elasticnet',\n",
    "    n_perm=0,\n",
    "    windows=windows,\n",
    "    save_dir=Path(pn.retry_decoder_folder_path) / 'cum_window_runs',\n",
    "    exists_ok=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['guat_first_vs_taft_first', 'guat_middle_vs_taft_middle', 'first_giveup_vs_first_persist', 'switch_vs_retry_after_miss', 'switch_vs_retry_after_retry']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = 'svm'\n",
    "do_testing = True\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "save_dir = Path(pn.retry_decoder_folder_path) / \"runs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#windows = [(t, t + 0.1) for t in np.arange(-0.2, 0.3, 0.1)]\n",
    "# windows = [(round(t, 2), round(t + 0.1, 2)) for t in np.arange(-0.2, 0.3, 0.05)]\n",
    "\n",
    "windows = [[-0.2, 0.2]]\n",
    "\n",
    "out_base = Path(pn.retry_decoder_folder_path) / 'temp_runs'\n",
    "\n",
    "df_results = decoding_analysis.run_all_decoding_comparisons(\n",
    "    comparisons=comparisons,\n",
    "    keys=['guat_first_vs_taft_first', 'guat_middle_vs_taft_middle',\n",
    "          'first_giveup_vs_first_persist', 'switch_vs_retry_after_miss',\n",
    "          'switch_vs_retry_after_retry'],\n",
    "    datasets=datasets,\n",
    "    pn=pn,\n",
    "    cfg=cfg,\n",
    "    model_name='logreg_elasticnet', # 'logreg_elasticnet'\n",
    "    #model_kwargs={'C': 2.0, 'gamma': 0.05},\n",
    "    tune=False,\n",
    "    k=5,\n",
    "    n_perm=5,          # >0 enables permutation testing\n",
    "    alpha=0.05,\n",
    "    windows=windows,\n",
    "    do_testing=True,    # <— set to False to skip all tests,\n",
    "    save_dir=out_base,\n",
    "    overwrite=False,\n",
    "    exists_ok=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, unit_ids = decoding_utils.build_Xy(an, window=(0,0.2))\n",
    "# real_auc, auc_null, p_perm = decoding_utils.permutation_test_auc(X, y)\n",
    "# mean_auc, sd, tstat, p_ttest, aucs = decoding_utils.ttest_auc_folds(X, y)\n",
    "# print(f\"Permutation p={p_perm:.4f}, t-test p={p_ttest:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['mean_auc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub = df_results[df_results['align_by_stop_end']==True].copy()\n",
    "# df_results_sub = df_results_sub[df_results_sub['key'].isin(keys)]\n",
    "\n",
    "plot_decoding.plot_decoding_auc_heatmap(\n",
    "    df_results_sub, threshold=0.55, cmap='magma', title='Align by stop end',\n",
    "    x_tick_position='center',\n",
    "    sig_col='p_perm_sig',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = df_results.sort_values(by=['key', 'a_label', 'b_label', 'window_start', 'align_by_stop_end', 'n_perm']).groupby(['key', 'a_label', 'b_label', 'window_start', 'align_by_stop_end']).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub[(df_results_sub['a_label'] == 'GUAT first') & (df_results_sub['b_label'] == 'TAFT first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub = df_results[df_results['align_by_stop_end']==False].copy()\n",
    "df_results_sub = df_results_sub[df_results_sub['key'].isin(keys)]\n",
    "plot_decoding.plot_decoding_auc_heatmap(\n",
    "    df_results_sub, threshold=0.55, cmap='magma', title='Align by stop beginning',\n",
    "    sig_col='p_perm_sig'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub = df_results[df_results['align_by_stop_end']==True].copy()\n",
    "df_results_sub = df_results_sub[df_results_sub['key'].isin(keys)]\n",
    "\n",
    "\n",
    "plot_decoding.plot_decoding_timecourse(\n",
    "    df_results_sub,\n",
    "    groupby_cols=('align_by_stop_end',),\n",
    "    split_by=('a_label','b_label'),\n",
    "    value_col='mean_auc',\n",
    "    sig_col='p_perm_sig',\n",
    "    err_col='sd_auc',\n",
    "    err_type='sem',\n",
    "    title_prefix='Decoding timecourse per comparison'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decoding.plot_decoding_timecourse(\n",
    "    df_results,\n",
    "    groupby_cols=('a_label','b_label'),\n",
    "    split_by=('align_by_stop_end'),\n",
    "    title_prefix='Decoding per alignment'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[(df_results['a_label']=='one stop miss') & (df_results['b_label']=='both first miss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = decoding_utils.run_decoding(\n",
    "#     an,\n",
    "#     model_name='svm',\n",
    "#     tune=False,\n",
    "#     model_kwargs={'C': 2.0, 'gamma': 0.05}\n",
    "# )\n",
    "\n",
    "# res = decoding_utils.run_decoding(an, model_name='rf', tune=False, search='grid')\n",
    "\n",
    "# res = run_decoding(an, model_name='mlp', tune=False, search='random', n_iter=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# PSTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = core_stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.05,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "\n",
    "keys = ['guat_first_vs_taft_first', 'guat_middle_vs_taft_middle', 'first_giveup_vs_first_persist', 'switch_vs_retry_after_retry']\n",
    "comparisons_sub = [c for c in comparisons if c['key'] in keys]\n",
    "\n",
    "runs = compare_events.run_all_comparisons(\n",
    "    comparisons_sub, datasets, pn.spikes_df, pn.monkey_information, cfg,\n",
    "    align_by_stop_end=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "# Get stop info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## new_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_with_stats['stop_time'] = stops_with_stats['stop_id_start_time']\n",
    "stops_with_stats['prev_time'] = stops_with_stats['stop_id_end_time'].shift(1)\n",
    "stops_with_stats['next_time'] = stops_with_stats['stop_id_start_time'].shift(-1)\n",
    "\n",
    "new_seg_info = event_binning.pick_event_window(stops_with_stats,\n",
    "                                                pre_s=0.2, post_s=1.0, min_pre_bins=1, min_post_bins=20, bin_dt=0.04)\n",
    "\n",
    "if 'stop_id' not in pn.closest_stop_to_capture_df.columns:\n",
    "    pn.closest_stop_to_capture_df = get_stops_utils.add_stop_id_to_closest_stop_to_capture_df(\n",
    "        pn.closest_stop_to_capture_df,\n",
    "        pn.monkey_information,\n",
    "    )\n",
    "    \n",
    "if 'captured' not in new_seg_info.columns:\n",
    "    pn.closest_stop_to_capture_df['captured'] = 1\n",
    "    new_seg_info = new_seg_info.merge(pn.closest_stop_to_capture_df[['stop_id', 'captured']].drop_duplicates(), on='stop_id', how='left')\n",
    "    new_seg_info['captured'] = new_seg_info['captured'].fillna(0)\n",
    "    \n",
    "new_seg_info['event_id'] = new_seg_info['stop_id']\n",
    "new_seg_info['event_time'] = new_seg_info['stop_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_with_stats = stops_with_stats[['stop_id','stop_cluster_id','stop_id_start_time','stop_id_end_time']].copy()\n",
    "events_with_stats.rename(columns={'stop_id':'event_id', 'stop_cluster_id':'event_cluster_id', \n",
    "                                  'stop_id_start_time':'event_id_start_time', \n",
    "                                  'stop_id_end_time':'event_id_end_time'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## what if only keep stop clusters w captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_id_w_captures = new_seg_info.loc[new_seg_info['captured'] == 1, 'stop_cluster_id'].unique()\n",
    "# new_seg_info = new_seg_info[new_seg_info['stop_cluster_id'].isin(cluster_id_w_captures)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_seg_info = new_seg_info[new_seg_info['captured'] == 1].copy()\n",
    "# new_seg_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## plot rastors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import raster_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute data into new_segments and add rel_spike_time\n",
    "pn.get_new_seg_info(cur_or_nxt='nxt', first_or_last='first', time_limit_to_count_sighting=2, pre_event_window=0.75, post_event_window=0.75, exists_ok=False)\n",
    "\n",
    "new_seg_info2 = new_seg_info.sort_values('stop_id_duration', ascending=True).reset_index(drop=True)\n",
    "new_seg_info2['new_segment'] = np.arange(len(new_seg_info2))\n",
    "new_seg_info2['new_seg_start_time'] = new_seg_info2['stop_id_start_time'] - 1\n",
    "new_seg_info2['new_seg_end_time'] = new_seg_info2['stop_id_end_time'] + 1\n",
    "new_seg_info2['next_stop_time'] = new_seg_info2['next_time']\n",
    "aligned_spike_trains = raster_plot.prepare_aligned_spike_trains(new_seg_info2, pn.spikes_df,\n",
    "                                                                columns_to_preserve=['event_time', 'stop_time', 'next_stop_time',\n",
    "                                                                                     'stop_id_end_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info['next_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_spike_trains.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_spike_trains = raster_plot.add_relative_times(aligned_spike_trains, reference_time_col='event_time')\n",
    "aligned_spike_trains = raster_plot.add_scaling_info(aligned_spike_trains, scale_anchor_col='event_time',\n",
    "                                                    scale_factor_lower_col='stop_time',\n",
    "                                                    scale_factor_upper_col='stop_id_end_time',\n",
    "                                                    )\n",
    "\n",
    "raster_plot.plot_rasters(aligned_spike_trains, \n",
    "                         max_clusters_to_plot=1,\n",
    "                         scale_spike_times=False,\n",
    "                         #col_to_rearrange_segments='rel_stop_time',\n",
    "                         xmin=-0.5,\n",
    "                         xmax=3,\n",
    "                         events_to_plot=[\n",
    "                                        #'rel_event_time', \n",
    "                                        'rel_stop_time', \n",
    "                                        #'rel_prev_ff_caught_time',\n",
    "                                        #'rel_new_seg_start_time', \n",
    "                                        'rel_stop_id_end_time',\n",
    "                                        'rel_new_seg_end_time',\n",
    "                                        'rel_next_stop_time',\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "## Dist of stop durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_seg_info2['stop_id_duration']\n",
    "for perc in [0, 25, 50, 90, 95, 98, 99, 99.5, 99.9, 100]:\n",
    "    print(f'{perc}th percentile: {np.percentile(data, perc)}')\n",
    "\n",
    "sns.histplot(data)\n",
    "plt.xlim(0, 5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "# Prepare for GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "## binned_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import assemble_stop_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes_og, binned_feats_og, offset_log_og, meta_used_og = assemble_stop_design.build_stop_design(new_seg_info, events_with_stats, \n",
    "                                                                             pn.monkey_information, \n",
    "                                                                             pn.spikes_df, pn.ff_dataframe, \n",
    "                                                                             datasets=datasets,\n",
    "                                                                             bin_dt=0.04, add_ff_visible_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats = binned_feats_og.copy()\n",
    "binned_spikes = binned_spikes_og.copy()\n",
    "offset_log = offset_log_og\n",
    "meta_used = meta_used_og.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "### take out subsets if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out stops that are misses\n",
    "\n",
    "binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "    binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "    np.asarray(binned_feats_og['miss']==1)\n",
    ")\n",
    "\n",
    "binned_feats.drop(columns=['miss'], inplace=True)\n",
    "\n",
    "# also need to drop columns that involve 'captured'\n",
    "cols_to_drop = [c for c in binned_feats.columns if 'captured' in c]\n",
    "binned_feats = binned_feats.drop(columns=cols_to_drop)\n",
    "\n",
    "# also drop 'TAFT_last' if present\n",
    "if 'TAFT_last' in binned_feats.columns:\n",
    "    binned_feats = binned_feats.drop(columns=['TAFT_last'])\n",
    "    \n",
    "# actually, just drop all columns that involve GUAT or TAFT\n",
    "cols_to_drop = [c for c in binned_feats.columns if 'GUAT' in c or 'TAFT' in c or 'miss' in c]\n",
    "binned_feats = binned_feats.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out stops that are not misses\n",
    "\n",
    "binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "    binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "    np.asarray(~binned_feats_og['miss']==1)\n",
    ")\n",
    "\n",
    "cols_to_drop = ['miss', 'GUAT_first', 'TAFT_first', 'GUAT_middle', 'TAFT_middle', 'GUAT_last', 'one_stop_miss']\n",
    "binned_feats.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out stops that are not retries\n",
    "\n",
    "binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "    binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "    np.asarray(binned_feats_og['whether_retry']==0)\n",
    ")\n",
    "\n",
    "cols_to_drop = ['whether_retry', 'next_gap_s_z', 'GUAT_first', 'TAFT_first', 'GUAT_middle', 'TAFT_middle', 'GUAT_last', 'one_stop_miss']\n",
    "binned_feats.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_og['random_bool'] = np.random.rand(len(binned_feats_og)) > 0.5\n",
    "binned_feats_og['random_bool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out second half of the data\n",
    "binned_feats = binned_feats_og.loc[15000:].copy()\n",
    "binned_spikes = binned_spikes_og.loc[15000:].copy()\n",
    "offset_log = offset_log_og[15000:].copy()\n",
    "meta_used = meta_used_og.loc[15000:].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out random stops\n",
    "binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "    binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "    np.asarray(binned_feats_og['random_bool'])\n",
    ")\n",
    "\n",
    "binned_feats.drop(columns=['random_bool'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### add interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of variables you want to interact with 'whether_retry'\n",
    "vars_to_interact = [c for c in binned_feats.columns \n",
    "                    if c not in ['whether_retry', 'intercept', 'const']]\n",
    "\n",
    "for c in vars_to_interact:\n",
    "    new_col = f'{c}_x_retry'\n",
    "    binned_feats[new_col] = binned_feats[c] * binned_feats['whether_retry']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "### scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_sc, scaled_cols = event_binning.selective_zscore(binned_feats)\n",
    "binned_feats_sc = sm.add_constant(binned_feats_sc, has_constant='add')\n",
    "print('Scaled columns:', scaled_cols)\n",
    "\n",
    "# drop columns that are constant\n",
    "const_cols = [c for c in binned_feats_sc.columns \n",
    "               if binned_feats_sc[c].nunique(dropna=False) <= 1 and c not in ['intercept', 'const']]\n",
    "print(\"Constant columns:\", const_cols)\n",
    "\n",
    "binned_feats_sc = binned_feats_sc.drop(columns=const_cols)\n",
    "df_X = binned_feats_sc.copy()\n",
    "df_Y = binned_spikes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "## check VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_sc = binned_feats_sc.drop(columns=['next_gap_s_z_x_retry', 'cluster_rel_time_s_z_x_retry', \n",
    "                                                'cluster_progress_c_x_retry', 'event_is_first_in_cluster_x_retry'], \n",
    "                                       errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pruned = design_checks.check_design(binned_feats_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "## inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_used.groupby('event_id').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats[(binned_feats['miss'] == False) & (binned_feats['whether_retry'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats['whether_retry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats['miss'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "## drop columns that involve 'captured'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop = [c for c in binned_feats_sc.columns if 'captured' in c]\n",
    "# binned_feats_sc = binned_feats_sc.drop(columns=cols_to_drop)\n",
    "# binned_feats_sc.describe()\n",
    "# ## check df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "# CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst = cca_class.CCAclass(X1=binned_spikes.copy(), X2=binned_feats_sc.copy(), lagging_included=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.conduct_cca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X2', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "# GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "## regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = binned_feats_sc.copy()\n",
    "df_Y = binned_spikes.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(glm_fit_utils)\n",
    "reload(stop_glm_fit)\n",
    "\n",
    "report = stop_glm_fit.glm_mini_report(\n",
    "    df_X=df_X, df_Y=df_Y, offset_log=offset_log,\n",
    "    cov_type='HC1', \n",
    "    fast_mle=True,\n",
    "    do_inference=True, \n",
    "    make_plots=True,\n",
    "    show_plots=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df    = report['coefs_df']\n",
    "metrics_df  = report['metrics_df']\n",
    "pop_tests   = report['population_tests_df']\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df[(coefs_df['term'] == 'captured') & (coefs_df['sig_FDR'] == True)].sort_values('p', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df['refit_on_support'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "## VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(variance_explained)\n",
    "df_Y_pred = variance_explained.build_df_Y_pred_from_results(\n",
    "    results=report['results'],\n",
    "    df_X=df_X,\n",
    "    offset_log=offset_log,\n",
    "    df_Y=df_Y\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "assert df_Y_pred.shape == df_Y.shape\n",
    "assert np.isfinite(df_Y_pred.values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert observed/predicted DataFrames to arrays\n",
    "X = df_Y.to_numpy()      # observed counts\n",
    "X_hat = df_Y_pred.to_numpy() # predicted expected counts\n",
    "event_ids = meta_used['event_id'].to_numpy()\n",
    "\n",
    "ve_pop, k_eff = variance_explained.population_VE_in_PCspace(X, X_hat, k=10, center='neuron')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_per_neuron, ve_mean = variance_explained.single_neuron_temporal_VE(X, X_hat, aggregate='mean')\n",
    "ve_median = float(np.median(ve_per_neuron))\n",
    "\n",
    "print(f'Mean single-neuron VE:   {ve_mean:.3f}')\n",
    "print(f'Median single-neuron VE: {ve_median:.3f}')\n",
    "\n",
    "variance_explained.plot_single_neuron_VE_hist(ve_per_neuron)\n",
    "\n",
    "ve_pop, k_eff = variance_explained.population_VE_in_PCspace(X, X_hat, k=10, center='neuron')\n",
    "print(f'Population VE in {k_eff} PCs: {ve_pop:.3f}')\n",
    "\n",
    "variance_explained.plot_population_VE_bar(ve_pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(ve_per_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-stop breakdown (good for figures)\n",
    "per_event_df = variance_explained.per_event_breakdown(X, X_hat, event_ids=meta_used['event_id'].to_numpy(), k=10)\n",
    "print(per_event_df.head())\n",
    "\n",
    "summary_metrics = {\n",
    "    'VE_population_PC': ve_pop,\n",
    "    'VE_single_unit_mean': ve_mean,\n",
    "    'VE_single_unit_median': ve_median,\n",
    "    'PCs_used': k_eff\n",
    "}\n",
    "print(pd.Series(summary_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## Plot spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_spikes)\n",
    "\n",
    "cluster_idx = 6\n",
    "\n",
    "for event_id in range(30, 33):\n",
    "    if event_id in meta_used['event_id'].values:\n",
    "        # If your GLM used offset_log = np.log(exposure_s), you can omit exposure_s:\n",
    "        plot_spikes.plot_observed_vs_predicted_event(\n",
    "            binned_feats_sc=binned_feats_sc,\n",
    "            binned_spikes=binned_spikes,\n",
    "            meta_used=meta_used,\n",
    "            offset_log=offset_log,\n",
    "            model_res=report['results'][cluster_idx],   # GLM for cluster 0\n",
    "            cluster_idx=cluster_idx,\n",
    "            seg_id=event_id,\n",
    "            seg_col='event_id',\n",
    "            time_col='rel_center'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "## Note large coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_fit_utils.summarize_large_coeffs(report['coefs_df'], df_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cv_stop_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = meta_used['event_id']  # or your session/stop-window IDs\n",
    "# scores = cv_stop_glm.cv_score_per_cluster(binned_feats_sc, binned_spikes, offset_log, groups, n_splits=5)\n",
    "# cv_stop_glm.plot_cv_scores(scores)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "## CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def _zscore(X, eps=1e-12):\n",
    "    mu = np.nanmean(X, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(X, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (X - mu) / sd, mu, sd\n",
    "\n",
    "def _nanmask_pair(X, Y):\n",
    "    m = np.isfinite(X).all(1) & np.isfinite(Y).all(1)\n",
    "    return X[m], Y[m], m\n",
    "\n",
    "def cca_pop_metric_train_test(\n",
    "    X_tr, Y_tr, X_te, Y_te, *,\n",
    "    n_components=10,\n",
    "    standardize=True,\n",
    "    map_type='diag',          # 'diag' (per-dim gain) or 'full' (least-squares)\n",
    "    max_iter=1000,\n",
    "    eps=1e-12\n",
    "):\n",
    "    \"\"\"\n",
    "    Population comparison in CCA subspace (train/test safe).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_tr, Y_tr : (n_train, n_neurons)\n",
    "    X_te, Y_te : (n_test,  n_neurons)\n",
    "    n_components : number of canonical pairs to keep\n",
    "    standardize : z-score each view before CCA (recommended)\n",
    "    map_type : 'diag' maps each canonical dim with a scalar gain learned on train;\n",
    "               'full' learns an unconstrained linear map between canonical spaces on train.\n",
    "    Returns\n",
    "    -------\n",
    "    out : dict\n",
    "        {\n",
    "          've_test': float in [0,1],\n",
    "          'corrs_train': (k,), canonical correlations on train,\n",
    "          'corrs_test':  (k,), canonical correlations on test (using train-fitted CCA),\n",
    "          'k_eff': int, number of dims used,\n",
    "          'A': mapping matrix in canonical space (k x k),\n",
    "          'cca': fitted CCA object,\n",
    "          'mu_sigmas': {(view): (mu, sd)} if standardized\n",
    "        }\n",
    "    \"\"\"\n",
    "    X_tr, Y_tr, _ = _nanmask_pair(np.asarray(X_tr, float), np.asarray(Y_tr, float))\n",
    "    X_te, Y_te, _ = _nanmask_pair(np.asarray(X_te, float), np.asarray(Y_te, float))\n",
    "\n",
    "    # standardize each view (prevents scale dominating CCA)\n",
    "    mu_sigmas = {}\n",
    "    if standardize:\n",
    "        X_tr, mu_x, sd_x = _zscore(X_tr, eps)\n",
    "        Y_tr, mu_y, sd_y = _zscore(Y_tr, eps)\n",
    "        X_te = (X_te - mu_x) / sd_x\n",
    "        Y_te = (Y_te - mu_y) / sd_y\n",
    "        mu_sigmas = {'X': (mu_x, sd_x), 'Y': (mu_y, sd_y)}\n",
    "\n",
    "    # fit CCA on TRAIN only\n",
    "    k = int(n_components)\n",
    "    cca = CCA(n_components=k, max_iter=max_iter)\n",
    "    Xc_tr, Yc_tr = cca.fit_transform(X_tr, Y_tr)         # (n_train, k), (n_train, k)\n",
    "\n",
    "    # canonical correlations on TRAIN\n",
    "    corrs_tr = np.array([np.corrcoef(Xc_tr[:, i], Yc_tr[:, i])[0, 1] for i in range(Xc_tr.shape[1])])\n",
    "    # transform TEST into the SAME canonical axes\n",
    "    Xc_te, Yc_te = cca.transform(X_te, Y_te)\n",
    "\n",
    "    # canonical correlations on TEST (no re-fit)\n",
    "    corrs_te = np.array([np.corrcoef(Xc_te[:, i], Yc_te[:, i])[0, 1] for i in range(Xc_te.shape[1])])\n",
    "\n",
    "    # learn mapping in canonical space on TRAIN\n",
    "    if map_type == 'diag':\n",
    "        # per-dim least-squares gains: minimize ||Xc - A_diag Yc||^2\n",
    "        gains = []\n",
    "        for i in range(Xc_tr.shape[1]):\n",
    "            y = Yc_tr[:, i]\n",
    "            x = Xc_tr[:, i]\n",
    "            num = float(np.dot(y, x))\n",
    "            den = float(np.dot(y, y)) + eps\n",
    "            gains.append(num / den)\n",
    "        A = np.diag(gains)\n",
    "    elif map_type == 'full':\n",
    "        # unconstrained least squares in canonical space\n",
    "        # solve Yc_tr @ A ≈ Xc_tr  =>  A = argmin ||Yc_tr A - Xc_tr||\n",
    "        A, _, _, _ = np.linalg.lstsq(Yc_tr, Xc_tr, rcond=None)\n",
    "    else:\n",
    "        raise ValueError('map_type must be \"diag\" or \"full\"')\n",
    "\n",
    "    # VE-style score on TEST in the learned canonical subspace\n",
    "    # 1 - ||Xc_te - A Yc_te||_F^2 / ||Xc_te||_F^2\n",
    "    X_hat_can_te = Yc_te @ A\n",
    "    den = float(np.sum(Xc_te**2))\n",
    "    if not np.isfinite(den) or den <= eps:\n",
    "        ve_test = 0.0\n",
    "    else:\n",
    "        num = float(np.sum((Xc_te - X_hat_can_te)**2))\n",
    "        ve_test = 1.0 - (num / den)\n",
    "        ve_test = float(np.clip(ve_test, 0.0, 1.0))\n",
    "\n",
    "    return {\n",
    "        've_test': ve_test,\n",
    "        'corrs_train': corrs_tr,\n",
    "        'corrs_test': corrs_te,\n",
    "        'k_eff': int(Xc_tr.shape[1]),\n",
    "        'A': A,\n",
    "        'cca': cca,\n",
    "        'mu_sigmas': mu_sigmas\n",
    "    }\n",
    "\n",
    "def kfold_cca_pop_metric(\n",
    "    X, Y, *, n_components=10, n_splits=5, standardize=True, map_type='diag', rng=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience K-fold wrapper returning fold-wise VE and canonical correlation spectra.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=rng)\n",
    "    ve_list, corrs_tr_list, corrs_te_list = [], [], []\n",
    "    for tr, te in kf.split(X):\n",
    "        out = cca_pop_metric_train_test(\n",
    "            X[tr], Y[tr], X[te], Y[te],\n",
    "            n_components=n_components, standardize=standardize, map_type=map_type\n",
    "        )\n",
    "        ve_list.append(out['ve_test'])\n",
    "        corrs_tr_list.append(out['corrs_train'])\n",
    "        corrs_te_list.append(out['corrs_test'])\n",
    "    # pad corr arrays if n_components varies (it shouldn’t with sklearn CCA)\n",
    "    corrs_tr = np.vstack(corrs_tr_list)\n",
    "    corrs_te = np.vstack(corrs_te_list)\n",
    "    return {\n",
    "        've_mean': float(np.mean(ve_list)),\n",
    "        've_std':  float(np.std(ve_list)),\n",
    "        'corrs_train_mean': np.nanmean(corrs_tr, axis=0),\n",
    "        'corrs_test_mean':  np.nanmean(corrs_te, axis=0),\n",
    "        'corrs_train_all': corrs_tr,\n",
    "        'corrs_test_all':  corrs_te\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, X_hat: (samples, neurons) aligned in time; split by trial/stop for CV!\n",
    "cv = kfold_cca_pop_metric(\n",
    "    X, X_hat,\n",
    "    n_components=10,\n",
    "    n_splits=5,\n",
    "    standardize=True,\n",
    "    map_type='diag'   # try 'full' as a sensitivity check\n",
    ")\n",
    "\n",
    "print('CCA-subspace VE (mean±sd):', cv['ve_mean'], cv['ve_std'])\n",
    "print('Mean canonical correlations (train):', cv['corrs_train_mean'])\n",
    "print('Mean canonical correlations (test): ', cv['corrs_test_mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "## RRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import lstsq, svd\n",
    "\n",
    "def rrr_fit(X_tr, Y_tr, rank, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Fit reduced-rank regression: X ≈ Y @ B,  rank(B) ≤ rank.\n",
    "    Returns B (features_Y × features_X) and centering stats.\n",
    "    \"\"\"\n",
    "    X_tr = np.asarray(X_tr, float); Y_tr = np.asarray(Y_tr, float)\n",
    "    mx = X_tr.mean(0, keepdims=True); my = Y_tr.mean(0, keepdims=True)\n",
    "    Xc = X_tr - mx; Yc = Y_tr - my\n",
    "\n",
    "    # Full OLS then truncate via SVD (optimal rank-r in Frobenius norm)\n",
    "    B_ols, *_ = lstsq(Yc, Xc, rcond=None)            # (pY × pX)\n",
    "    U, s, Vt = svd(B_ols, full_matrices=False)\n",
    "    r = int(max(1, min(rank, np.sum(s > eps))))\n",
    "    B_rr = (U[:, :r] @ np.diag(s[:r]) @ Vt[:r, :])    # rank-r projection of B_ols\n",
    "    return {'B': B_rr, 'mx': mx, 'my': my, 'rank': r}\n",
    "\n",
    "def rrr_predict(fit, Y):\n",
    "    Y = np.asarray(Y, float)\n",
    "    return (Y - fit['my']) @ fit['B'] + fit['mx']\n",
    "\n",
    "def ve_fro(X, Xhat, eps=1e-12):\n",
    "    X = np.asarray(X, float); Xhat = np.asarray(Xhat, float)\n",
    "    num = np.sum((X - Xhat)**2)\n",
    "    den = np.sum((X - X.mean(0, keepdims=True))**2)\n",
    "    if not np.isfinite(den) or den <= eps: return 0.0\n",
    "    ve = 1.0 - num / den\n",
    "    return float(np.clip(ve, 0.0, 1.0))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def rrr_cv(X, Y, ranks=(1,2,3,5,8,12,16), n_splits=5, rng=0):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=rng)\n",
    "    scores = {r: [] for r in ranks}\n",
    "    for tr, te in kf.split(X):\n",
    "        for r in ranks:\n",
    "            fit = rrr_fit(X[tr], Y[tr], rank=r)\n",
    "            Xhat_te = rrr_predict(fit, Y[te])\n",
    "            scores[r].append(ve_fro(X[te], Xhat_te))\n",
    "    means = {r: float(np.mean(v)) for r, v in scores.items()}\n",
    "    best_rank = max(means, key=means.get)\n",
    "    return best_rank, means, scores\n",
    "\n",
    "# Split by trials/stops, not random bins (to avoid leakage).\n",
    "best_r, mean_by_r, cv_scores = rrr_cv(X, X_hat, ranks=(1,2,3,5,8,12), n_splits=5, rng=0)\n",
    "print('Best rank:', best_r, 'CV VE:', mean_by_r[best_r])\n",
    "\n",
    "# Train final model at best rank on all data, or retrain on a train split and evaluate on held-out:\n",
    "fit = rrr_fit(X, X_hat, rank=best_r)\n",
    "Xhat_rr = rrr_predict(fit, X_hat)\n",
    "ve_total = ve_fro(X, Xhat_rr)\n",
    "print('RRR VE (all data, optimistic):', ve_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "## CCA shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def cca_shuffle_test(X, Y, n_components=10, n_splits=100, shuffle_axis=0, random_state=0):\n",
    "    \"\"\"\n",
    "    Permutation test for canonical correlations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : arrays (samples × neurons), same shape\n",
    "    n_components : # of CCA dimensions to compute\n",
    "    n_splits : number of permutations\n",
    "    shuffle_axis : 0 = shuffle rows (samples/trials), 1 = shuffle cols (neurons)\n",
    "    random_state : reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrs_real : array, canonical correlations from true data\n",
    "    thresh : array, 95th percentile across shuffles for each dim\n",
    "    sig_dims : list, indices of significant canonical dims\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # fit on true data\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    Xc, Yc = cca.fit_transform(X, Y)\n",
    "    corrs_real = np.array([np.corrcoef(Xc[:, i], Yc[:, i])[0, 1] for i in range(n_components)])\n",
    "\n",
    "    # shuffle null\n",
    "    corrs_shuff = []\n",
    "    for _ in range(n_splits):\n",
    "        if shuffle_axis == 0:\n",
    "            Y_perm = rng.permutation(Y)      # shuffle rows\n",
    "        else:\n",
    "            Y_perm = Y.copy()\n",
    "            rng.shuffle(Y_perm.T)           # shuffle columns independently\n",
    "        Xs, Ys = cca.fit_transform(X, Y_perm)\n",
    "        corrs = [np.corrcoef(Xs[:, i], Ys[:, i])[0, 1] for i in range(n_components)]\n",
    "        corrs_shuff.append(corrs)\n",
    "    corrs_shuff = np.vstack(corrs_shuff)\n",
    "\n",
    "    # significance threshold per dim\n",
    "    thresh = np.percentile(corrs_shuff, 95, axis=0)\n",
    "    sig_dims = [i for i, c in enumerate(corrs_real) if c > thresh[i]]\n",
    "\n",
    "    return corrs_real, thresh, sig_dims\n",
    "\n",
    "corrs_real, thresh, sig_dims = cca_shuffle_test(X, X_hat, n_components=10, n_splits=100)\n",
    "\n",
    "print(\"Canonical correlations:\", corrs_real)\n",
    "print(\"95% shuffle thresholds:\", thresh)\n",
    "print(\"Significant dims:\", sig_dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population_latent_benchmark.py\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Dict, Any, Tuple, List\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from numpy.linalg import lstsq, svd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- small utils ----------\n",
    "def _zscore(A: np.ndarray, eps: float = 1e-12) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    mu = np.nanmean(A, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(A, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (A - mu) / sd, mu, sd\n",
    "\n",
    "def _fro_ve(X: np.ndarray, Xhat: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    num = np.sum((X - Xhat) ** 2)\n",
    "    den = np.sum((X - np.mean(X, axis=0, keepdims=True)) ** 2)\n",
    "    if not np.isfinite(den) or den <= eps:\n",
    "        return 0.0\n",
    "    ve = 1.0 - (num / den)\n",
    "    return float(np.clip(ve, 0.0, 1.0))\n",
    "\n",
    "def _safe_mask_pair(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    m = np.isfinite(X).all(1) & np.isfinite(Y).all(1)\n",
    "    return X[m], Y[m], m\n",
    "\n",
    "# ---------- RRR ----------\n",
    "def rrr_fit(X_tr: np.ndarray, Y_tr: np.ndarray, rank: int, eps: float = 1e-12) -> Dict[str, Any]:\n",
    "    X_tr = np.asarray(X_tr, float); Y_tr = np.asarray(Y_tr, float)\n",
    "    mx = X_tr.mean(0, keepdims=True); my = Y_tr.mean(0, keepdims=True)\n",
    "    Xc = X_tr - mx; Yc = Y_tr - my\n",
    "    B_ols, *_ = lstsq(Yc, Xc, rcond=None)               # (pY × pX)\n",
    "    U, s, Vt = svd(B_ols, full_matrices=False)\n",
    "    r = int(max(1, min(rank, np.sum(s > eps))))\n",
    "    B_rr = U[:, :r] @ np.diag(s[:r]) @ Vt[:r, :]\n",
    "    return {'B': B_rr, 'mx': mx, 'my': my, 'rank': r}\n",
    "\n",
    "def rrr_predict(fit: Dict[str, Any], Y: np.ndarray) -> np.ndarray:\n",
    "    return (np.asarray(Y, float) - fit['my']) @ fit['B'] + fit['mx']\n",
    "\n",
    "def rrr_cv(X: np.ndarray, Y: np.ndarray, groups: np.ndarray,\n",
    "           ranks: Iterable[int] = (1, 2, 3, 5, 8, 12, 16),\n",
    "           n_splits: int = 5, rng: int = 0) -> Dict[str, Any]:\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    ranks = list(ranks)\n",
    "    ve_mat = {r: [] for r in ranks}\n",
    "    for tr_idx, te_idx in gkf.split(X, groups=groups):\n",
    "        for r in ranks:\n",
    "            fit = rrr_fit(X[tr_idx], Y[tr_idx], rank=r)\n",
    "            Xhat_te = rrr_predict(fit, Y[te_idx])\n",
    "            ve_mat[r].append(_fro_ve(X[te_idx], Xhat_te))\n",
    "    ve_mean = {r: float(np.mean(v)) for r, v in ve_mat.items()}\n",
    "    best_rank = max(ve_mean, key=ve_mean.get)\n",
    "    return {'ve_by_rank': ve_mean, 'best_rank': best_rank, 'cv_scores': ve_mat}\n",
    "\n",
    "# ---------- CCA (train/test safe) ----------\n",
    "def cca_train_test(X_tr: np.ndarray, Y_tr: np.ndarray,\n",
    "                   X_te: np.ndarray, Y_te: np.ndarray,\n",
    "                   n_components: int = 10, standardize: bool = True,\n",
    "                   map_type: str = 'diag', max_iter: int = 1000,\n",
    "                   eps: float = 1e-12) -> Dict[str, Any]:\n",
    "    if standardize:\n",
    "        X_tr, mux, sdx = _zscore(X_tr); X_te = (X_te - mux) / sdx\n",
    "        Y_tr, muy, sdy = _zscore(Y_tr); Y_te = (Y_te - muy) / sdy\n",
    "    cca = CCA(n_components=int(n_components), max_iter=max_iter)\n",
    "    Xc_tr, Yc_tr = cca.fit_transform(X_tr, Y_tr)\n",
    "    Xc_te, Yc_te = cca.transform(X_te, Y_te)\n",
    "\n",
    "    corrs_tr = np.array([np.corrcoef(Xc_tr[:, i], Yc_tr[:, i])[0, 1] for i in range(Xc_tr.shape[1])])\n",
    "    corrs_te = np.array([np.corrcoef(Xc_te[:, i], Yc_te[:, i])[0, 1] for i in range(Xc_te.shape[1])])\n",
    "\n",
    "    if map_type == 'diag':\n",
    "        gains = []\n",
    "        for i in range(Xc_tr.shape[1]):\n",
    "            num = float(np.dot(Yc_tr[:, i], Xc_tr[:, i]))\n",
    "            den = float(np.dot(Yc_tr[:, i], Yc_tr[:, i])) + eps\n",
    "            gains.append(num / den)\n",
    "        A = np.diag(gains)\n",
    "    elif map_type == 'full':\n",
    "        A, *_ = lstsq(Yc_tr, Xc_tr, rcond=None)\n",
    "    else:\n",
    "        raise ValueError('map_type must be \"diag\" or \"full\"')\n",
    "\n",
    "    Xc_hat_te = Yc_te @ A\n",
    "    den = float(np.sum(Xc_te ** 2))\n",
    "    ve_te = 0.0 if den <= eps else float(np.clip(1.0 - np.sum((Xc_te - Xc_hat_te) ** 2) / den, 0.0, 1.0))\n",
    "    return {'corrs_train': corrs_tr, 'corrs_test': corrs_te, 've_test': ve_te, 'A': A, 'cca': cca}\n",
    "\n",
    "def cca_shuffle_test_on_test(X_te: np.ndarray, Y_te: np.ndarray, cca_obj: CCA,\n",
    "                             n_components: int, n_shuffles: int = 100,\n",
    "                             rng: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Use fixed CCA axes (trained on train) to avoid leakage\n",
    "    rng = np.random.default_rng(rng)\n",
    "    Xc_te, Yc_te = cca_obj.transform(X_te, Y_te)\n",
    "    k = int(n_components)\n",
    "    corrs_real = np.array([np.corrcoef(Xc_te[:, i], Yc_te[:, i])[0, 1] for i in range(k)])\n",
    "    null = []\n",
    "    for _ in range(n_shuffles):\n",
    "        Yp = rng.permutation(Y_te)  # row-shuffle on test\n",
    "        _, Yc_perm = cca_obj.transform(X_te, Yp)\n",
    "        null.append([np.corrcoef(Xc_te[:, i], Yc_perm[:, i])[0, 1] for i in range(k)])\n",
    "    null = np.vstack(null)\n",
    "    thresh = np.percentile(null, 95, axis=0)\n",
    "    return corrs_real, thresh\n",
    "\n",
    "# ---------- FA latent metric ----------\n",
    "def fa_train_test(X_tr: np.ndarray, Y_tr: np.ndarray,\n",
    "                  X_te: np.ndarray, Y_te: np.ndarray,\n",
    "                  n_factors: int = 8, standardize: bool = True,\n",
    "                  map_type: str = 'full', eps: float = 1e-12) -> Dict[str, Any]:\n",
    "    if standardize:\n",
    "        X_tr, mux, sdx = _zscore(X_tr); X_te = (X_te - mux) / sdx\n",
    "        Y_tr, muy, sdy = _zscore(Y_tr); Y_te = (Y_te - muy) / sdy\n",
    "    fa = FactorAnalysis(n_components=int(n_factors), rotation=None)\n",
    "    Zx_tr = fa.fit_transform(X_tr); Zx_te = fa.transform(X_te)\n",
    "    Zy_tr = fa.transform(Y_tr);    Zy_te = fa.transform(Y_te)\n",
    "    if map_type == 'diag':\n",
    "        gains = []\n",
    "        for i in range(Zx_tr.shape[1]):\n",
    "            den = float(np.dot(Zy_tr[:, i], Zy_tr[:, i])) + eps\n",
    "            gains.append(float(np.dot(Zy_tr[:, i], Zx_tr[:, i])) / den)\n",
    "        A = np.diag(gains)\n",
    "    elif map_type == 'full':\n",
    "        A, *_ = lstsq(Zy_tr, Zx_tr, rcond=None)\n",
    "    else:\n",
    "        raise ValueError('map_type must be \"diag\" or \"full\"')\n",
    "    Zx_hat_te = Zy_te @ A\n",
    "    den = float(np.sum((Zx_te - Zx_te.mean(0)) ** 2))\n",
    "    ve_lat = 0.0 if den <= eps else float(np.clip(1.0 - np.sum((Zx_te - Zx_hat_te) ** 2) / den, 0.0, 1.0))\n",
    "    # per-factor test correlations\n",
    "    corrs = []\n",
    "    for i in range(Zx_te.shape[1]):\n",
    "        xi, yi = Zx_te[:, i], Zx_hat_te[:, i]\n",
    "        si = np.std(xi); sj = np.std(yi)\n",
    "        corrs.append(0.0 if si < 1e-12 or sj < 1e-12 else float(np.corrcoef(xi, yi)[0, 1]))\n",
    "    return {'ve_latent_test': ve_lat, 'corrs_latent_test': np.array(corrs), 'A': A, 'fa': fa}\n",
    "\n",
    "# ---------- Main benchmark ----------\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    fa_ve_mean: float\n",
    "    fa_ve_std: float\n",
    "    cca_ve_mean: float\n",
    "    cca_ve_std: float\n",
    "    cca_corr_test_mean: np.ndarray\n",
    "    cca_corr_thresh95: np.ndarray\n",
    "    rrr_best_rank: int\n",
    "    rrr_ve_by_rank: Dict[int, float]\n",
    "    plots: Dict[str, Any]    # matplotlib Figure objects\n",
    "\n",
    "def population_latent_benchmark(X: np.ndarray, Y: np.ndarray, trial_ids: np.ndarray,\n",
    "                                *, n_splits: int = 5,\n",
    "                                fa_factors: int = 8,\n",
    "                                cca_components: int = 10,\n",
    "                                rrr_ranks: Iterable[int] = (1, 2, 3, 5, 8, 12, 16),\n",
    "                                shuffle_reps: int = 100,\n",
    "                                rng: int = 0) -> BenchmarkResult:\n",
    "    X = np.asarray(X, float); Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, 'X and Y must have same shape'\n",
    "    groups = np.asarray(trial_ids)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    fa_ves, cca_ves, cca_corrs_list, cca_thresh_list = [], [], [], []\n",
    "\n",
    "    # RRR CV across ranks (once; GroupKFold inside)\n",
    "    rrr = rrr_cv(X, Y, groups=groups, ranks=rrr_ranks, n_splits=n_splits, rng=rng)\n",
    "\n",
    "    # Fold loop for FA & CCA\n",
    "    for fold, (tr_idx, te_idx) in enumerate(gkf.split(X, groups=groups)):\n",
    "        Xtr, Ytr = X[tr_idx], Y[tr_idx]\n",
    "        Xte, Yte = X[te_idx], Y[te_idx]\n",
    "\n",
    "        # FA latent VE\n",
    "        fa_out = fa_train_test(Xtr, Ytr, Xte, Yte, n_factors=fa_factors, standardize=True, map_type='full')\n",
    "        fa_ves.append(fa_out['ve_latent_test'])\n",
    "\n",
    "        # CCA subspace VE + test corr spectrum + shuffle baseline\n",
    "        cca_out = cca_train_test(Xtr, Ytr, Xte, Yte, n_components=cca_components, standardize=True, map_type='diag')\n",
    "        cca_ves.append(cca_out['ve_test'])\n",
    "\n",
    "        corrs_real, thresh = cca_shuffle_test_on_test(Xte, Yte, cca_out['cca'],\n",
    "                                                      n_components=cca_components,\n",
    "                                                      n_shuffles=shuffle_reps, rng=rng + fold)\n",
    "        cca_corrs_list.append(corrs_real)\n",
    "        cca_thresh_list.append(thresh)\n",
    "\n",
    "    fa_ve_mean, fa_ve_std = float(np.mean(fa_ves)), float(np.std(fa_ves))\n",
    "    cca_ve_mean, cca_ve_std = float(np.mean(cca_ves)), float(np.std(cca_ves))\n",
    "    cca_corr_test_mean = np.mean(np.vstack(cca_corrs_list), axis=0)\n",
    "    cca_corr_thresh95 = np.mean(np.vstack(cca_thresh_list), axis=0)\n",
    "\n",
    "    # ---------- Plots ----------\n",
    "    figs: Dict[str, Any] = {}\n",
    "\n",
    "    # 1) CCA spectrum vs shuffle\n",
    "    fig1 = plt.figure(figsize=(4, 3))\n",
    "    x = np.arange(1, len(cca_corr_test_mean) + 1)\n",
    "    plt.plot(x, cca_corr_test_mean, marker='o', label='Test corr')\n",
    "    plt.plot(x, cca_corr_thresh95, linestyle='--', label='Shuffle 95%')\n",
    "    plt.xlabel('Canonical component')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.title('CCA test spectrum')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    figs['cca_spectrum'] = fig1\n",
    "\n",
    "    # 2) VE summary bars\n",
    "    fig2 = plt.figure(figsize=(4, 3))\n",
    "    names = ['FA-latent VE', 'CCA-VE', f'RRR-VE@r={rrr[\"best_rank\"]}']\n",
    "    vals = [fa_ve_mean, cca_ve_mean, rrr['ve_by_rank'][rrr['best_rank']]]\n",
    "    y = np.arange(len(names))\n",
    "    plt.bar(y, vals)\n",
    "    plt.xticks(y, names, rotation=20)\n",
    "    plt.ylim(0, max(0.01, max(vals) * 1.2))\n",
    "    plt.ylabel('Variance explained')\n",
    "    plt.title('Population VE summary')\n",
    "    plt.tight_layout()\n",
    "    figs['ve_summary'] = fig2\n",
    "\n",
    "    # 3) RRR VE vs rank\n",
    "    fig3 = plt.figure(figsize=(4, 3))\n",
    "    rs = sorted(rrr['ve_by_rank'].keys())\n",
    "    vrs = [rrr['ve_by_rank'][r] for r in rs]\n",
    "    plt.plot(rs, vrs, marker='o')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('CV VE')\n",
    "    plt.title('RRR VE vs rank')\n",
    "    plt.tight_layout()\n",
    "    figs['rrr_curve'] = fig3\n",
    "\n",
    "    return BenchmarkResult(\n",
    "        fa_ve_mean=fa_ve_mean,\n",
    "        fa_ve_std=fa_ve_std,\n",
    "        cca_ve_mean=cca_ve_mean,\n",
    "        cca_ve_std=cca_ve_std,\n",
    "        cca_corr_test_mean=cca_corr_test_mean,\n",
    "        cca_corr_thresh95=cca_corr_thresh95,\n",
    "        rrr_best_rank=rrr['best_rank'],\n",
    "        rrr_ve_by_rank=rrr['ve_by_rank'],\n",
    "        plots=figs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = observed (samples × neurons)\n",
    "# X_hat = predictions (same shape)\n",
    "# trial_ids = your per-row trial/segment ids\n",
    "res = population_latent_benchmark(X, X_hat, trial_ids,\n",
    "                                  n_splits=5,\n",
    "                                  fa_factors=8,\n",
    "                                  cca_components=10,\n",
    "                                  rrr_ranks=(1,2,3,5,8,12,16),\n",
    "                                  shuffle_reps=100,\n",
    "                                  rng=0)\n",
    "\n",
    "print('FA-latent VE (mean±sd):', res.fa_ve_mean, res.fa_ve_std)\n",
    "print('CCA-VE (mean±sd):      ', res.cca_ve_mean, res.cca_ve_std)\n",
    "print('RRR best rank:', res.rrr_best_rank)\n",
    "print('RRR VE by rank:', res.rrr_ve_by_rank)\n",
    "\n",
    "# show or save figures\n",
    "for name, fig in res.plots.items():\n",
    "    fig.show()           # or fig.savefig(f'{name}.png', dpi=200, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "## CCA latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def _zscore(A, eps=1e-12):\n",
    "    mu = np.nanmean(A, axis=0, keepdims=True)\n",
    "    sd = np.nanstd(A, axis=0, keepdims=True)\n",
    "    sd = np.where(sd < eps, 1.0, sd)\n",
    "    return (A - mu) / sd, mu, sd\n",
    "\n",
    "def _sem(a, axis=0):\n",
    "    a = np.asarray(a, float)\n",
    "    n = np.sum(np.isfinite(a), axis=axis)\n",
    "    n = np.maximum(n, 1)\n",
    "    return np.nanstd(a, axis=axis) / np.sqrt(n)\n",
    "\n",
    "def _smooth_1d(x, k=1):\n",
    "    if k <= 1: \n",
    "        return x\n",
    "    k = int(k)\n",
    "    kern = np.ones(k) / k\n",
    "    return np.convolve(x, kern, mode='same')\n",
    "\n",
    "def visualize_shared_manifold(\n",
    "    X, Y, trial_ids, time_idx, cond=None, *,\n",
    "    n_components=3,\n",
    "    n_splits=5,\n",
    "    which_fold=0,            # pick which fold to visualize\n",
    "    standardize=True,\n",
    "    smooth_bins=1,           # boxcar smoothing across time for the mean curves\n",
    "    colors=('C0','C1'),      # (observed, predicted)\n",
    "    show_phase_plane=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit CCA on TRAIN trials, project TEST trials into canonical space, and plot\n",
    "    observed vs predicted trajectories (mean±SEM) over time for the first components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : arrays (samples × neurons), same shape\n",
    "    trial_ids : array (samples,)\n",
    "    time_idx : array (samples,) time within aligned window (e.g., -1.5..+1.5s or bin index)\n",
    "    cond : optional array-like (samples,), categorical condition label per sample\n",
    "    n_components : # of canonical components to visualize\n",
    "    n_splits : GroupKFold splits by trial_ids\n",
    "    which_fold : index of fold to visualize (0..n_splits-1)\n",
    "    standardize : z-score each view prior to CCA\n",
    "    smooth_bins : boxcar width for smoothing mean curves (in bins)\n",
    "    colors : tuple for (observed, predicted) lines\n",
    "    show_phase_plane : also plot Comp1 vs Comp2 phase-plane (obs vs pred)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, \"X and Y must have same shape\"\n",
    "\n",
    "    # group-wise split by trials\n",
    "    trial_ids = np.asarray(trial_ids)\n",
    "    time_idx = np.asarray(time_idx)\n",
    "    if cond is None:\n",
    "        cond = np.array(['all'] * len(time_idx))\n",
    "    else:\n",
    "        cond = np.asarray(cond)\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    splits = list(gkf.split(X, groups=trial_ids))\n",
    "    tr_idx, te_idx = splits[which_fold]\n",
    "\n",
    "    Xtr, Ytr = X[tr_idx], Y[tr_idx]\n",
    "    Xte, Yte = X[te_idx], Y[te_idx]\n",
    "    trial_te = trial_ids[te_idx]\n",
    "    time_te  = time_idx[te_idx]\n",
    "    cond_te  = cond[te_idx]\n",
    "\n",
    "    # standardize each view\n",
    "    if standardize:\n",
    "        Xtr, mux, sdx = _zscore(Xtr)\n",
    "        Ytr, muy, sdy = _zscore(Ytr)\n",
    "        Xte = (Xte - mux) / sdx\n",
    "        Yte = (Yte - muy) / sdy\n",
    "\n",
    "    # fit CCA on TRAIN only\n",
    "    k = int(n_components)\n",
    "    cca = CCA(n_components=k, max_iter=1000)\n",
    "    Xc_tr, Yc_tr = cca.fit_transform(Xtr, Ytr)\n",
    "    Xc_te, Yc_te = cca.transform(Xte, Yte)\n",
    "\n",
    "    # package canonical coordinates for TEST into a tidy DataFrame\n",
    "    dfs = []\n",
    "    for view_name, Z in [('obs', Xc_te), ('pred', Yc_te)]:\n",
    "        df = pd.DataFrame({\n",
    "            'trial': trial_te,\n",
    "            'time': time_te,\n",
    "            'cond': cond_te,\n",
    "        })\n",
    "        for i in range(k):\n",
    "            df[f'can{i+1}'] = Z[:, i]\n",
    "        df['view'] = view_name\n",
    "        dfs.append(df)\n",
    "    df_all = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # aggregate mean ± SEM over trials at each time, by condition & view\n",
    "    # we align on time bins; ensure numeric sorting\n",
    "    df_all = df_all.sort_values(['cond', 'view', 'time'])\n",
    "    results = {}\n",
    "    for c in np.unique(cond_te):\n",
    "        results[c] = {}\n",
    "        dsub = df_all[df_all['cond'] == c]\n",
    "        # group by time & view, compute mean/sem per component\n",
    "        gb = dsub.groupby(['view', 'time'])\n",
    "        means = gb.mean(numeric_only=True)\n",
    "        sems  = gb.apply(lambda g: pd.Series({\n",
    "            **{f'can{i+1}': _sem(g[f'can{i+1}'].values) for i in range(k)}\n",
    "        }))\n",
    "        # pivot to have time index, separate views\n",
    "        means = means.reset_index().pivot(index='time', columns='view', values=[f'can{i+1}' for i in range(k)])\n",
    "        sems  = sems.reset_index().pivot(index='time', columns='view', values=[f'can{i+1}' for i in range(k)])\n",
    "        # sort by time\n",
    "        means = means.sort_index()\n",
    "        sems  = sems.sort_index()\n",
    "        results[c]['mean'] = means\n",
    "        results[c]['sem']  = sems\n",
    "\n",
    "    # --- plotting ---\n",
    "    figs = []\n",
    "    # 1) Trajectories for each canonical component\n",
    "    for i in range(k):\n",
    "        fig = plt.figure(figsize=(5, 3.2))\n",
    "        ax = plt.gca()\n",
    "        for j, c in enumerate(results.keys()):\n",
    "            means = results[c]['mean'][(f'can{i+1}',)]\n",
    "            sems  = results[c]['sem'][(f'can{i+1}',)]\n",
    "            for view_idx, view in enumerate(['obs','pred']):\n",
    "                y = means[view].values\n",
    "                s = sems[view].values\n",
    "                if smooth_bins > 1:\n",
    "                    y = _smooth_1d(y, smooth_bins)\n",
    "                    s = _smooth_1d(s, smooth_bins)\n",
    "                t = means.index.values\n",
    "                ax.plot(t, y, label=f'{c} — {view}', \n",
    "                        linestyle='-' if view=='obs' else '--',\n",
    "                        color=colors[view_idx])\n",
    "                ax.fill_between(t, y - s, y + s, alpha=0.15, color=colors[view_idx])\n",
    "        ax.set_title(f'Canonical component {i+1} (fold {which_fold+1}/{n_splits})')\n",
    "        ax.set_xlabel('Time (aligned bins)')\n",
    "        ax.set_ylabel('Canonical coordinate')\n",
    "        ax.legend(ncol=2, fontsize=8)\n",
    "        ax.axvline(0, color='k', linewidth=1, alpha=0.2)\n",
    "        ax.grid(alpha=0.2, linestyle=':')\n",
    "        plt.tight_layout()\n",
    "        figs.append(fig)\n",
    "\n",
    "    # 2) Phase-plane plot (Comp1 vs Comp2)\n",
    "    if show_phase_plane and k >= 2:\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        ax = plt.gca()\n",
    "        for j, c in enumerate(results.keys()):\n",
    "            m1 = results[c]['mean'][('can1',)]\n",
    "            m2 = results[c]['mean'][('can2',)]\n",
    "            t = m1.index.values\n",
    "            for view_idx, view in enumerate(['obs','pred']):\n",
    "                x = m1[view].values\n",
    "                y = m2[view].values\n",
    "                if smooth_bins > 1:\n",
    "                    x = _smooth_1d(x, smooth_bins)\n",
    "                    y = _smooth_1d(y, smooth_bins)\n",
    "                ax.plot(x, y, label=f'{c} — {view}',\n",
    "                        linestyle='-' if view=='obs' else '--',\n",
    "                        color=colors[view_idx])\n",
    "                # mark time zero if present\n",
    "                if 0 in t:\n",
    "                    idx0 = np.where(t==0)[0]\n",
    "                    if len(idx0): \n",
    "                        ax.scatter(x[idx0[0]], y[idx0[0]], s=30, color=colors[view_idx])\n",
    "        ax.set_xlabel('Can1')\n",
    "        ax.set_ylabel('Can2')\n",
    "        ax.set_title(f'Phase plane (fold {which_fold+1}/{n_splits})')\n",
    "        ax.legend(ncol=2, fontsize=8)\n",
    "        ax.grid(alpha=0.2, linestyle=':')\n",
    "        plt.tight_layout()\n",
    "        figs.append(fig)\n",
    "\n",
    "    return {'figs': figs, 'cca': cca}\n",
    "\n",
    "# X, X_hat: (samples, neurons)\n",
    "# trial_ids: trial/segment identifier per row (for GroupKFold)\n",
    "# time_idx: bin index (e.g., -30..+30) or seconds relative to event\n",
    "# cond: optional labels (e.g., visibility state or stop outcome)\n",
    "out = visualize_shared_manifold(\n",
    "    X, X_hat, trial_ids, data['new_bin'], cond=None,\n",
    "    n_components=3, n_splits=5, which_fold=0,\n",
    "    standardize=True, smooth_bins=3, show_phase_plane=True\n",
    ")\n",
    "# Show or save figures:\n",
    "for i, fig in enumerate(out['figs'], 1):\n",
    "    fig.show()  # or fig.savefig(f'shared_manifold_{i}.png', dpi=200, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def population_ve_cca(X, Y, n_components=10):\n",
    "    \"\"\"\n",
    "    Canonical correlation analysis between observed (X) and predicted (Y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (samples, neurons)\n",
    "    Y : array, shape (samples, neurons) -- same shape as X\n",
    "    n_components : number of canonical pairs to compute\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrs : array of canonical correlations (length n_components)\n",
    "    mean_corr : average canonical correlation across components\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    Y = np.asarray(Y, float)\n",
    "    assert X.shape == Y.shape, 'X and Y must have same shape'\n",
    "\n",
    "    cca = CCA(n_components=n_components, max_iter=1000)\n",
    "    Xc, Yc = cca.fit_transform(X, Y)\n",
    "    corrs = [np.corrcoef(Xc[:,i], Yc[:,i])[0,1] for i in range(Xc.shape[1])]\n",
    "    return np.array(corrs), np.mean(corrs)\n",
    "\n",
    "corrs, mean_corr = population_ve_cca(X, X_hat, n_components=10)\n",
    "print('Canonical correlations:', corrs)\n",
    "print('Mean correlation:', mean_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "# tuning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_tuning_func)\n",
    "\n",
    "cluster_id = 10\n",
    "model_res = report['results'][cluster_id]\n",
    "var = 'captured'\n",
    "\n",
    "exposure_s = np.exp(offset_log)\n",
    "# make empirical curve\n",
    "tc_emp = plot_tuning_func.empirical_tuning_curve(\n",
    "    binned_spikes=binned_spikes[cluster_id].to_numpy(),\n",
    "    predictor_vals=binned_feats[var].to_numpy(),\n",
    "    exposure_s=exposure_s,\n",
    "    nbins=20\n",
    ")\n",
    "\n",
    "# make GLM curve\n",
    "tc_glm = plot_tuning_func.glm_tuning_curve(\n",
    "    model_res, df_X,\n",
    "    var=var,\n",
    "    offset_log=offset_log,\n",
    "    average='marginal',\n",
    "    weights=exposure_s,                 # time-weighted average rate (recommended)\n",
    "    return_ci=True\n",
    ")\n",
    "\n",
    "# overlay\n",
    "plot_tuning_func.overlay_tuning_curves(tc_emp, tc_glm, xcol=var,\n",
    "                      title=f'Unit {cluster_id}: {var} tuning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tuning_func.plot_tuning_with_ci(tc_glm, xcol='captured', ycol='rate_hz')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {},
   "source": [
    "# plot pred_vs_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 5\n",
    "cv_stop_glm.plot_pred_vs_obs(report['results'][cluster_id], df_X, binned_spikes[cluster_id], offset_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "# check fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.04\n",
    "rates = binned_spikes.sum(axis=0) / (len(binned_spikes) * dt)\n",
    "# rates is a Series indexed by unit, in Hz\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def firing_rates_from_df(spikes_df, time_col='time', cluster_col='cluster'):\n",
    "    start_s = spikes_df[time_col].min()\n",
    "    end_s = spikes_df[time_col].max()\n",
    "    duration = end_s - start_s\n",
    "\n",
    "    counts = spikes_df.groupby(cluster_col).size()\n",
    "    rates_hz = counts / duration\n",
    "    return rates_hz.rename('rate_hz').reset_index()\n",
    "\n",
    "firing_rates_from_df(pn.spikes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sub = new_seg_info[new_seg_info['captured'] > 0]\n",
    "sns.histplot(new_seg_info['n_pre_bins'])\n",
    "sns.histplot(seg_sub['n_pre_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(new_seg_info['n_post_bins'])\n",
    "sns.histplot(seg_sub['n_post_bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "# plot_spaghetti_per_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "## run func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['event_id', 'rel_center', 't_left', 't_right']\n",
    "binned_spikes2 = binned_spikes.copy()\n",
    "binned_spikes2[cols] = meta_used[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a unit column by name or int (e.g., 3)\n",
    "# unit_col = 3  # or '3' if your columns are strings\n",
    "for unit_col in binned_spikes.columns:\n",
    "    df_rate = plot_spikes.make_rate_df_from_binned(binned_spikes2, unit_col)\n",
    "\n",
    "    # plot (with gentle smoothing and pre-stop baseline subtraction)\n",
    "    fig, ax, n = plot_spikes.plot_spaghetti_per_stop(\n",
    "        df_rate,\n",
    "        smooth_sigma_s=0.08,          # ~80 ms sigma (auto-converted to bins)\n",
    "        # baseline_window=(-0.5, -0.1), # subtract mean pre-stop activity\n",
    "        baseline_window=None,\n",
    "        max_stops=None,               # or an int to limit how many lines\n",
    "        median_label='median (all stops)',\n",
    "        title=f'Unit {unit_col}: rate per stop'\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f'Plotted {n} stops.')\n",
    "    \n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {},
   "source": [
    "# Hyperparam tuning (try different configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "## just elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(stop_glm_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res = stop_glm_fit.glm_mini_report(\n",
    "    df_X, df_Y, offset_log,\n",
    "    regularization='elasticnet',\n",
    "    # alpha_grid=(0.05, 0.1, 0.2, 0.5, 1.0),\n",
    "    # l1_wt_grid=(0.75, 0.5, 0.25, 0.0),\n",
    "    alpha_grid=(0.1, 0.2),\n",
    "    l1_wt_grid=(0.75, 0.5),\n",
    "    groups=cluster_df['event_cluster_id'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['cv_tables_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['metrics_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['figures']['rr_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all chosen alphas\n",
    "print(res['metrics_df']['alpha'].value_counts())\n",
    "\n",
    "# mean/median chosen alpha across clusters\n",
    "print(res['metrics_df']['alpha'].agg(['mean','median']))\n",
    "\n",
    "# joint distribution of alpha × l1_wt\n",
    "pd.crosstab(res['metrics_df']['alpha'], res['metrics_df']['l1_wt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "## configs (systematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    # 1) Ridge (ultra-light grid) ─ combos: ~18\n",
    "    dict(name='Poisson_Ridge_ultra',\n",
    "         regularization='elasticnet',\n",
    "         alpha_grid=tuple(np.concatenate([\n",
    "             np.logspace(-6, -4.7, 6),   # 1e-6 … ~2e-5\n",
    "             np.logspace(-4.7, -2.3, 12) # ~2e-5 … 5e-3\n",
    "         ])),\n",
    "         l1_wt_grid=(0.0,),                 # pure L2\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "         refit_on_support=False),\n",
    "\n",
    "    # 2) Lasso (ultra-light grid) + refit ─ combos: ~16\n",
    "    dict(name='Poisson_Lasso_ultra_refit',\n",
    "         regularization='elasticnet',\n",
    "         alpha_grid=tuple(np.concatenate([\n",
    "             np.logspace(-6, -4.7, 6),   # 1e-6 … ~2e-5\n",
    "             np.logspace(-4.7, -3.3, 10) # ~2e-5 … 5e-4\n",
    "         ])),\n",
    "         l1_wt_grid=(1.0,),                 # pure L1\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "         refit_on_support=True),\n",
    "    \n",
    "    \n",
    "    # 0) Plain Poisson MLE, robust SEs (baseline) ─ combos: 1\n",
    "    dict(name='Poisson_MLE_HC3',\n",
    "         regularization='none',\n",
    "         alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "         refit_on_support=False),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs = [\n",
    "#     # 0) Plain Poisson MLE, robust SEs (baseline)  ─ combos: 1\n",
    "#     dict(name='Poisson_MLE_HC3',\n",
    "#          regularization='none',\n",
    "#          alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "#          refit_on_support=False),\n",
    "\n",
    "#     # 1) Ridge (fine grid) ─ combos: 20\n",
    "#     dict(name='Poisson_Ridge_fine',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 2, 20)),\n",
    "#          l1_wt_grid=(0.0,),                 # pure L2\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=False),\n",
    "\n",
    "#     # 2) Lasso (fine) + refit ─ combos: 20\n",
    "#     dict(name='Poisson_Lasso_refit_fine',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 0, 20)),\n",
    "#          l1_wt_grid=(1.0,),                 # pure L1\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 3) Elastic-Net (mix) ─ combos: 14 × 5 = 70  ← HEAVY\n",
    "#     dict(name='Poisson_EN_fine',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 1, 14)),\n",
    "#          l1_wt_grid=(1.0, 0.75, 0.5, 0.25, 0.0),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 4) EN, time-aware CV ─ combos: 12 × 3 = 36\n",
    "#     dict(name='Poisson_EN_timecv',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 0, 12)),\n",
    "#          l1_wt_grid=(1.0, 0.5, 0.0),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True,\n",
    "#          cv_splitter='blocked_time'),\n",
    "\n",
    "#     # 5) Ridge, time-aware CV ─ combos: 16\n",
    "#     dict(name='Poisson_Ridge_timecv',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 1, 16)),\n",
    "#          l1_wt_grid=(0.0,),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=False,\n",
    "#          cv_splitter='blocked_time'),\n",
    "\n",
    "#     # 6) Lasso (HC3) + refit ─ combos: 16\n",
    "#     dict(name='Poisson_Lasso_HC3_refit',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 0, 16)),\n",
    "#          l1_wt_grid=(1.0,),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 7) EN (deviance CV metric) ─ combos: 10 × 4 = 40\n",
    "#     dict(name='Poisson_EN_devianceCV',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-3, 0.7, 10)),\n",
    "#          l1_wt_grid=(1.0, 0.5, 0.25, 0.0),\n",
    "#          cv_metric='deviance', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 8) Quasi-Poisson flavor (same mean; inflated SEs) ─ combos: 1\n",
    "#     dict(name='QuasiPoisson_like',\n",
    "#          regularization='none',\n",
    "#          alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "#          cv_metric='deviance', n_splits=5, cov_type='HC3',\n",
    "#          refit_on_support=False,\n",
    "#          use_overdispersion_scale=True),\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "## compare (w class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build your configs (as before)\n",
    "configs = [\n",
    "    dict(name='Poisson_MLE_HC3',\n",
    "         regularization='none',\n",
    "         alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "         refit_on_support=False),\n",
    "    dict(name='Poisson_Ridge_fine',\n",
    "         regularization='elasticnet',\n",
    "         alpha_grid=tuple(np.logspace(-4, 2, 20)),\n",
    "         l1_wt_grid=(0.0,),\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "         refit_on_support=False),\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# 2) Spin up the runner\n",
    "runner = glm_hyperparams_class.SweepAndCompare(\n",
    "    configs=configs,\n",
    "    df_X=df_X,\n",
    "    df_Y=df_Y,\n",
    "    offset_log=offset_log,\n",
    "    fit_fn=glm_hyperparams_class.fit_fn,                           # <-- the adapter above\n",
    "    feature_names=list(df_X.columns),\n",
    "    cluster_ids=list(df_Y.columns),\n",
    "    groups=cluster_df['event_cluster_id'].values,\n",
    "    cov_type='HC1',\n",
    "    out_dir='glm_sweep_compare_out',\n",
    "    autosave=True,\n",
    "    autosave_every=1,\n",
    "    extra_fit_kwargs=dict(\n",
    "        cv_splitter=None,                    # or 'blocked_time'\n",
    "        use_overdispersion_scale=False,\n",
    "        add_outer_cv_summary=True,           # optional: outer-CV summary\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 3) Run (and safely interrupt/resume any time)\n",
    "runner.run()\n",
    "summary_df = runner.get_summary()\n",
    "cv_tables_df   = runner.get_cv_tables()\n",
    "results = runner.per_config_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.per_config_results  # list of dicts (detailed)\n",
    "\n",
    "# resume later from checkpoint:\n",
    "resumed = glm_hyperparams_class.SweepAndCompare.load_from_checkpoint(\n",
    "    checkpoint_dir=runner._ckpt_dir,\n",
    "    df_X=df_X, df_Y=df_Y, offset_log=offset_log, fit_fn=glm_hyperparams_class.fit_fn,\n",
    "    feature_names=list(df_X.columns), cluster_ids=list(df_Y.columns), groups=cluster_df['event_cluster_id'].values\n",
    ")\n",
    "resumed.run()   # continues only the unfinished configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191",
   "metadata": {},
   "source": [
    "## win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_tables_df = pd.read_csv(\"all_monkey_data/glm_runs/glm_sweep_2025-09-06/cv_tables.csv\")\n",
    "# summary_df = pd.read_csv(\"all_monkey_data/glm_runs/glm_sweep_2025-09-06/summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_all = cv_tables_df  # shorthand: concatenated CV results across ALL configs\n",
    "                       # expected columns: ['config','cluster','alpha','l1_wt','score','rank','selected',...]\n",
    "\n",
    "# ---------------- Winner frequency by hyper-params ----------------\n",
    "# Filter to the *winning* combo per (config, cluster): rows where `selected == True`.\n",
    "# Then count how many clusters each (alpha, l1_wt) won within each config.\n",
    "wins = (\n",
    "    cv_all[cv_all['selected']]\n",
    "      .groupby(['config', 'alpha', 'l1_wt'])   # tally wins per config and hyper-param combo\n",
    "      .size()                                  # number of clusters where this combo was selected\n",
    "      .rename('wins')                          # name the count column\n",
    "      .reset_index()                           # turn groupby index back into columns\n",
    "      .sort_values(['config', 'wins'],        # show most frequent winners first within each config\n",
    "                   ascending=[True, False])\n",
    ")\n",
    "print(wins.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- “Gap to 2nd best” per cluster ----------------\n",
    "# This measures how decisively the winning combo beat the runner-up for each (config, cluster).\n",
    "# Larger gap => more confidence that the chosen hyper-params are truly better for that cluster.\n",
    "\n",
    "def gap_to_second(g):\n",
    "    \"\"\"\n",
    "    Given a group g containing all CV rows for a single (config, cluster),\n",
    "    compute the difference between the best and the second-best CV score.\n",
    "    Return NaN if there aren't at least two candidates.\n",
    "    \"\"\"\n",
    "    s = g.sort_values('score', ascending=False)['score'].to_numpy()\n",
    "    return np.nan if len(s) < 2 else (s[0] - s[1])\n",
    "\n",
    "# Apply the gap function per (config, cluster) and summarize by config.\n",
    "gap = (\n",
    "    cv_all.groupby(['config', 'cluster'])\n",
    "          .apply(gap_to_second)                # gap per cluster within each config\n",
    "          .rename('cv_gap_next')               # name the metric\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Summary stats (count/mean/std/min/quantiles/max) of gap sizes per config.\n",
    "gap_summary = gap.groupby('config')['cv_gap_next'].describe()\n",
    "print(gap_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot convergence problems quickly\n",
    "fails = cv_all.query(\"fit_attempted == True and (fit_ok == False)\")\n",
    "if not fails.empty:\n",
    "    print(\"Combos that failed to fit:\")\n",
    "    display(fails[['config','cluster','alpha','l1_wt','error']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = summary_df.iloc[0]['config']          # already sorted by your criteria\n",
    "winners, dist = compare_glm_configs.show_hyperparams_for_config(results, best_cfg)\n",
    "print('Chosen config:', best_cfg)\n",
    "print('Per-cluster α/l1_wt:\\n', winners)\n",
    "print('Winning-combo counts:\\n', dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or choose one config\n",
    "winners, dist = compare_glm_configs.show_hyperparams_for_config(results, 'ElasticNet')\n",
    "print(winners)   # α & l1_wt per cluster\n",
    "print(dist)      # how often each combo won\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-table driven reporting across all configs\n",
    "\n",
    "# winners across all configs (needs the concatenated cv_tables_df returned by sweep_and_compare)\n",
    "winners_all = (cv_tables_df[cv_tables_df['selected']]\n",
    "               .groupby(['config','alpha','l1_wt'])\n",
    "               .size().rename('n_clusters')\n",
    "               .reset_index()\n",
    "               .sort_values(['config','n_clusters'], ascending=[True, False]))\n",
    "\n",
    "# “confidence” in choice per cluster (gap to 2nd best)\n",
    "def gap_to_second(g):\n",
    "    s = g.sort_values('score', ascending=False)['score'].to_numpy()\n",
    "    return np.nan if len(s) < 2 else s[0] - s[1]\n",
    "\n",
    "cv_gaps = (cv_tables_df.groupby(['config','cluster'])\n",
    "           .apply(gap_to_second).rename('cv_gap_next').reset_index())\n",
    "cv_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best for cluster\n",
    "# grab that config’s result dict\n",
    "best_config = summary_df.iloc[0]['config']\n",
    "print('Best config:', best_config)\n",
    "\n",
    "res_best = next(r for r in results if r['config'] == best_config)\n",
    "\n",
    "# 1) From metrics_df (one row per cluster)\n",
    "if {'cluster','alpha','l1_wt'}.issubset(res_best['metrics_df'].columns):\n",
    "    winners = res_best['metrics_df'][['cluster','alpha','l1_wt']].sort_values('cluster')\n",
    "else:\n",
    "    # 2) Fallback: from CV grid (selected == True)\n",
    "    winners = (res_best['cv_tables_df']\n",
    "               .query('selected')\n",
    "               [['cluster','alpha','l1_wt','score']]\n",
    "               .sort_values(['cluster']))\n",
    "print(winners)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyper-params across all configs (using the concatenated CV grid)\n",
    "# one row per cluster where the winning combo was chosen under each config\n",
    "chosen = (cv_tables_df.query('selected')\n",
    "          [['config','cluster','alpha','l1_wt','score']]\n",
    "          .sort_values(['config','cluster']))\n",
    "print(chosen.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {},
   "source": [
    "## Debug ff dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_or_retrieve_ff_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_pathway = os.path.join(os.path.join(\n",
    "    pn.processed_data_folder_path, 'ff_dataframe.h5'))\n",
    "\n",
    "h5_file_pathway = 'all_monkey_data/processed_data/monkey_Schro/data_0413/ff_dataframe.h5'\n",
    "\n",
    "ff_dataframe = pd.read_hdf(h5_file_pathway, 'ff_dataframe')\n",
    "print(\"Retrieved ff_dataframe from\", h5_file_pathway)\n",
    "ff_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207",
   "metadata": {},
   "source": [
    "## use concat_new_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info['new_segment'] = np.arange(len(new_seg_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_seg_data = pn_utils.concat_new_seg_info(\n",
    "    pn.monkey_information, new_seg_info, bin_width=0.04)\n",
    "\n",
    "concat_seg_data['time_since_start_time'] = concat_seg_data['time'] - concat_seg_data['new_seg_start_time']\n",
    "concat_seg_data['dt'] = np.minimum(concat_seg_data['time_since_start_time'], concat_seg_data['dt'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
