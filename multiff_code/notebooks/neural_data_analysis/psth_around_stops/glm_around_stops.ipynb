{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from decision_making_analysis.event_detection import detect_rsw_and_rcap\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import get_stops_utils, collect_stop_data\n",
    "\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_fit import general_glm_fit, cv_stop_glm, glm_fit_utils, variance_explained, glm_runner\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_plotting import plot_spikes, plot_glm_fit, plot_tuning_func, compare_glm_fit\n",
    "from neural_data_analysis.design_kits.design_around_event import event_binning, stop_design, cluster_design, design_checks\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_hyperparams import compare_glm_configs, glm_hyperparams_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.ff_visibility import ff_vis_epochs, vis_design\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import assemble_stop_design\n",
    "\n",
    "# import decoding\n",
    "from neural_data_analysis.neural_analysis_tools.decoding_tools.event_decoding import decoding_utils, decoding_analysis, plot_decoding, cmp_decode, load_results\n",
    "from neural_data_analysis.design_kits.design_by_segment import spike_history\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "import json\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0301\"\n",
    "\n",
    "pn, datasets, comparisons = collect_stop_data.collect_stop_data_func(\n",
    "    raw_data_folder_path)\n",
    "\n",
    "globals().update(datasets)\n",
    "\n",
    "captures_df, valid_captures_df, filtered_no_capture_stops_df, stops_with_stats = get_stops_utils.prepare_no_capture_and_captures(\n",
    "    monkey_information=pn.monkey_information,\n",
    "    closest_stop_to_capture_df=pn.closest_stop_to_capture_df,\n",
    "    ff_caught_T_new=pn.ff_caught_T_new,\n",
    "    distance_col=\"distance_from_ff_to_stop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Get stop info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## new_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_with_stats['stop_time'] = stops_with_stats['stop_id_start_time']\n",
    "stops_with_stats['prev_time'] = stops_with_stats['stop_id_end_time'].shift(1)\n",
    "stops_with_stats['next_time'] = stops_with_stats['stop_id_start_time'].shift(-1)\n",
    "\n",
    "new_seg_info = event_binning.make_new_seg_info_for_stop_design(stops_with_stats, pn.closest_stop_to_capture_df, pn.monkey_information)\n",
    "\n",
    "\n",
    "events_with_stats = stops_with_stats[['stop_id','stop_cluster_id','stop_id_start_time','stop_id_end_time']].copy()\n",
    "events_with_stats.rename(columns={'stop_id':'event_id', 'stop_cluster_id':'event_cluster_id', \n",
    "                                  'stop_id_start_time':'event_id_start_time', \n",
    "                                  'stop_id_end_time':'event_id_end_time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## what if only keep stop clusters w captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_id_w_captures = new_seg_info.loc[new_seg_info['captured'] == 1, 'stop_cluster_id'].unique()\n",
    "# new_seg_info = new_seg_info[new_seg_info['stop_cluster_id'].isin(cluster_id_w_captures)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_seg_info = new_seg_info[new_seg_info['captured'] == 1].copy()\n",
    "# new_seg_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Prepare for GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## binned_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import assemble_stop_design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binned_spikes_og, binned_feats_og, offset_log_og, meta_used_og, meta_groups = assemble_stop_design.build_stop_design(new_seg_info, events_with_stats, \n",
    "                                                                             pn.monkey_information, \n",
    "                                                                             pn.spikes_df, pn.ff_dataframe, \n",
    "                                                                             datasets=datasets,\n",
    "                                                                             bin_dt=pn.bin_width, add_ff_visible_info=True)\n",
    "\n",
    "binned_feats_og['random_bool'] = np.random.rand(len(binned_feats_og)) > 0.5 # do this, sort of as a control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats = binned_feats_og.copy()\n",
    "binned_spikes = binned_spikes_og.copy()\n",
    "offset_log = offset_log_og\n",
    "meta_used = meta_used_og.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### take out subsets if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take out stops that are misses\n",
    "# binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "#     binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "#     np.asarray(binned_feats_og['miss']==1)\n",
    "# )\n",
    "\n",
    "# binned_feats.drop(columns=['miss'], inplace=True)\n",
    "\n",
    "# # also need to drop columns that involve 'captured'\n",
    "# cols_to_drop = [c for c in binned_feats.columns if 'captured' in c]\n",
    "# binned_feats = binned_feats.drop(columns=cols_to_drop)\n",
    "\n",
    "# # also drop 'rcap_last' if present\n",
    "# if 'rcap_last' in binned_feats.columns:\n",
    "#     binned_feats = binned_feats.drop(columns=['rcap_last'])\n",
    "    \n",
    "# # actually, just drop all columns that involve rsw or rcap\n",
    "# cols_to_drop = [c for c in binned_feats.columns if 'rsw' in c or 'rcap' in c or 'miss' in c]\n",
    "# binned_feats = binned_feats.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take out stops that are not misses\n",
    "# binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "#     binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "#     np.asarray(~binned_feats_og['miss']==1)\n",
    "# )\n",
    "\n",
    "# cols_to_drop = ['miss', 'rsw_first', 'rcap_first', 'rsw_middle', 'rcap_middle', 'rsw_last', 'one_stop_miss']\n",
    "# binned_feats.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take out stops that are not retries\n",
    "# binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "#     binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "#     np.asarray(binned_feats_og['whether_in_retry_series']==0)\n",
    "# )\n",
    "\n",
    "# cols_to_drop = ['whether_in_retry_series', 'next_gap_s_z', 'rsw_first', 'rcap_first', 'rsw_middle', 'rcap_middle', 'rsw_last', 'one_stop_miss']\n",
    "# binned_feats.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take out second half of the data\n",
    "# binned_feats = binned_feats_og.loc[15000:].copy()\n",
    "# binned_spikes = binned_spikes_og.loc[15000:].copy()\n",
    "# offset_log = offset_log_og[15000:].copy()\n",
    "# meta_used = meta_used_og.loc[15000:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take out random stops\n",
    "# binned_feats, binned_spikes, offset_log, meta_used = assemble_stop_design.subset_binned_data(\n",
    "#     binned_feats_og, binned_spikes_og, offset_log_og, meta_used_og, \n",
    "#     np.asarray(binned_feats_og['random_bool'])\n",
    "# )\n",
    "\n",
    "# binned_feats.drop(columns=['random_bool'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_og.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### add interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats = assemble_stop_design.add_interaction_columns(binned_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_sc = assemble_stop_design.scale_binned_feats(binned_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = binned_feats_sc.copy()\n",
    "df_Y = binned_spikes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## see features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## check VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_sc = binned_feats_sc.drop(columns=['next_gap_s_z_x_retry', 'cluster_rel_time_s_z_x_retry', \n",
    "                                                'cluster_progress_c_x_retry', 'event_is_first_in_cluster_x_retry'], \n",
    "                                       errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.topic_based_neural_analysis.full_session import selected_raw_data_features, selected_pn_design_features, selected_stop_design_features\n",
    "print(f'set(binned_feats_sc.columns) - set(selected_stop_design_features.full_stop_design_predictors): {set(binned_feats_sc.columns) - set(selected_stop_design_features.full_stop_design_predictors)}')\n",
    "binned_feats_sc_sub = binned_feats_sc[selected_stop_design_features.full_stop_design_predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(design_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_feats_sc_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_path = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops', 'selected_cols.json')\n",
    "X_pruned, vif_report = design_checks.load_or_compute_selected_cols(binned_feats_sc_sub, cols_path, exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## drop columns that involve 'captured'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop = [c for c in binned_feats_sc.columns if 'captured' in c]\n",
    "# binned_feats_sc = binned_feats_sc.drop(columns=cols_to_drop)\n",
    "# binned_feats_sc.describe()\n",
    "# ## check df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Get spike history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_df = spike_history.make_bin_df_from_stop_meta(meta_used)\n",
    "dt = pn.bin_width\n",
    "t_max = 0.20\n",
    "\n",
    "design_w_history, basis, colnames, meta_groups = (\n",
    "    spike_history.build_design_with_spike_history_from_bins(\n",
    "        spikes_df=pn.spikes_df,\n",
    "        bin_df=bin_df,\n",
    "        X_pruned=X_pruned,\n",
    "        meta_groups=meta_groups,\n",
    "        dt=dt,\n",
    "        t_max=t_max,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_seg_info['new_segment'] = new_seg_info['event_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_used[['bin', 't_left', 't_right']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = binned_feats_sc.copy()\n",
    "df_Y = binned_spikes.copy()\n",
    "\n",
    "output_root = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops')\n",
    "\n",
    "pipeline = glm_runner.GLMPipeline(\n",
    "    spikes_df=pn.spikes_df,\n",
    "    bin_df=bin_df,\n",
    "    df_X=df_X,\n",
    "    df_Y=df_Y,\n",
    "    meta_groups=meta_groups,\n",
    "    bin_width=pn.bin_width,\n",
    "    output_root=output_root,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "pipeline.plot_comparisons()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## just behavioral vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = binned_feats_sc.copy()\n",
    "df_Y = binned_spikes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops', 'glm_fit')\n",
    "\n",
    "report0 = general_glm_fit.glm_mini_report(\n",
    "    df_X=X_pruned, df_Y=df_Y, offset_log=offset_log,\n",
    "    cov_type='HC1', \n",
    "    fast_mle=True,\n",
    "    do_inference=True, \n",
    "    make_plots=True,\n",
    "    show_plots=True,\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df = report0['coefs_df']\n",
    "coefs_df[(coefs_df['term'] == 'captured') & (coefs_df['sig_FDR'] == True)].sort_values('p', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df['refit_on_support'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## both (behav and spike history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_path = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops', 'selected_cols_w_spike_history.json')\n",
    "try:\n",
    "    with open(cols_path, 'r') as f:\n",
    "        selected_cols_w_history = json.load(f)\n",
    "    X_pruned1 = design_w_history[selected_cols_w_history].copy()\n",
    "    print(f'Loaded selected columns from {cols_path}')\n",
    "except:\n",
    "    os.makedirs(os.path.dirname(cols_path), exist_ok=True)\n",
    "    X_pruned1, vif_report = design_checks.check_design(design_w_history)\n",
    "    with open(cols_path, 'w') as f:\n",
    "        json.dump(X_pruned1.columns.tolist(), f)\n",
    "    print(f'Saved selected columns to {cols_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops', 'glm_fit')\n",
    "\n",
    "report1 = general_glm_fit.glm_mini_report(\n",
    "    df_X=X_pruned1, df_Y=df_Y, \n",
    "    offset_log=offset_log,\n",
    "    cov_type='HC1', \n",
    "    fast_mle=True,\n",
    "    do_inference=True, \n",
    "    make_plots=True,\n",
    "    show_plots=True,\n",
    "    meta_groups=meta_groups,\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## just spike history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_history_cols = [c for c in design_w_history.columns if (c.startswith('cluster_') \n",
    "                                                            and c not in binned_feats_sc.columns)]\n",
    "cols_path = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops', 'selected_spike_history_cols.json')\n",
    "try:\n",
    "    with open(cols_path, 'r') as f:\n",
    "        selected_history_cols = json.load(f)\n",
    "    X_pruned2 = design_w_history[selected_history_cols].copy()\n",
    "    print(f'Loaded selected columns from {cols_path}')\n",
    "except:\n",
    "    os.makedirs(os.path.dirname(cols_path), exist_ok=True)\n",
    "    X_pruned2, vif_report = design_checks.check_design(design_w_history[all_history_cols])\n",
    "    with open(cols_path, 'w') as f:\n",
    "        json.dump(X_pruned2.columns.tolist(), f)\n",
    "    print(f'Saved selected columns to {cols_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(pn.planning_and_neural_folder_path, 'glm_around_stops', 'glm_fit')\n",
    "\n",
    "report2 = general_glm_fit.glm_mini_report(\n",
    "    df_X=X_pruned2, df_Y=df_Y, \n",
    "    offset_log=offset_log,\n",
    "    cov_type='HC1', \n",
    "    fast_mle=True,\n",
    "    do_inference=True, \n",
    "    make_plots=True,\n",
    "    show_plots=True,\n",
    "    meta_groups=meta_groups,\n",
    "    save_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# Compare deviance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## In-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_model = {\n",
    "    'Behavior only': report0['metrics_df'],\n",
    "    'Behavior + history': report1['metrics_df'],\n",
    "    'History only': report2['metrics_df'],\n",
    "}\n",
    "\n",
    "compare_glm_fit.plot_insample_model_comparison(metrics_by_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_model = {\n",
    "    'Behavior only': report0['metrics_df'],\n",
    "    'Behavior + history': report1['metrics_df'],\n",
    "    'History only': report2['metrics_df'],\n",
    "}\n",
    "\n",
    "compare_glm_fit.plot_cv_model_comparison(metrics_by_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "# Deviance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glm_fit.plot_insample_model_diagnostics(\n",
    "    report0['metrics_df'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glm_fit.plot_insample_model_diagnostics(\n",
    "    report1['metrics_df'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glm_fit.plot_insample_model_diagnostics(\n",
    "    report2['metrics_df'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glm_fit.plot_cv_model_diagnostics(\n",
    "    report0['metrics_df'],\n",
    "    bins=20,\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_df = report['metrics_df'].copy()\n",
    "\n",
    "# ---- derived quantities ----\n",
    "metrics_df['ll_improvement'] = metrics_df['llf'] - metrics_df['llnull']\n",
    "metrics_df['ll_improvement_per_obs'] = metrics_df['ll_improvement'] / metrics_df['n_obs']\n",
    "\n",
    "# ---- figure layout ----\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# ========== 1. Deviance explained distribution ==========\n",
    "axes[0].hist(metrics_df['deviance_explained'], bins=20)\n",
    "axes[0].axvline(metrics_df['deviance_explained'].median(), linestyle='--')\n",
    "axes[0].set_xlabel('Deviance explained')\n",
    "axes[0].set_ylabel('Number of neurons')\n",
    "axes[0].set_title('Model performance (deviance explained)')\n",
    "\n",
    "# ========== 2. McFadden R² distribution ==========\n",
    "axes[1].hist(metrics_df['mcfadden_R2'], bins=20)\n",
    "axes[1].axvline(metrics_df['mcfadden_R2'].median(), linestyle='--')\n",
    "axes[1].set_xlabel('McFadden $R^2$')\n",
    "axes[1].set_ylabel('Number of neurons')\n",
    "axes[1].set_title('Pseudo-$R^2$ distribution')\n",
    "\n",
    "# ========== 3. Deviance explained vs McFadden R² ==========\n",
    "axes[2].scatter(\n",
    "    metrics_df['deviance_explained'],\n",
    "    metrics_df['mcfadden_R2'],\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[2].set_xlabel('Deviance explained')\n",
    "axes[2].set_ylabel('McFadden $R^2$')\n",
    "axes[2].set_title('Consistency check')\n",
    "\n",
    "# ========== 4. Deviance explained vs null deviance ==========\n",
    "axes[3].scatter(\n",
    "    metrics_df['null_deviance'],\n",
    "    metrics_df['deviance_explained'],\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[3].set_xlabel('Null deviance (rate / variability proxy)')\n",
    "axes[3].set_ylabel('Deviance explained')\n",
    "axes[3].set_title('Dependence on firing statistics')\n",
    "\n",
    "# ========== 5. Log-likelihood improvement ==========\n",
    "axes[4].hist(metrics_df['ll_improvement'], bins=20)\n",
    "axes[4].axvline(0, linestyle='--')\n",
    "axes[4].set_xlabel('Log-likelihood improvement')\n",
    "axes[4].set_ylabel('Number of neurons')\n",
    "axes[4].set_title('Improvement over null model')\n",
    "\n",
    "# ========== 6. LL improvement per observation ==========\n",
    "axes[5].hist(metrics_df['ll_improvement_per_obs'], bins=20)\n",
    "axes[5].axvline(0, linestyle='--')\n",
    "axes[5].set_xlabel('Δ log-likelihood per observation')\n",
    "axes[5].set_ylabel('Number of neurons')\n",
    "axes[5].set_title('Predictive gain (normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_df = report['metrics_df'].copy()\n",
    "\n",
    "# ---- derived quantities ----\n",
    "metrics_df['ll_improvement'] = metrics_df['llf'] - metrics_df['llnull']\n",
    "metrics_df['ll_improvement_per_obs'] = metrics_df['ll_improvement'] / metrics_df['n_obs']\n",
    "\n",
    "# ---- figure layout ----\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# ========== 1. Deviance explained distribution ==========\n",
    "axes[0].hist(metrics_df['deviance_explained'], bins=20)\n",
    "axes[0].axvline(metrics_df['deviance_explained'].median(), linestyle='--')\n",
    "axes[0].set_xlabel('Deviance explained')\n",
    "axes[0].set_ylabel('Number of neurons')\n",
    "axes[0].set_title('Model performance (deviance explained)')\n",
    "\n",
    "# ========== 2. McFadden R² distribution ==========\n",
    "axes[1].hist(metrics_df['mcfadden_R2'], bins=20)\n",
    "axes[1].axvline(metrics_df['mcfadden_R2'].median(), linestyle='--')\n",
    "axes[1].set_xlabel('McFadden $R^2$')\n",
    "axes[1].set_ylabel('Number of neurons')\n",
    "axes[1].set_title('Pseudo-$R^2$ distribution')\n",
    "\n",
    "# ========== 3. Deviance explained vs McFadden R² ==========\n",
    "axes[2].scatter(\n",
    "    metrics_df['deviance_explained'],\n",
    "    metrics_df['mcfadden_R2'],\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[2].set_xlabel('Deviance explained')\n",
    "axes[2].set_ylabel('McFadden $R^2$')\n",
    "axes[2].set_title('Consistency check')\n",
    "\n",
    "# ========== 4. Deviance explained vs null deviance ==========\n",
    "axes[3].scatter(\n",
    "    metrics_df['null_deviance'],\n",
    "    metrics_df['deviance_explained'],\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[3].set_xlabel('Null deviance (rate / variability proxy)')\n",
    "axes[3].set_ylabel('Deviance explained')\n",
    "axes[3].set_title('Dependence on firing statistics')\n",
    "\n",
    "# ========== 5. Log-likelihood improvement ==========\n",
    "axes[4].hist(metrics_df['ll_improvement'], bins=20)\n",
    "axes[4].axvline(0, linestyle='--')\n",
    "axes[4].set_xlabel('Log-likelihood improvement')\n",
    "axes[4].set_ylabel('Number of neurons')\n",
    "axes[4].set_title('Improvement over null model')\n",
    "\n",
    "# ========== 6. LL improvement per observation ==========\n",
    "axes[5].hist(metrics_df['ll_improvement_per_obs'], bins=20)\n",
    "axes[5].axvline(0, linestyle='--')\n",
    "axes[5].set_xlabel('Δ log-likelihood per observation')\n",
    "axes[5].set_ylabel('Number of neurons')\n",
    "axes[5].set_title('Predictive gain (normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "# More analysis of glm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['metrics_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "## Note large coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_fit_utils.summarize_large_coeffs(report['coefs_df'], df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(variance_explained)\n",
    "df_Y_pred = variance_explained.build_df_Y_pred_from_results(\n",
    "    results=report['results'],\n",
    "    df_X=df_X,\n",
    "    offset_log=offset_log,\n",
    "    df_Y=df_Y\n",
    ")\n",
    "\n",
    "# sanity checks\n",
    "assert df_Y_pred.shape == df_Y.shape\n",
    "assert np.isfinite(df_Y_pred.values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert observed/predicted DataFrames to arrays\n",
    "X = df_Y.to_numpy()      # observed counts\n",
    "X_hat = df_Y_pred.to_numpy() # predicted expected counts\n",
    "event_ids = meta_used['event_id'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## population latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_eval import shared_manifold, parity_utils, population_latent_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_idx = data['new_bin']\n",
    "time_idx = meta_used['rel_center'].values\n",
    "condition_labels = None\n",
    "trial_ids = meta_used['event_id'].values\n",
    "\n",
    "res = population_latent_benchmark.population_latent_benchmark_function(\n",
    "    X, X_hat, trial_ids, time_idx, condition_labels,\n",
    "    fa_factors=8,\n",
    "    max_rank=10,\n",
    "    shuffle_mode='latent'\n",
    ")\n",
    "\n",
    "res['figs']['cca_spectrum']\n",
    "res['figs']['ve_summary']\n",
    "res['figs']['rrr_curve']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## shared components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "Does the GLM recover the same latent population dynamics that are present in the real neural data?\n",
    "\n",
    "Are the dominant latent trajectories aligned?\n",
    "\n",
    "Are they time-locked similarly?\n",
    "\n",
    "Are the shared dimensions smooth, structured, and consistent across trials?\n",
    "\n",
    "Your plots are asking:\n",
    "\n",
    "If I look at neural activity through the lens of shared canonical components, do real data and model predictions trace the same paths?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_eval.shared_manifold import plot_shared_components\n",
    "import pandas as pd\n",
    "\n",
    "df_lat = pd.concat(\n",
    "    res['parity']['latents_by_fold'].values(),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "best_rank = res['rrr']['best_rank']\n",
    "\n",
    "figs = plot_shared_components(\n",
    "    df_lat,\n",
    "    components=[1, 2], # or use n_components=best_rank instead\n",
    "    overlay_mode='separate'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "## shared phase planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: one fold\n",
    "fig = shared_manifold.plot_shared_phase_plane(\n",
    "    res['parity']['latents_by_fold'][0],\n",
    "    components=(1, 2),\n",
    "    smooth_bins=3\n",
    ")\n",
    "\n",
    "# or concat all folds (one caveat: Different folds use slightly different CCA fits.)\n",
    "\n",
    "df_lat = pd.concat(\n",
    "    res['parity']['latents_by_fold'].values(),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "fig = shared_manifold.plot_shared_phase_plane(\n",
    "    df_lat,\n",
    "    components=(1, 2),\n",
    "    smooth_bins=3\n",
    ")\n",
    "\n",
    "\n",
    "fig = shared_manifold.plot_shared_phase_plane_by_fold(\n",
    "    res['parity']['latents_by_fold'],\n",
    "    components=(1, 2),\n",
    "    smooth_bins=3,\n",
    "    alpha_fold=0.6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## Plot spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_spikes)\n",
    "\n",
    "cluster_idx = 6\n",
    "\n",
    "for event_id in range(30, 33):\n",
    "    if event_id in meta_used['event_id'].values:\n",
    "        # If your GLM used offset_log = np.log(exposure_s), you can omit exposure_s:\n",
    "        plot_spikes.plot_observed_vs_predicted_event(\n",
    "            binned_feats_sc=binned_feats_sc,\n",
    "            binned_spikes=binned_spikes,\n",
    "            meta_used=meta_used,\n",
    "            offset_log=offset_log,\n",
    "            model_res=report['results'][cluster_idx],   # GLM for cluster 0\n",
    "            cluster_idx=cluster_idx,\n",
    "            seg_id=event_id,\n",
    "            seg_col='event_id',\n",
    "            time_col='rel_center'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "## tuning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['coefs_df']['term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_tuning_func)\n",
    "\n",
    "cluster_id = 10\n",
    "model_res = report['results'][cluster_id]\n",
    "var = 'captured'\n",
    "\n",
    "exposure_s = np.exp(offset_log)\n",
    "# make empirical curve\n",
    "tc_emp = plot_tuning_func.empirical_tuning_curve(\n",
    "    binned_spikes=binned_spikes[cluster_id].to_numpy(),\n",
    "    predictor_vals=binned_feats[var].to_numpy(),\n",
    "    exposure_s=exposure_s,\n",
    "    nbins=20\n",
    ")\n",
    "\n",
    "# make GLM curve\n",
    "tc_glm = plot_tuning_func.glm_tuning_curve(\n",
    "    model_res, df_X,\n",
    "    var=var,\n",
    "    offset_log=offset_log,\n",
    "    average='marginal',\n",
    "    weights=exposure_s,                 # time-weighted average rate (recommended)\n",
    "    return_ci=True\n",
    ")\n",
    "\n",
    "# overlay\n",
    "plot_tuning_func.overlay_tuning_curves(tc_emp, tc_glm, xcol=var,\n",
    "                      title=f'Unit {cluster_id}: {var} tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tuning_func.plot_tuning_with_ci(tc_glm, xcol='captured', ycol='rate_hz')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## plot pred_vs_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 5\n",
    "cv_stop_glm.plot_pred_vs_obs(report['results'][cluster_id], df_X, binned_spikes[cluster_id], offset_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "# CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst = cca_class.CCAclass(X1=binned_spikes.copy(), X2=binned_feats_sc.copy(), lagging_included=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.conduct_cca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X2', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "# check fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.04\n",
    "rates = binned_spikes.sum(axis=0) / (len(binned_spikes) * dt)\n",
    "# rates is a Series indexed by unit, in Hz\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def firing_rates_from_df(spikes_df, time_col='time', cluster_col='cluster'):\n",
    "    start_s = spikes_df[time_col].min()\n",
    "    end_s = spikes_df[time_col].max()\n",
    "    duration = end_s - start_s\n",
    "\n",
    "    counts = spikes_df.groupby(cluster_col).size()\n",
    "    rates_hz = counts / duration\n",
    "    return rates_hz.rename('rate_hz').reset_index()\n",
    "\n",
    "firing_rates_from_df(pn.spikes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sub = new_seg_info[new_seg_info['captured'] > 0]\n",
    "sns.histplot(new_seg_info['n_pre_bins'])\n",
    "sns.histplot(seg_sub['n_pre_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(new_seg_info['n_post_bins'])\n",
    "sns.histplot(seg_sub['n_post_bins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "# plot_spaghetti_per_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## run func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['event_id', 'rel_center', 't_left', 't_right']\n",
    "binned_spikes2 = binned_spikes.copy()\n",
    "binned_spikes2[cols] = meta_used[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a unit column by name or int (e.g., 3)\n",
    "# unit_col = 3  # or '3' if your columns are strings\n",
    "for unit_col in binned_spikes.columns:\n",
    "    df_rate = plot_spikes.make_rate_df_from_binned(binned_spikes2, unit_col)\n",
    "\n",
    "    # plot (with gentle smoothing and pre-stop baseline subtraction)\n",
    "    fig, ax, n = plot_spikes.plot_spaghetti_per_stop(\n",
    "        df_rate,\n",
    "        smooth_sigma_s=0.08,          # ~80 ms sigma (auto-converted to bins)\n",
    "        # baseline_window=(-0.5, -0.1), # subtract mean pre-stop activity\n",
    "        baseline_window=None,\n",
    "        max_stops=None,               # or an int to limit how many lines\n",
    "        median_label='median (all stops)',\n",
    "        title=f'Unit {unit_col}: rate per stop'\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f'Plotted {n} stops.')\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "# Hyperparam tuning (try different configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "## just elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(general_glm_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res = general_glm_fit.glm_mini_report(\n",
    "    df_X, df_Y, offset_log,\n",
    "    regularization='elasticnet',\n",
    "    # alpha_grid=(0.05, 0.1, 0.2, 0.5, 1.0),\n",
    "    # l1_wt_grid=(0.75, 0.5, 0.25, 0.0),\n",
    "    alpha_grid=(0.1, 0.2),\n",
    "    l1_wt_grid=(0.75, 0.5),\n",
    "    groups=cluster_df['event_cluster_id'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['cv_tables_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['metrics_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['figures']['rr_hist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all chosen alphas\n",
    "print(res['metrics_df']['alpha'].value_counts())\n",
    "\n",
    "# mean/median chosen alpha across clusters\n",
    "print(res['metrics_df']['alpha'].agg(['mean','median']))\n",
    "\n",
    "# joint distribution of alpha × l1_wt\n",
    "pd.crosstab(res['metrics_df']['alpha'], res['metrics_df']['l1_wt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "## configs (systematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    # 1) Ridge (ultra-light grid) ─ combos: ~18\n",
    "    dict(name='Poisson_Ridge_ultra',\n",
    "         regularization='elasticnet',\n",
    "         alpha_grid=tuple(np.concatenate([\n",
    "             np.logspace(-6, -4.7, 6),   # 1e-6 … ~2e-5\n",
    "             np.logspace(-4.7, -2.3, 12) # ~2e-5 … 5e-3\n",
    "         ])),\n",
    "         l1_wt_grid=(0.0,),                 # pure L2\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "         refit_on_support=False),\n",
    "\n",
    "    # 2) Lasso (ultra-light grid) + refit ─ combos: ~16\n",
    "    dict(name='Poisson_Lasso_ultra_refit',\n",
    "         regularization='elasticnet',\n",
    "         alpha_grid=tuple(np.concatenate([\n",
    "             np.logspace(-6, -4.7, 6),   # 1e-6 … ~2e-5\n",
    "             np.logspace(-4.7, -3.3, 10) # ~2e-5 … 5e-4\n",
    "         ])),\n",
    "         l1_wt_grid=(1.0,),                 # pure L1\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "         refit_on_support=True),\n",
    "    \n",
    "    \n",
    "    # 0) Plain Poisson MLE, robust SEs (baseline) ─ combos: 1\n",
    "    dict(name='Poisson_MLE_HC3',\n",
    "         regularization='none',\n",
    "         alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "         refit_on_support=False),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs = [\n",
    "#     # 0) Plain Poisson MLE, robust SEs (baseline)  ─ combos: 1\n",
    "#     dict(name='Poisson_MLE_HC3',\n",
    "#          regularization='none',\n",
    "#          alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "#          refit_on_support=False),\n",
    "\n",
    "#     # 1) Ridge (fine grid) ─ combos: 20\n",
    "#     dict(name='Poisson_Ridge_fine',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 2, 20)),\n",
    "#          l1_wt_grid=(0.0,),                 # pure L2\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=False),\n",
    "\n",
    "#     # 2) Lasso (fine) + refit ─ combos: 20\n",
    "#     dict(name='Poisson_Lasso_refit_fine',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 0, 20)),\n",
    "#          l1_wt_grid=(1.0,),                 # pure L1\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 3) Elastic-Net (mix) ─ combos: 14 × 5 = 70  ← HEAVY\n",
    "#     dict(name='Poisson_EN_fine',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 1, 14)),\n",
    "#          l1_wt_grid=(1.0, 0.75, 0.5, 0.25, 0.0),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 4) EN, time-aware CV ─ combos: 12 × 3 = 36\n",
    "#     dict(name='Poisson_EN_timecv',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 0, 12)),\n",
    "#          l1_wt_grid=(1.0, 0.5, 0.0),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True,\n",
    "#          cv_splitter='blocked_time'),\n",
    "\n",
    "#     # 5) Ridge, time-aware CV ─ combos: 16\n",
    "#     dict(name='Poisson_Ridge_timecv',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 1, 16)),\n",
    "#          l1_wt_grid=(0.0,),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=False,\n",
    "#          cv_splitter='blocked_time'),\n",
    "\n",
    "#     # 6) Lasso (HC3) + refit ─ combos: 16\n",
    "#     dict(name='Poisson_Lasso_HC3_refit',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-4, 0, 16)),\n",
    "#          l1_wt_grid=(1.0,),\n",
    "#          cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 7) EN (deviance CV metric) ─ combos: 10 × 4 = 40\n",
    "#     dict(name='Poisson_EN_devianceCV',\n",
    "#          regularization='elasticnet',\n",
    "#          alpha_grid=tuple(np.logspace(-3, 0.7, 10)),\n",
    "#          l1_wt_grid=(1.0, 0.5, 0.25, 0.0),\n",
    "#          cv_metric='deviance', n_splits=5, cov_type='HC1',\n",
    "#          refit_on_support=True),\n",
    "\n",
    "#     # 8) Quasi-Poisson flavor (same mean; inflated SEs) ─ combos: 1\n",
    "#     dict(name='QuasiPoisson_like',\n",
    "#          regularization='none',\n",
    "#          alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "#          cv_metric='deviance', n_splits=5, cov_type='HC3',\n",
    "#          refit_on_support=False,\n",
    "#          use_overdispersion_scale=True),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "## compare (w class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build your configs (as before)\n",
    "configs = [\n",
    "    dict(name='Poisson_MLE_HC3',\n",
    "         regularization='none',\n",
    "         alpha_grid=(0.0,), l1_wt_grid=(0.0,),\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC3',\n",
    "         refit_on_support=False),\n",
    "    dict(name='Poisson_Ridge_fine',\n",
    "         regularization='elasticnet',\n",
    "         alpha_grid=tuple(np.logspace(-4, 2, 20)),\n",
    "         l1_wt_grid=(0.0,),\n",
    "         cv_metric='loglik', n_splits=5, cov_type='HC1',\n",
    "         refit_on_support=False),\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# 2) Spin up the runner\n",
    "runner = glm_hyperparams_class.SweepAndCompare(\n",
    "    configs=configs,\n",
    "    df_X=df_X,\n",
    "    df_Y=df_Y,\n",
    "    offset_log=offset_log,\n",
    "    fit_fn=glm_hyperparams_class.fit_fn,                           # <-- the adapter above\n",
    "    feature_names=list(df_X.columns),\n",
    "    cluster_ids=list(df_Y.columns),\n",
    "    groups=cluster_df['event_cluster_id'].values,\n",
    "    cov_type='HC1',\n",
    "    out_dir='glm_sweep_compare_out',\n",
    "    autosave=True,\n",
    "    autosave_every=1,\n",
    "    extra_fit_kwargs=dict(\n",
    "        cv_splitter=None,                    # or 'blocked_time'\n",
    "        use_overdispersion_scale=False,\n",
    "        add_outer_cv_summary=True,           # optional: outer-CV summary\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 3) Run (and safely interrupt/resume any time)\n",
    "runner.run()\n",
    "summary_df = runner.get_summary()\n",
    "cv_tables_df   = runner.get_cv_tables()\n",
    "results = runner.per_config_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.per_config_results  # list of dicts (detailed)\n",
    "\n",
    "# resume later from checkpoint:\n",
    "resumed = glm_hyperparams_class.SweepAndCompare.load_from_checkpoint(\n",
    "    checkpoint_dir=runner._ckpt_dir,\n",
    "    df_X=df_X, df_Y=df_Y, offset_log=offset_log, fit_fn=glm_hyperparams_class.fit_fn,\n",
    "    feature_names=list(df_X.columns), cluster_ids=list(df_Y.columns), groups=cluster_df['event_cluster_id'].values\n",
    ")\n",
    "resumed.run()   # continues only the unfinished configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "## win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_tables_df = pd.read_csv(\"all_monkey_data/glm_runs/glm_sweep_2025-09-06/cv_tables.csv\")\n",
    "# summary_df = pd.read_csv(\"all_monkey_data/glm_runs/glm_sweep_2025-09-06/summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_all = cv_tables_df  # shorthand: concatenated CV results across ALL configs\n",
    "                       # expected columns: ['config','cluster','alpha','l1_wt','score','rank','selected',...]\n",
    "\n",
    "# ---------------- Winner frequency by hyper-params ----------------\n",
    "# Filter to the *winning* combo per (config, cluster): rows where `selected == True`.\n",
    "# Then count how many clusters each (alpha, l1_wt) won within each config.\n",
    "wins = (\n",
    "    cv_all[cv_all['selected']]\n",
    "      .groupby(['config', 'alpha', 'l1_wt'])   # tally wins per config and hyper-param combo\n",
    "      .size()                                  # number of clusters where this combo was selected\n",
    "      .rename('wins')                          # name the count column\n",
    "      .reset_index()                           # turn groupby index back into columns\n",
    "      .sort_values(['config', 'wins'],        # show most frequent winners first within each config\n",
    "                   ascending=[True, False])\n",
    ")\n",
    "print(wins.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- “Gap to 2nd best” per cluster ----------------\n",
    "# This measures how decisively the winning combo beat the runner-up for each (config, cluster).\n",
    "# Larger gap => more confidence that the chosen hyper-params are truly better for that cluster.\n",
    "\n",
    "def gap_to_second(g):\n",
    "    \"\"\"\n",
    "    Given a group g containing all CV rows for a single (config, cluster),\n",
    "    compute the difference between the best and the second-best CV score.\n",
    "    Return NaN if there aren't at least two candidates.\n",
    "    \"\"\"\n",
    "    s = g.sort_values('score', ascending=False)['score'].to_numpy()\n",
    "    return np.nan if len(s) < 2 else (s[0] - s[1])\n",
    "\n",
    "# Apply the gap function per (config, cluster) and summarize by config.\n",
    "gap = (\n",
    "    cv_all.groupby(['config', 'cluster'])\n",
    "          .apply(gap_to_second)                # gap per cluster within each config\n",
    "          .rename('cv_gap_next')               # name the metric\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Summary stats (count/mean/std/min/quantiles/max) of gap sizes per config.\n",
    "gap_summary = gap.groupby('config')['cv_gap_next'].describe()\n",
    "print(gap_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot convergence problems quickly\n",
    "fails = cv_all.query(\"fit_attempted == True and (fit_ok == False)\")\n",
    "if not fails.empty:\n",
    "    print(\"Combos that failed to fit:\")\n",
    "    display(fails[['config','cluster','alpha','l1_wt','error']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = summary_df.iloc[0]['config']          # already sorted by your criteria\n",
    "winners, dist = compare_glm_configs.show_hyperparams_for_config(results, best_cfg)\n",
    "print('Chosen config:', best_cfg)\n",
    "print('Per-cluster α/l1_wt:\\n', winners)\n",
    "print('Winning-combo counts:\\n', dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or choose one config\n",
    "winners, dist = compare_glm_configs.show_hyperparams_for_config(results, 'ElasticNet')\n",
    "print(winners)   # α & l1_wt per cluster\n",
    "print(dist)      # how often each combo won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-table driven reporting across all configs\n",
    "\n",
    "# winners across all configs (needs the concatenated cv_tables_df returned by sweep_and_compare)\n",
    "winners_all = (cv_tables_df[cv_tables_df['selected']]\n",
    "               .groupby(['config','alpha','l1_wt'])\n",
    "               .size().rename('n_clusters')\n",
    "               .reset_index()\n",
    "               .sort_values(['config','n_clusters'], ascending=[True, False]))\n",
    "\n",
    "# “confidence” in choice per cluster (gap to 2nd best)\n",
    "def gap_to_second(g):\n",
    "    s = g.sort_values('score', ascending=False)['score'].to_numpy()\n",
    "    return np.nan if len(s) < 2 else s[0] - s[1]\n",
    "\n",
    "cv_gaps = (cv_tables_df.groupby(['config','cluster'])\n",
    "           .apply(gap_to_second).rename('cv_gap_next').reset_index())\n",
    "cv_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best for cluster\n",
    "# grab that config’s result dict\n",
    "best_config = summary_df.iloc[0]['config']\n",
    "print('Best config:', best_config)\n",
    "\n",
    "res_best = next(r for r in results if r['config'] == best_config)\n",
    "\n",
    "# 1) From metrics_df (one row per cluster)\n",
    "if {'cluster','alpha','l1_wt'}.issubset(res_best['metrics_df'].columns):\n",
    "    winners = res_best['metrics_df'][['cluster','alpha','l1_wt']].sort_values('cluster')\n",
    "else:\n",
    "    # 2) Fallback: from CV grid (selected == True)\n",
    "    winners = (res_best['cv_tables_df']\n",
    "               .query('selected')\n",
    "               [['cluster','alpha','l1_wt','score']]\n",
    "               .sort_values(['cluster']))\n",
    "print(winners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyper-params across all configs (using the concatenated CV grid)\n",
    "# one row per cluster where the winning combo was chosen under each config\n",
    "chosen = (cv_tables_df.query('selected')\n",
    "          [['config','cluster','alpha','l1_wt','score']]\n",
    "          .sort_values(['config','cluster']))\n",
    "print(chosen.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "## Iterate through sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys, json, hashlib\n",
    "# from pathlib import Path\n",
    "\n",
    "# def list_sessions(monkey_dir: Path):\n",
    "#     return sorted([p.name for p in monkey_dir.iterdir() if p.is_dir() and p.name.startswith('data_')])\n",
    "\n",
    "# monkey_dir = Path('all_monkey_data/raw_monkey_data/monkey_Bruno')\n",
    "# all_sessions = list_sessions(monkey_dir)\n",
    "\n",
    "# for session in all_sessions:\n",
    "#     print(session)\n",
    "#     try:\n",
    "#         raw_data_folder_path = f\"all_monkey_data/raw_monkey_data/monkey_Bruno/{session}\"\n",
    "#         cmp_decode.summarize_and_plot_decoding(raw_data_folder_path)   \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in {session}: {e}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "## Debug ff dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.make_or_retrieve_ff_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ff_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_pathway = os.path.join(os.path.join(\n",
    "    pn.processed_data_folder_path, 'ff_dataframe.h5'))\n",
    "\n",
    "h5_file_pathway = 'all_monkey_data/processed_data/monkey_Schro/data_0413/ff_dataframe.h5'\n",
    "\n",
    "ff_dataframe = pd.read_hdf(h5_file_pathway, 'ff_dataframe')\n",
    "print(\"Retrieved ff_dataframe from\", h5_file_pathway)\n",
    "ff_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "## use concat_new_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_info['new_segment'] = np.arange(len(new_seg_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_seg_data = pn_utils.concat_new_seg_info(\n",
    "    pn.monkey_information, new_seg_info, bin_width=0.04)\n",
    "\n",
    "concat_seg_data['time_since_start_time'] = concat_seg_data['time'] - concat_seg_data['new_seg_start_time']\n",
    "concat_seg_data['dt'] = np.minimum(concat_seg_data['time_since_start_time'], concat_seg_data['dt'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
