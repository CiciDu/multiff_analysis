{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, sys\n",
    "from pathlib import Path\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "    \n",
    "from data_wrangling import specific_utils, process_monkey_information, general_utils\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from neural_data_analysis.neural_analysis_tools.get_neural_data import neural_data_processing\n",
    "from neural_data_analysis.neural_analysis_tools.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import transform_vars, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from neural_data_analysis.topic_based_neural_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.planning_and_neural import planning_and_neural_class, pn_utils, pn_helper_class, pn_aligned_by_seg, pn_aligned_by_event\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods import cca_class, cca_utils, cca_cv_utils\n",
    "from neural_data_analysis.neural_analysis_tools.cca_methods.cca_plotting import cca_plotting, cca_plot_lag_vs_no_lag, cca_plot_cv\n",
    "from machine_learning.ml_methods import regression_utils, regz_regression_utils, ml_methods_class, classification_utils, ml_plotting_utils, ml_methods_utils\n",
    "from planning_analysis.show_planning import nxt_ff_utils, show_planning_utils\n",
    "from neural_data_analysis.neural_analysis_tools.gpfa_methods import elephant_utils, fit_gpfa_utils, plot_gpfa_utils, gpfa_helper_class\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import time_resolved_regression, time_resolved_gpfa_regression,plot_time_resolved_regression\n",
    "from neural_data_analysis.neural_analysis_tools.align_trials import align_trial_utils\n",
    "from decision_making_analysis.event_detection import detect_rsw_and_rcap\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_psth import core_stops_psth, psth_postprocessing, psth_stats, compare_events, dpca_utils\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import get_stops_utils, collect_stop_data\n",
    "\n",
    "from neural_data_analysis.neural_analysis_tools.glm_tools.glm_fit import general_glm_fit, cv_stop_glm, glm_fit_utils, variance_explained, glm_runner\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_plotting import plot_spikes, plot_glm_fit, plot_tuning_func\n",
    "from neural_data_analysis.design_kits.design_around_event import event_binning, stop_design, cluster_design, design_checks\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.stop_glm.glm_hyperparams import compare_glm_configs, glm_hyperparams_class\n",
    "from neural_data_analysis.topic_based_neural_analysis.ff_visibility import ff_vis_epochs, vis_design\n",
    "\n",
    "from neural_data_analysis.topic_based_neural_analysis.stop_event_analysis.get_stop_events import assemble_stop_design\n",
    "\n",
    "# import decoding\n",
    "from neural_data_analysis.neural_analysis_tools.decoding_tools.event_decoding import decoding_utils, decoding_analysis, plot_decoding, cmp_decode, load_results\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "import json\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0327\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0301\"\n",
    "\n",
    "pn, datasets, comparisons = collect_stop_data.collect_stop_data_func(\n",
    "    raw_data_folder_path)\n",
    "\n",
    "globals().update(datasets)\n",
    "\n",
    "captures_df, valid_captures_df, filtered_no_capture_stops_df, stops_with_stats = get_stops_utils.prepare_no_capture_and_captures(\n",
    "    monkey_information=pn.monkey_information,\n",
    "    closest_stop_to_capture_df=pn.closest_stop_to_capture_df,\n",
    "    ff_caught_T_new=pn.ff_caught_T_new,\n",
    "    distance_col=\"distance_from_ff_to_stop\",\n",
    ")\n",
    "\n",
    "keys = ['rsw_first_vs_rcap_first', 'rsw_middle_vs_rcap_middle', 'one_stop_vs_both_first_miss', 'will_switch_vs_retry_after_miss', 'switch_vs_retry_after_retry']\n",
    "\n",
    "cfg = core_stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.04,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## save comparisons.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if save all combos\n",
    "\n",
    "import json, os\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "with open(\"multiff_analysis/configs/comparisons.json\", \"w\") as f:\n",
    "    json.dump(comparisons, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only save a subset of comparisons\n",
    "\n",
    "keys = [\n",
    "    'rsw_first_vs_rcap_first',\n",
    "    'rsw_middle_vs_rcap_middle',\n",
    "    'one_stop_vs_both_first_miss',\n",
    "    'will_switch_vs_retry_after_miss',\n",
    "    'switch_vs_retry_after_retry'\n",
    "]\n",
    "\n",
    "# Filter the comparisons list by key\n",
    "comparisons_sub = [c for c in comparisons if c['key'] in keys]\n",
    "\n",
    "\n",
    "import json, os\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "with open(\"multiff_analysis/configs/comparisons.json\", \"w\") as f:\n",
    "    json.dump(comparisons_sub, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"multiff_analysis/configs/comparisons.json\", \"r\") as f:\n",
    "    comparisons_sub = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comparisons_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Decoding results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## plot by key (all sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### non-cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined = load_results.load_all_retry_decoder_results(\n",
    "    monkey_name='monkey_Bruno', cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined_sub = df_all_combined[df_all_combined['align_by_stop_end']==True]\n",
    "plot_decoding.plot_decoding_timecourse_by_session(\n",
    "    df_all_combined_sub,\n",
    "    align_col=\"align_by_stop_end\",\n",
    "    value_col=\"mean_auc\",\n",
    "    sig_col=\"sig_ttest\" if \"sig_ttest\" in df_all_combined.columns else \"sig_FDR\",\n",
    "    err_col=\"sd_auc\" if \"sd_auc\" in df_all_combined.columns else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined = load_results.load_all_retry_decoder_results(\n",
    "    monkey_name='monkey_Bruno', cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined_sub = df_all_combined[df_all_combined['align_by_stop_end']==True]\n",
    "plot_decoding.plot_decoding_timecourse_by_session(\n",
    "    df_all_combined_sub,\n",
    "    align_col=\"align_by_stop_end\",\n",
    "    value_col=\"mean_auc\",\n",
    "    sig_col=\"sig_ttest\" if \"sig_ttest\" in df_all_combined.columns else \"sig_FDR\",\n",
    "    err_col=\"sd_auc\" if \"sd_auc\" in df_all_combined.columns else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_combined_sub[(df_all_combined_sub['key']=='guat_first_vs_taft_first')&\n",
    "#                     (df_all_combined_sub['model_name']=='logreg')&\n",
    "#                     (df_all_combined_sub['session']=='data_0220')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## plot by session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import combine_info_utils, specific_utils\n",
    "# df_all_combined = pd.DataFrame()\n",
    "\n",
    "monkey_name = 'monkey_Bruno'\n",
    "num_sessions = 5\n",
    "\n",
    "raw_data_dir_name = 'all_monkey_data/raw_monkey_data'\n",
    "sessions_df_for_one_monkey = combine_info_utils.make_sessions_df_for_one_monkey(\n",
    "                raw_data_dir_name, monkey_name)\n",
    "\n",
    "\n",
    "session_count = 0\n",
    "for index, row in sessions_df_for_one_monkey.iterrows():\n",
    "    print(row['data_name'])\n",
    "    raw_data_folder_path = os.path.join(\n",
    "        raw_data_dir_name, row['monkey_name'], row['data_name'])\n",
    "    print(raw_data_folder_path)\n",
    "    \n",
    "    try:\n",
    "        df_all = cmp_decode.summarize_and_plot_decoding(raw_data_folder_path, cumulative=True)   \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {raw_data_folder_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    df_all['session'] = row['data_name']\n",
    "    sessions_df_for_one_monkey.loc[index, 'finished'] = True\n",
    "    # df_all_combined = pd.concat([df_all_combined, df_all], ignore_index=True)\n",
    "    \n",
    "    session_count += 1\n",
    "    # if session_count >= num_sessions:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## plot one session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0301\"\n",
    "df_all = cmp_decode.summarize_and_plot_decoding(raw_data_folder_path, cumulative=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the uniqueness of entries in df_all\n",
    "df_all.groupby(['key', 'window_start', 'model_name', 'align_by_stop_end']).count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = _summarize(df_all)\n",
    "cmp_decode._plot_summary_bars(summary, title_prefix=\"Peak AUC by model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# How much do models agree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### combine all keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df_all_combined.copy()\n",
    "key_cols = ['window_start', 'window_end', 'key', 'session']\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1ï¸âƒ£ Compute Jaccard overlap (significant only)\n",
    "df_sig = df[df['sig_ttest']]\n",
    "\n",
    "sig_sets = {\n",
    "    model: set(map(tuple, g[key_cols].values))\n",
    "    for model, g in df_sig.groupby('model_name')\n",
    "}\n",
    "models = list(sig_sets.keys())\n",
    "\n",
    "jaccard_sig = np.zeros((len(models), len(models)))\n",
    "for i, m1 in enumerate(models):\n",
    "    for j, m2 in enumerate(models):\n",
    "        inter = len(sig_sets[m1] & sig_sets[m2])\n",
    "        union = len(sig_sets[m1] | sig_sets[m2])\n",
    "        jaccard_sig[i, j] = inter / union if union > 0 else np.nan\n",
    "jaccard_sig_df = pd.DataFrame(jaccard_sig, index=models, columns=models)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2ï¸âƒ£ Compute Jaccard overlap (all windows)\n",
    "all_sets = {\n",
    "    model: set(map(tuple, g[key_cols].values))\n",
    "    for model, g in df.groupby('model_name')\n",
    "}\n",
    "jaccard_all = np.zeros((len(models), len(models)))\n",
    "for i, m1 in enumerate(models):\n",
    "    for j, m2 in enumerate(models):\n",
    "        inter = len(all_sets[m1] & all_sets[m2])\n",
    "        union = len(all_sets[m1] | all_sets[m2])\n",
    "        jaccard_all[i, j] = inter / union if union > 0 else np.nan\n",
    "jaccard_all_df = pd.DataFrame(jaccard_all, index=models, columns=models)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3ï¸âƒ£ Compute AUC correlation (significant only)\n",
    "sig_auc = df_sig.pivot_table(index=key_cols, columns='model_name', values='mean_auc')\n",
    "auc_corr_sig = sig_auc.corr(method='pearson')\n",
    "\n",
    "# 4ï¸âƒ£ Compute AUC correlation (all windows)\n",
    "auc_all = df.pivot_table(index=key_cols, columns='model_name', values='mean_auc')\n",
    "auc_corr_all = auc_all.corr(method='pearson')\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ðŸ–¼ Plot: 2Ã—2 grid of heatmaps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "    jaccard_sig_df, ax=axes[0, 0], cmap='YlGnBu', vmin=0, vmax=1, annot=True, fmt='.2f',\n",
    "    cbar_kws={'label': 'Jaccard overlap'}\n",
    ")\n",
    "axes[0, 0].set_title('Significant windows only â€” Jaccard overlap')\n",
    "\n",
    "sns.heatmap(\n",
    "    auc_corr_sig, ax=axes[0, 1], cmap='viridis', vmin=0, vmax=1, annot=True, fmt='.2f',\n",
    "    cbar_kws={'label': 'Pearson r'}\n",
    ")\n",
    "axes[0, 1].set_title('Significant windows only â€” AUC correlation')\n",
    "\n",
    "sns.heatmap(\n",
    "    jaccard_all_df, ax=axes[1, 0], cmap='YlGnBu', vmin=0, vmax=1, annot=True, fmt='.2f',\n",
    "    cbar_kws={'label': 'Jaccard overlap'}\n",
    ")\n",
    "axes[1, 0].set_title('All windows â€” Jaccard overlap')\n",
    "\n",
    "sns.heatmap(\n",
    "    auc_corr_all, ax=axes[1, 1], cmap='viridis', vmin=0, vmax=1, annot=True, fmt='.2f',\n",
    "    cbar_kws={'label': 'Pearson r'}\n",
    ")\n",
    "axes[1, 1].set_title('All windows â€” AUC correlation')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Model')\n",
    "\n",
    "plt.suptitle('Model similarity across significance scope', fontsize=14, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df_all_combined.copy()\n",
    "\n",
    "# Keep only significant windows\n",
    "df_sig = df[df['sig_ttest']]\n",
    "\n",
    "# Define key columns\n",
    "key_cols = ['window_start', 'window_end', 'key', 'session']\n",
    "\n",
    "# Prepare output containers\n",
    "all_jaccards = {}\n",
    "all_auc_corrs = {}\n",
    "\n",
    "# Loop over decoding contrasts\n",
    "for key, df_key in df_sig.groupby('key'):\n",
    "    # --- Jaccard overlaps ---\n",
    "    sig_sets = {\n",
    "        model: set(map(tuple, g[key_cols].values))\n",
    "        for model, g in df_key.groupby('model_name')\n",
    "    }\n",
    "    models = list(sig_sets.keys())\n",
    "    jaccard = np.zeros((len(models), len(models)))\n",
    "    for i, m1 in enumerate(models):\n",
    "        for j, m2 in enumerate(models):\n",
    "            inter = len(sig_sets[m1] & sig_sets[m2])\n",
    "            union = len(sig_sets[m1] | sig_sets[m2])\n",
    "            jaccard[i, j] = inter / union if union > 0 else np.nan\n",
    "\n",
    "    jaccard_df = pd.DataFrame(jaccard, index=models, columns=models)\n",
    "    all_jaccards[key] = jaccard_df\n",
    "\n",
    "    # --- AUC correlations ---\n",
    "    sig_auc = df_key.pivot_table(index=key_cols, columns='model_name', values='mean_auc')\n",
    "    auc_corr = sig_auc.corr(method='pearson')\n",
    "    all_auc_corrs[key] = auc_corr\n",
    "\n",
    "    # --- Plot heatmaps ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    sns.heatmap(jaccard_df, ax=axes[0], annot=True, cmap='YlGnBu', vmin=0, vmax=1)\n",
    "    axes[0].set_title(f'{key}\\nJaccard overlap of significant windows')\n",
    "    sns.heatmap(auc_corr, ax=axes[1], annot=True, cmap='viridis', vmin=0, vmax=1)\n",
    "    axes[1].set_title(f'{key}\\nAUC correlation (significant windows)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## between 2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### both windows and auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "# --- Setup ---\n",
    "df = df_all_combined.copy()\n",
    "key_cols = ['window_start', 'window_end', 'key', 'session']\n",
    "model_a, model_b = 'svm', 'logreg'\n",
    "\n",
    "# --- Helper ---\n",
    "def compute_metrics(df_subset):\n",
    "    results = []\n",
    "    for key, df_key in df_subset.groupby('key'):\n",
    "        sets = {m: set(map(tuple, g[key_cols].values)) for m, g in df_key.groupby('model_name')}\n",
    "        if model_a not in sets or model_b not in sets:\n",
    "            continue\n",
    "\n",
    "        inter = len(sets[model_a] & sets[model_b])\n",
    "        union = len(sets[model_a] | sets[model_b])\n",
    "        jaccard = inter / union if union > 0 else np.nan\n",
    "\n",
    "        sig_auc = df_key.pivot_table(index=key_cols, columns='model_name', values='mean_auc')\n",
    "        if model_a in sig_auc and model_b in sig_auc:\n",
    "            valid = sig_auc[[model_a, model_b]].dropna()\n",
    "            auc_corr = valid[model_a].corr(valid[model_b]) if not valid.empty else np.nan\n",
    "        else:\n",
    "            auc_corr = np.nan\n",
    "\n",
    "        results.append({'key': key, 'jaccard': jaccard, 'auc_corr': auc_corr})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# --- Compute for sig-only and all ---\n",
    "df_sig = df[df['sig_ttest']]\n",
    "res_sig = compute_metrics(df_sig)\n",
    "res_all = compute_metrics(df)\n",
    "\n",
    "df_res = res_sig.merge(res_all, on='key', suffixes=('_sig', '_all')).sort_values('jaccard_sig', ascending=False)\n",
    "\n",
    "# --- Shorten and wrap key labels for readability ---\n",
    "def clean_label(k):\n",
    "    # simplify and shorten text, but keep it all on one line\n",
    "    k = (k.replace('captures_vs_', 'cap vs ')\n",
    "           .replace('first_', '1st ')\n",
    "           .replace('_vs_', ' vs ')\n",
    "           .replace('_', ' '))\n",
    "    return k.strip()\n",
    "\n",
    "\n",
    "df_res['label'] = df_res['key'].apply(clean_label)\n",
    "\n",
    "# --- Prepare tidy format for plotting ---\n",
    "plot_df = pd.melt(\n",
    "    df_res,\n",
    "    id_vars='label',\n",
    "    value_vars=['jaccard_sig', 'auc_corr_sig', 'auc_corr_all'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "metric_labels = {\n",
    "    'jaccard_sig': 'Jaccard (sig-only)',\n",
    "    'auc_corr_sig': 'AUC corr (sig-only)',\n",
    "    'auc_corr_all': 'AUC corr (all)'\n",
    "}\n",
    "plot_df['metric'] = plot_df['metric'].map(metric_labels)\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(13, 5))\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x='label', y='value', hue='metric',\n",
    "    palette=['C0', 'C2', 'gold'], edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Decoding contrast (key)')\n",
    "plt.title(f'Model similarity across decoding contrasts\\n({model_a} vs {model_b})', pad=12)\n",
    "\n",
    "# Rotate x labels neatly\n",
    "plt.xticks(rotation=30, ha='right', fontsize=9)\n",
    "plt.legend(frameon=False, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### just sig windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = df_all_combined.copy()\n",
    "df_sig = df[df['sig_ttest']]\n",
    "key_cols = ['window_start', 'window_end', 'key', 'session']\n",
    "\n",
    "model_a, model_b = 'svm', 'logreg'\n",
    "\n",
    "# Compute per-key Jaccard overlap\n",
    "overlap_scores = {}\n",
    "for key, df_key in df_sig.groupby('key'):\n",
    "    sets = {\n",
    "        m: set(map(tuple, g[key_cols].values))\n",
    "        for m, g in df_key.groupby('model_name')\n",
    "    }\n",
    "    if model_a in sets and model_b in sets:\n",
    "        inter = len(sets[model_a] & sets[model_b])\n",
    "        union = len(sets[model_a] | sets[model_b])\n",
    "        jaccard = inter / union if union > 0 else 0\n",
    "        overlap_scores[key] = jaccard\n",
    "\n",
    "# Convert to tidy DataFrame\n",
    "df_overlap = pd.DataFrame(list(overlap_scores.items()), columns=['key', 'jaccard'])\n",
    "df_overlap = df_overlap.sort_values('jaccard', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=df_overlap, x='key', y='jaccard', palette='viridis')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Jaccard overlap (SVM vs LogReg)')\n",
    "plt.xlabel('Decoding contrast (key)')\n",
    "plt.title('Model agreement across decoding contrasts')\n",
    "for i, v in enumerate(df_overlap['jaccard']):\n",
    "    plt.text(i, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# See best params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## All sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_all_combined' not in globals():\n",
    "    df_all_combined = load_results.load_all_retry_decoder_results(\n",
    "        monkey_name='monkey_Bruno', cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single param\n",
    "cmp_decode.plot_best_params_3d(df_all_combined, 'logreg', param_x='C')\n",
    "\n",
    "# Two params\n",
    "cmp_decode.plot_best_params_3d(df_all_combined, 'svm', param_x='C', param_y='gamma')\n",
    "\n",
    "# Three params\n",
    "cmp_decode.plot_best_params_3d(df_all_combined, 'rf', param_x='max_depth', param_y='min_samples_leaf', param_z='n_estimators')\n",
    "\n",
    "\n",
    "cmp_decode.plot_best_params_3d(df_all_combined, 'logreg_elasticnet', param_x='C', param_y='l1_ratio')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_combined[df_all_combined['model_name'] == 'rf']['best_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## One session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = pn_aligned_by_event.PlanningAndNeuralEventAligned(\n",
    "        raw_data_folder_path=raw_data_folder_path)\n",
    "job_result_dir = Path(pn.retry_decoder_folder_path) / \"runs\"\n",
    "df_all = cmp_decode._load_all_results(job_result_dir, None)\n",
    "cmp_decode.plot_best_logreg_elasticnet_params(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# Reset cluster run progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(progress_path, \"r\") as f:\n",
    "#     progress = json.load(f)\n",
    "# progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "monkey_dir = Path('all_monkey_data/raw_monkey_data/monkey_Bruno')\n",
    "retry_monkey_dir = Path('all_monkey_data/retry_decoder/monkey_Bruno')\n",
    "retry_monkey_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "progress_filename = f\"_decoding_progress_all.json\"\n",
    "progress_path = retry_monkey_dir / progress_filename\n",
    "\n",
    "all_sessions = []\n",
    "if monkey_dir.exists():\n",
    "    all_sessions = sorted([p.name for p in monkey_dir.iterdir() if p.is_dir() and p.name.startswith('data_')])\n",
    "\n",
    "with open(progress_path, \"r\") as f:\n",
    "    progress = json.load(f)\n",
    "    \n",
    "models = progress['models']\n",
    "    \n",
    "progress = {\n",
    "    'monkey': monkey_dir.name if monkey_dir else None,\n",
    "    'all': all_sessions,\n",
    "    'done': [],\n",
    "    'pending': all_sessions,\n",
    "    'last_updated': datetime.now().isoformat(timespec='seconds'),\n",
    "    'keys': progress['keys'],\n",
    "    'models': progress['models'],\n",
    "    'per_model_done': {m: [] for m in models} if models else {},\n",
    "}\n",
    "\n",
    "with open(progress_path, 'w') as f:\n",
    "    json.dump(progress, f, indent=2)\n",
    "\n",
    "print(f\"[SLURM] Progress reset: {progress_path} | done_all=0 | pending_all={len(progress['pending'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# Decode (run locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [(round(t, 1), round(t + 0.1, 1)) for t in np.arange(-0.2, 0.2, 0.1)]\n",
    "df_results = decoding_analysis.run_all_decoding_comparisons_cumulative(\n",
    "    comparisons=comparisons, keys=keys, datasets=datasets, pn=pn, cfg=cfg,\n",
    "    model_name='svm',\n",
    "    n_perm=200,\n",
    "    tune=True,\n",
    "    windows=windows,\n",
    "    save_dir=Path(pn.retry_decoder_folder_path) / 'cum_window_runs',\n",
    "    exists_ok=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['rsw_first_vs_rcap_first', 'rsw_middle_vs_rcap_middle', 'one_stop_vs_both_first_miss', 'will_switch_vs_retry_after_miss', 'switch_vs_retry_after_retry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = 'svm'\n",
    "do_testing = True\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "save_dir = Path(pn.retry_decoder_folder_path) / \"runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#windows = [(t, t + 0.1) for t in np.arange(-0.2, 0.3, 0.1)]\n",
    "# windows = [(round(t, 2), round(t + 0.1, 2)) for t in np.arange(-0.2, 0.3, 0.05)]\n",
    "\n",
    "windows = [[-0.2, 0.2]]\n",
    "\n",
    "out_base = Path(pn.retry_decoder_folder_path) / 'temp_runs'\n",
    "\n",
    "df_results = decoding_analysis.run_all_decoding_comparisons(\n",
    "    comparisons=comparisons,\n",
    "    keys=['rsw_first_vs_rcap_first', 'rsw_middle_vs_rcap_middle',\n",
    "          'one_stop_vs_both_first_miss', 'will_switch_vs_retry_after_miss',\n",
    "          'switch_vs_retry_after_retry'],\n",
    "    datasets=datasets,\n",
    "    pn=pn,\n",
    "    cfg=cfg,\n",
    "    model_name='logreg_elasticnet', # 'logreg_elasticnet'\n",
    "    #model_kwargs={'C': 2.0, 'gamma': 0.05},\n",
    "    tune=False,\n",
    "    k=5,\n",
    "    n_perm=5,          # >0 enables permutation testing\n",
    "    alpha=0.05,\n",
    "    windows=windows,\n",
    "    do_testing=True,    # <â€” set to False to skip all tests,\n",
    "    save_dir=out_base,\n",
    "    overwrite=False,\n",
    "    exists_ok=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, unit_ids = decoding_utils.build_Xy(an, window=(0,0.2))\n",
    "# real_auc, auc_null, p_perm = decoding_utils.permutation_test_auc(X, y)\n",
    "# mean_auc, sd, tstat, p_ttest, aucs = decoding_utils.ttest_auc_folds(X, y)\n",
    "# print(f\"Permutation p={p_perm:.4f}, t-test p={p_ttest:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "# Plot heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['mean_auc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub = df_results[df_results['align_by_stop_end']==True].copy()\n",
    "# df_results_sub = df_results_sub[df_results_sub['key'].isin(keys)]\n",
    "\n",
    "plot_decoding.plot_decoding_auc_heatmap(\n",
    "    df_results_sub, threshold=0.55, cmap='magma', title='Align by stop end',\n",
    "    x_tick_position='center',\n",
    "    sig_col='p_perm_sig',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = df_results.sort_values(by=['key', 'a_label', 'b_label', 'window_start', 'align_by_stop_end', 'n_perm']).groupby(['key', 'a_label', 'b_label', 'window_start', 'align_by_stop_end']).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub[(df_results_sub['a_label'] == 'rsw first') & (df_results_sub['b_label'] == 'rcap first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub = df_results[df_results['align_by_stop_end']==False].copy()\n",
    "df_results_sub = df_results_sub[df_results_sub['key'].isin(keys)]\n",
    "plot_decoding.plot_decoding_auc_heatmap(\n",
    "    df_results_sub, threshold=0.55, cmap='magma', title='Align by stop beginning',\n",
    "    sig_col='p_perm_sig'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sub = df_results[df_results['align_by_stop_end']==True].copy()\n",
    "df_results_sub = df_results_sub[df_results_sub['key'].isin(keys)]\n",
    "\n",
    "\n",
    "plot_decoding.plot_decoding_timecourse(\n",
    "    df_results_sub,\n",
    "    groupby_cols=('align_by_stop_end',),\n",
    "    split_by=('a_label','b_label'),\n",
    "    value_col='mean_auc',\n",
    "    sig_col='p_perm_sig',\n",
    "    err_col='sd_auc',\n",
    "    err_type='sem',\n",
    "    title_prefix='Decoding timecourse per comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decoding.plot_decoding_timecourse(\n",
    "    df_results,\n",
    "    groupby_cols=('a_label','b_label'),\n",
    "    split_by=('align_by_stop_end'),\n",
    "    title_prefix='Decoding per alignment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[(df_results['a_label']=='one stop miss') & (df_results['b_label']=='both first miss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = decoding_utils.run_decoding(\n",
    "#     an,\n",
    "#     model_name='svm',\n",
    "#     tune=False,\n",
    "#     model_kwargs={'C': 2.0, 'gamma': 0.05}\n",
    "# )\n",
    "\n",
    "# res = decoding_utils.run_decoding(an, model_name='rf', tune=False, search='grid')\n",
    "\n",
    "# res = run_decoding(an, model_name='mlp', tune=False, search='random', n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "# PSTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = core_stops_psth.PSTHConfig(\n",
    "    pre_window=0.5,\n",
    "    post_window=0.5,\n",
    "    bin_width=0.04,\n",
    "    smoothing_sigma=0.1,\n",
    "    min_trials=5,\n",
    "    normalize=\"zscore\",            # try: None, \"sub\", or \"div\"\n",
    ")\n",
    "\n",
    "\n",
    "keys = ['rsw_first_vs_rcap_first', 'rsw_middle_vs_rcap_middle', 'one_stop_vs_both_first_miss', 'switch_vs_retry_after_retry']\n",
    "comparisons_sub = [c for c in comparisons if c['key'] in keys]\n",
    "\n",
    "runs = compare_events.run_all_comparisons(\n",
    "    comparisons_sub, datasets, pn.spikes_df, pn.monkey_information, cfg,\n",
    "    align_by_stop_end=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
