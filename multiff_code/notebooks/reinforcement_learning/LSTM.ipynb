{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "KMRCk6qpFsyH"
   },
   "source": [
    "Source of LSTM codes:\n",
    "https://github.com/quantumiracle/Popular-RL-Algorithms/blob/master/sac_v2_lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent import MlpLstmPolicy, CnnLstmPolicy, MultiInputLstmPolicy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "93C64_FAEoSJ"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "06f9c7f3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "for p in [Path.cwd()] + list(Path.cwd().parents):\n",
    "    if p.name == 'Multifirefly-Project':\n",
    "        os.chdir(p)\n",
    "        sys.path.insert(0, str(p / 'multiff_analysis/multiff_code/methods'))\n",
    "        break\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from data_wrangling import specific_utils, process_monkey_information, base_processing_class\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_utils, plot_cluster_replacement\n",
    "from decision_making_analysis.decision_making import decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, add_features_GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from visualization.matplotlib_tools import plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics\n",
    "from visualization.animation import animation_func, animation_utils, animation_class\n",
    "from null_behaviors import sample_null_distributions, show_null_trajectory\n",
    "from machine_learning.ml_methods import regression_utils, classification_utils, prep_ml_data_utils, hyperparam_tuning_class\n",
    "from reinforcement_learning.base_classes import env_utils, base_env, more_envs, rl_base_class, rl_base_utils\n",
    "from reinforcement_learning.agents.rnn import gru_utils, lstm_utils, lstm_utils, lstm_class\n",
    "from reinforcement_learning.agents.rppo import rppo_class\n",
    "from reinforcement_learning.agents.feedforward import interpret_neural_network, sb3_class, sb3_utils\n",
    "from eye_position_analysis import eye_positions\n",
    "from neural_data_analysis.neural_analysis_tools.model_neural_data import neural_data_modeling\n",
    "from decision_making_analysis.compare_GUAT_and_TAFT import find_GUAT_or_TAFT_trials\n",
    "from reinforcement_learning.agents.feedforward import interpret_neural_network, sb3_class, sb3_utils\n",
    "\n",
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gymnasium import spaces, Env\n",
    "import torch\n",
    "import optuna\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.linalg import vector_norm\n",
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import gc\n",
    "from importlib import reload\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device_idx = 0\n",
    "# device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "## if using Jupyter Notebook\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "PLAYER = \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# RPPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Iterate through agents stored in 'RL_models/RPPO_stored_models/obs4_mem3_rank_keep_160627' that starts with 'dv', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agents_folder = 'multiff_analysis/RL_models/RPPO_stored_models/all_agents'\n",
    "# list the directories in all_agents_folder\n",
    "all_agents = os.listdir(all_agents_folder)\n",
    "all_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall_folder = 'multiff_analysis/RL_models/RPPO_stored_models/all_agents/lr3e-4_numobs3_mem2_stratdrop_fill_nenvs1_job160109_6/'\n",
    "#overall_folder = 'multiff_analysis/RL_models/RPPO_stored_models/all_agents/example2/'\n",
    "#overall_folder =\"multiff_analysis/RL_models/RPPO_stored_models/obs4_mem3_rank_keep_160627/dv0p2_dw5p0_w2p0\"\n",
    "overall_folder =\"multiff_analysis/RL_models/RPPO_stored_models/quick_test\"\n",
    "env_kwargs = {'num_obs_ff': 1,\n",
    "              'add_action_to_obs': True,\n",
    "              'angular_terminal_vel': 0.05,\n",
    "              \"dt\": 0.1,\n",
    "              \"max_in_memory_time\": 2,\n",
    "            }   \n",
    "rl = rppo_class.RPPOforMultifirefly(overall_folder=overall_folder,\n",
    "                                                **env_kwargs)\n",
    "\n",
    "\n",
    "rl.streamline_everything(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 40],\n",
    "                         to_load_latest_agent=True,\n",
    "                         best_model_postcurriculum_exists_ok=True,\n",
    "                         to_train_agent=True,\n",
    "                         use_curriculum_training=True\n",
    "                         )\n",
    "                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## see animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(os.path.normpath(overall_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.env_for_data_collection.dv_cost_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"multiff_analysis/RL_models/RPPO_stored_models/all_agents/ff5_mem3_drop_fill_lr3e-4_nenvs1_job160811_16\"\n",
    "base = Path(base_path).expanduser()\n",
    "\n",
    "# If this run has fine-tunes under ft/, list those as agents\n",
    "target = base / \"ft\" if (base / \"ft\").is_dir() else base\n",
    "print(f\"[info] Listing agents under: {target}\")\n",
    "\n",
    "# Collect immediate child directories\n",
    "dirs = [p for p in target.iterdir() if p.is_dir()]\n",
    "dirs.sort(key=lambda p: p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(agent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## just to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "col = 'num_stops'\n",
    "\n",
    "cost_results2 = cost_results[cost_results['agent_name'].str.contains('175252', na=False)]\n",
    "\n",
    "df_mean = cost_results2.groupby(['dv', 'jerk', 'stop'], as_index=False)[col].mean()\n",
    "\n",
    " \n",
    "unique_dv = sorted(df_mean['dv'].unique())\n",
    "\n",
    "for dv_value in unique_dv:\n",
    "    df_plot = df_mean[df_mean['dv'] == dv_value].pivot(index='stop', columns='jerk', values=col)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(df_plot, annot=True, cmap='magma', fmt='.0f')\n",
    "    plt.title(f'dv = {dv_value}')\n",
    "    plt.xlabel('jerk')\n",
    "    plt.ylabel('stop')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cost_results = pd.DataFrame()\n",
    "for agent_dir in dirs:\n",
    "    agent_name = os.path.basename(agent_dir)\n",
    "    print('==============================================')\n",
    "    print(agent_name)\n",
    "    \n",
    "    if '175252' not in agent_name:\n",
    "        continue\n",
    "    \n",
    "    rl = rppo_class.RPPOforMultifirefly(overall_folder=str(agent_dir))\n",
    "\n",
    "    rl.collect_data(n_steps=500, exists_ok=False)\n",
    "    \n",
    "    num_caught_ff = len(rl.ff_caught_T_sorted)\n",
    "        \n",
    "    result = {'dv': rl.env_for_data_collection.dv_cost_factor, \n",
    "              'jerk': rl.env_for_data_collection.jerk_cost_factor, \n",
    "              'stop': rl.env_for_data_collection.cost_per_stop, \n",
    "              'num_caught_ff': num_caught_ff,\n",
    "              'num_stops': (rl.monkey_information['monkey_speeddummy']==0).sum(),\n",
    "              'agent_name': agent_name,\n",
    "              }\n",
    "    cost_results = pd.concat([cost_results, pd.DataFrame([result])], ignore_index=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## to make animation too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "for agent_dir in dirs:\n",
    "    agent_name = os.path.basename(agent_dir)\n",
    "    print('==============================================')\n",
    "    print(agent_name)\n",
    "    \n",
    "\n",
    "    \n",
    "    rl = rppo_class.RPPOforMultifirefly(overall_folder=str(agent_dir))\n",
    "\n",
    "    rl.streamline_making_animation(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 20], n_steps=250, video_dir=None,\n",
    "                                  data_exists_ok=True, display_inline=False,\n",
    "                                  save_video=False)\n",
    "\n",
    "\n",
    "    num_caught_ff = len(rl.ff_caught_T_sorted)\n",
    "\n",
    "    # Convert animation to HTML\n",
    "    html_str = rl.anim.to_jshtml()\n",
    "\n",
    "    video_path_name = os.path.join(base_path, 'derived_agent_animation', f'{agent_name}_C_{num_caught_ff}.html')\n",
    "    os.makedirs(os.path.dirname(video_path_name), exist_ok=True)\n",
    "    with open(video_path_name, 'w') as f:\n",
    "        f.write(html_str)\n",
    "        print(f\"Animation saved to {video_path_name}\")\n",
    "        \n",
    "    print(rl.monkey_information['speed'].values[:30])\n",
    "        \n",
    "\n",
    "    # from IPython.display import HTML\n",
    "    # HTML(rl.anim.to_jshtml()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.monkey_information['speed'].values[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# # Ensure env/agent exist; your animation call likely created them\n",
    "# # If needed:\n",
    "# # rl.make_env(**env_kwargs)\n",
    "# # rl.make_agent()\n",
    "\n",
    "# avg_ret, std_ret = evaluate_policy(\n",
    "#     rl.rl_agent,               # RecurrentPPO model\n",
    "#     rl.env,                    # VecMonitor-wrapped env\n",
    "#     n_eval_episodes=10,\n",
    "#     deterministic=True,\n",
    "# )\n",
    "# print(f\"Avg return: {avg_ret:.2f} ± {std_ret:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl.make_env(**rl.env_kwargs)\n",
    "# rl.make_agent()\n",
    "# # rl.make_initial_env_for_curriculum_training()\n",
    "# rl.load_best_model_postcurriculum()\n",
    "rl.streamline_making_animation(currentTrial_for_animation=None, num_trials_for_animation=None, duration=[10, 12], n_steps=1000, video_dir=None,\n",
    "                               data_exists_ok=False, display_inline=False,\n",
    "                               save_video=True)\n",
    "\n",
    "# Convert animation to HTML\n",
    "html_str = rl.anim.to_jshtml()\n",
    "with open(self.video_path_name, 'w') as f:\n",
    "    f.write(html_str)\n",
    "    \n",
    "# from IPython.display import Video\n",
    "# Video(rl.video_path_name, embed=True)\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# HTML(rl.anim.to_jshtml())  # or HTML(your_instance.anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## manually eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lstm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = lstm_utils.evaluate_agent(\n",
    "    rl.env, rl.rl_agent, 512, 2, deterministic=True)\n",
    "print(\n",
    "    f\"Best average reward: {rl.best_avg_reward}, Current average reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Sweep results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "log_dir = 'multiff_analysis/logs/rppo/run_curriculum/160811'\n",
    "files = [f for f in glob.glob(os.path.join(log_dir, '*.csv'))]\n",
    "\n",
    "# log_dir = 'multiff_analysis/logs/rppo/run_summary/160811'\n",
    "# files = [f for f in glob.glob(os.path.join(log_dir, '*.csv'))]\n",
    "\n",
    "if len(files) == 0:\n",
    "    raise ValueError(f\"No files found in {log_dir}\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"File {f} is empty\")\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"File {f} has no valid timestamps\")\n",
    "        \n",
    "\n",
    "    df = df.sort_values('stage')\n",
    "    last_stage = df.iloc[-1]\n",
    "\n",
    "    train_time = (df['timestamp'].iloc[-1] - df['timestamp'].iloc[0]).total_seconds() / 60  # minutes\n",
    "\n",
    "    # Extract run ID (last part of overall_folder)\n",
    "    run_id = os.path.basename(str(last_stage['overall_folder']))\n",
    "\n",
    "    records.append({\n",
    "        'file': os.path.basename(f),\n",
    "        'run_id': run_id,  # ← added here\n",
    "        'agent_type': last_stage['agent_type'],\n",
    "        'lr': last_stage['param_lr'],\n",
    "        'num_obs_ff': last_stage['param_num_obs_ff'],\n",
    "        'max_in_memory_time': last_stage['param_max_in_memory_time'],\n",
    "        'strategy': last_stage['param_identity_slot_strategy'],\n",
    "        'n_envs': last_stage['param_n_envs'],\n",
    "        'angular_terminal_vel': last_stage['param_angular_terminal_vel'],\n",
    "        #'final_stage': last_stage['stage'],\n",
    "        'best_reward_last': last_stage['best_avg_reward'],\n",
    "        'reward_threshold': last_stage['reward_threshold'],\n",
    "        'flash_on_interval': last_stage['flash_on_interval'],\n",
    "        'train_time_min': train_time,\n",
    "        'finished_curriculum': last_stage['finished_curriculum'],\n",
    "        'overall_folder': last_stage['overall_folder']\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(records)\n",
    "summary = summary.sort_values('best_reward_last', ascending=False)\n",
    "\n",
    "print(summary[['run_id', 'num_obs_ff', 'strategy', 'best_reward_last', 'train_time_min', 'max_in_memory_time']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary[summary['finished_curriculum'] == True].copy()\n",
    "# summary.loc[summary['finished_curriculum'] == False, 'train_time_min'] = max(summary['train_time_min'] + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- Setup ---\n",
    "strategies = summary['strategy'].unique()\n",
    "palette = sns.color_palette('tab10', len(strategies))\n",
    "color_map = dict(zip(strategies, palette))\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# --- Marker size scaling ---\n",
    "min_size, max_size = 50, 300  # scatter point area (points^2)\n",
    "summary['marker_size'] = np.interp(\n",
    "    summary['max_in_memory_time'],\n",
    "    (summary['max_in_memory_time'].min(), summary['max_in_memory_time'].max()),\n",
    "    (min_size, max_size)\n",
    ")\n",
    "\n",
    "# --- Scatter plot ---\n",
    "for strat in strategies:\n",
    "    subset = summary[summary['strategy'] == strat]\n",
    "    plt.scatter(\n",
    "        subset['train_time_min'],\n",
    "        subset['best_reward_last'],\n",
    "        label=strat,\n",
    "        color=color_map[strat],\n",
    "        alpha=0.8,\n",
    "        edgecolor='black',\n",
    "        s=subset['marker_size']\n",
    "    )\n",
    "\n",
    "# --- Text annotations (num_obs_ff) ---\n",
    "# Label all if few points, or just a representative subset if large\n",
    "if len(summary) <= 100:\n",
    "    annotated = summary\n",
    "else:\n",
    "    annotated = summary.nlargest(15, 'best_reward_last')  # top 15 for readability\n",
    "\n",
    "for _, row in annotated.iterrows():\n",
    "    plt.text(row['train_time_min'] + 0.5,\n",
    "             row['best_reward_last'] + 3,\n",
    "             str(row['num_obs_ff']),\n",
    "             fontsize=10,\n",
    "             alpha=0.8)\n",
    "\n",
    "# --- Legends ---\n",
    "# Color legend (strategy)\n",
    "color_handles = [\n",
    "    Line2D([0], [0], marker='o', color='w',\n",
    "           markerfacecolor=color_map[strat], markeredgecolor='black',\n",
    "           label=strat, markersize=10)\n",
    "    for strat in strategies\n",
    "]\n",
    "\n",
    "# Size legend (max_in_memory_time)\n",
    "size_values = np.linspace(summary['max_in_memory_time'].min(),\n",
    "                          summary['max_in_memory_time'].max(), 3)\n",
    "size_handles = [\n",
    "    Line2D([0], [0], marker='o', color='w',\n",
    "           markerfacecolor='gray', markeredgecolor='black',\n",
    "           label=f'{val:.1f} s',\n",
    "           markersize=np.sqrt(np.interp(val,\n",
    "                                        (summary['max_in_memory_time'].min(),\n",
    "                                         summary['max_in_memory_time'].max()),\n",
    "                                        (min_size, max_size))) / 2)\n",
    "    for val in size_values\n",
    "]\n",
    "\n",
    "first_legend = plt.legend(handles=color_handles, title='Strategy',\n",
    "                          loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "second_legend = plt.legend(handles=size_handles, title='Max In-Memory Time',\n",
    "                           loc='lower left', bbox_to_anchor=(1.02, 0))\n",
    "plt.gca().add_artist(first_legend)\n",
    "\n",
    "# --- Aesthetics ---\n",
    "plt.xlabel('Training Time (minutes)')\n",
    "plt.ylabel('Best Avg Reward (Last Stage)')\n",
    "plt.title('Reward vs. Training Time by Strategy')\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=summary, x='strategy', y='best_reward_last', hue='num_obs_ff', palette='muted')\n",
    "plt.ylabel('Best Avg Reward')\n",
    "plt.title('Final Performance by Strategy and num_obs_ff')\n",
    "plt.legend(title='num_obs_ff')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    data=summary,\n",
    "    x='strategy',\n",
    "    y='train_time_min',\n",
    "    hue='num_obs_ff',\n",
    "    palette='muted'\n",
    ")\n",
    "plt.ylabel('Training Time (minutes)')\n",
    "plt.xlabel('Identity Slot Strategy')\n",
    "plt.title('Total Training Time by Strategy and num_obs_ff')\n",
    "plt.legend(title='num_obs_ff')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "PkFUlYfELK_e"
   },
   "source": [
    "# Optuna (LSTM)\n",
    "\n",
    "(my own codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "d0vtL5qnLQ0e"
   },
   "source": [
    "##### sample_sac_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "CM9Yhsb7LQ0e"
   },
   "outputs": [],
   "source": [
    "def sample_sac_params(trial):\n",
    "    \"\"\"\n",
    "    Sampler for SAC hyperparams.\n",
    "    :param trial: (optuna.trial)\n",
    "    :return: (dict)\n",
    "    \"\"\"\n",
    "  \n",
    "    gamma = 1.0 - trial.suggest_float(\"1-gamma\", 1e-4, 0.1, log=True)\n",
    "    soft_q_lr = trial.suggest_float(\"soft_q_lr\", 1e-5, 1, log=True)\n",
    "    policy_lr = trial.suggest_float(\"policy_lr\", 1e-5, 1, log=True)\n",
    "    alpha_lr  = trial.suggest_float(\"alpha_lr\", 1e-5, 1, log=True)\n",
    "    batch_size  = trial.suggest_categorical('batch_size', [5, 10, 15, 20, 25, 30])\n",
    "    update_itr = trial.suggest_categorical('update_itr', [1, 2, 3, 5])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64, 100, 150, 200, 256])\n",
    "    reward_scale = trial.suggest_categorical('reward_scale', [1, 3, 5, 10, 15, 20]) # I updated after running\n",
    "    target_entropy = trial.suggest_categorical('target_entropy', [-1, -2, -3, -5, -8, -10]) # I updated after running\n",
    "    soft_tau= trial.suggest_float(\"soft_tau\", 1e-6, 1, log=True)\n",
    "    #activation_fn\n",
    "\n",
    "\n",
    "    return {\n",
    "        'gamma': gamma,\n",
    "        'soft_q_lr':soft_q_lr,\n",
    "        'policy_lr':policy_lr,\n",
    "        'alpha_lr':alpha_lr,\n",
    "        'batch_size': batch_size,\n",
    "        'update_itr':update_itr,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'reward_scale':reward_scale,\n",
    "        'target_entropy':target_entropy,\n",
    "        'soft_tau':soft_tau\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "w8snZtlopA-9"
   },
   "source": [
    "## put_in_fixed_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "1RFcdEk5pAmv"
   },
   "outputs": [],
   "source": [
    "def put_in_fixed_params():\n",
    "    return {\n",
    "        'model_folder_name':  None, \n",
    "        'train_freq': 100, \n",
    "        'batch_size': 10, \n",
    "        'update_itr': 1,\n",
    "        'num_train_episodes': 50, \n",
    "        'eval_eps_freq': 10, \n",
    "        'max_steps_per_eps': 1024, \n",
    "        'auto_entropy': True, \n",
    "        'DETERMINISTIC': False, \n",
    "        'num_eval_episodes': 3, \n",
    "        'print_episode_reward':  True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "9TMGTKP3LTVV"
   },
   "source": [
    "## objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "NhoGtlzHqxKY"
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float: \n",
    "  try:\n",
    "    # Sample hyperparameters\n",
    "    kwargs = sample_sac_params(trial)\n",
    "    kwargs = put_in_fixed_params()\n",
    "\n",
    "    num_train_episodes = kwargs['num_train_episodes'] \n",
    "    eval_eps_freq = kwargs['eval_eps_freq']\n",
    "    max_steps_per_eps = kwargs['max_steps_per_eps'] \n",
    "    auto_entropy = kwargs['auto_entropy'] \n",
    "    num_eval_episodes = kwargs['num_eval_episodes'] \n",
    "    print_episode_reward = kwargs['print_episode_reward']\n",
    "\n",
    "\n",
    "    env = rnn_env.EnvForRNN()\n",
    "    rl_agent = lstm_utils.SAC_Trainer(**kwargs)\n",
    "    rl2 = lstm_class.LSTMforMultifirefly()\n",
    "    rl2.rl_agent = rl_agent\n",
    "    rl_agent, best_avg_reward_record, alpha_df = rl2.train_agent(env, device,\n",
    "                                                         num_train_episodes=num_train_episodes, eval_eps_freq=eval_eps_freq, max_steps_per_eps=max_steps_per_eps, \n",
    "                                                         num_eval_episodes=num_eval_episodes, print_episode_reward=print_episode_reward, auto_entropy=auto_entropy)\n",
    "\n",
    "  except ValueError as e:\n",
    "    # Sometimes, random hyperparams can generate NaN\n",
    "      print(e)\n",
    "\n",
    "  return best_avg_reward_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "zWLQGb9cW2TQ"
   },
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "L4wV8ncwW2TQ"
   },
   "outputs": [],
   "source": [
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    " \n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "## Do not prune before 1/3 of the max budget is used\n",
    "# pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS 2//3 3)\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1FShUys0iUmi3huyQwtaEdyhGivLG2R_5",
     "timestamp": 1681095282536
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "multiff_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
