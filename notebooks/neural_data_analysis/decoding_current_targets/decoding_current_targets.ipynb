{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r multiff_analysis/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_folder = '/Users/dusiyi/Documents/Multifirefly-Project'\n",
    "os.chdir(project_folder)\n",
    "sys.path.append(os.path.join(project_folder, 'multiff_analysis', 'methods'))\n",
    "\n",
    "from data_wrangling import general_utils, specific_utils, process_monkey_information\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from non_behavioral_analysis.neural_data_analysis.get_neural_data import neural_data_processing\n",
    "from non_behavioral_analysis.neural_data_analysis.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from non_behavioral_analysis.neural_data_analysis.model_neural_data import neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from non_behavioral_analysis.neural_data_analysis.model_neural_data.cca_methods import cca_class, cca_utils, cca_plotting, cca_lag_vs_no_lag_plotting, cca_cv_utils, cca_cv_plotting\n",
    "from non_behavioral_analysis.neural_data_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from non_behavioral_analysis.neural_data_analysis.planning_neural import planning_neural_class, planning_neural_utils\n",
    "from non_behavioral_analysis.neural_data_analysis.decode_targets import behav_features_to_keep, decode_target_class, plot_gpfa_utils, decode_target_utils, fit_gpfa_utils, gpfa_regression_utils\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "from numpy import var\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "# To fit gpfa\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from elephant.gpfa import gpfa_core, gpfa_util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Bruno/data_0328\"\n",
    "dec = decode_target_class.DecodeTargetClass(raw_data_folder_path=raw_data_folder_path,\n",
    "                                                               bin_width=0.02, window_width=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.streamline_making_behav_and_neural_data(exists_ok=True)\n",
    "dec.get_x_and_y_var(exists_ok=True)\n",
    "dec._free_up_memory()\n",
    "dec.pursuit_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.x_var_lags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_lags.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce columns in lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec.reduce_x_var_lags() # this has been shown to not do anything\n",
    "dec.reduce_y_var_lags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get final vif\n",
    "# vif_df = drop_high_vif_vars.get_vif_df(dec.y_var_lags_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data for GPFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.single_vis_target_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.prepare_spikes_for_gpfa()\n",
    "\n",
    "print(len(dec.spiketrains))\n",
    "print(len(dec.spiketrains[0]))\n",
    "print(len(dec.spiketrains[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit gpfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dec.spiketrains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.get_gpfa_traj(latent_dimensionality=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_uniform_color()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, enable interactive mode in your notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create the interactive plot\n",
    "fig, ax = plot_gpfa_utils.plot_gpfa_traj_3d(\n",
    "    trajectories=dec.trajectories,\n",
    "    figsize=(15, 5),\n",
    "    linewidth_single_trial=0.75,\n",
    "    alpha_single_trial=0.3,\n",
    "    linewidth_trial_average=2,\n",
    "    title='Latent dynamics extracted by GPFA',\n",
    "    view_azim=-5,\n",
    "    view_elev=60\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_gpfa_utils.plot_gpfa_traj_3d_plotly(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variance explained by each latent dimension\n",
    "traj_stack = np.stack(dec.trajectories, axis=0)  # shape: (n_trials, 3, T)\n",
    "var_by_dim = var(traj_stack, axis=(0, 2))    # variance across trials and time\n",
    "var_by_dim /= var_by_dim.sum()               # normalize to get explained variance ratio\n",
    "print(\"Variance explained by each latent dimension:\", var_by_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.set_title('Latent dynamics extracted by GPFA')\n",
    "ax.set_xlabel('Time [s]')\n",
    "\n",
    "average_trajectory = np.mean(, axis=0)\n",
    "time = np.arange(len(average_trajectory[0])) * 0.02  # assuming all trajectories have the same length\n",
    "\n",
    "for i, x in enumerate(average_trajectory):\n",
    "    ax.plot(time, x, label=f'Dim {i+1}')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.common_t_stop = max(dec.spike_segs_df['t_duration']) * pq.s + dec.bin_width_w_unit        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.get_gpfa_and_behav_data_for_all_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timepoints = int(1.5/0.02)\n",
    "scores_by_time, times = gpfa_regression_utils.time_resolved_regression_variable_length(dec.gpfa_trials, dec.behavior_trials, time_step=0.02, cv_folds=5, max_timepoints=max_timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_labels = dec.pursuit_data_by_trial.drop(columns=['segment']).columns\n",
    "len(behavior_labels)\n",
    "scores_by_time_df = pd.DataFrame(scores_by_time, columns=behavior_labels)\n",
    "# see the percentage of 1 of this dummy variable\n",
    "dec.pursuit_data_by_trial[['whether_new_distinct_stop']].sum()/len(dec.pursuit_data_by_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpfa_regression_utils.plot_time_resolved_scores(scores_by_time, times, behavior_labels=behavior_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_counts = np.array([sum(latent.shape[0] > t for latent in gpfa_trials)\n",
    "                         for t in range(scores_by_time.shape[0])])\n",
    "plt.plot(times, trial_counts)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Trials with data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# behav features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find set difference between dec.behav_data_all.columns and dec.behav_data.columns\n",
    "diff_columns = set(dec.behav_data_all.columns) - set(dec.behav_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check result of reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## also check correlations between x vars without lags\n",
    "high_corr_pair_df, top_n_corr_df = drop_high_corr_vars.get_pairs_of_columns_w_high_corr(\n",
    "            dec.x_var, corr_threshold=0.8)\n",
    "top_n_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression (didn't modify yet)\n",
    "\n",
    "Regressing the behavioral variables individually (as y_var) against all neural activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put results in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dec.make_or_retrieve_y_var_lr_resault_df(exists_ok=True)\n",
    "dec.y_var_lr_result_df = neural_data_modeling.get_y_var_lr_result_df(\n",
    "                dec.x_var_lags_reduced, dec.y_var)\n",
    "dec.y_var_lr_result_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  plot all neural clusters vs one behavioral var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop existing columns involving bin (most likely there's only one or zero after being reduced, because different lags of bins can have very high correlations)\n",
    "bin_cols = [col for col in dec.y_var_lags_reduced.columns if 'bin' in col]\n",
    "dec.y_var_lags_reduced.drop(columns=bin_cols, inplace=True)\n",
    "\n",
    "# then we add the variable bin (so that only the 0 lag is used)\n",
    "dec.y_var_lags_reduced['bin'] = dec.y_var_lags['bin_0'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# conduct linear regression on X and y\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "bins_to_plot = dec.y_var_lags_reduced['bin'].values\n",
    "for i, column in enumerate(dec.y_var_lags_reduced.columns):\n",
    "\n",
    "    plot_neural_data.plot_regression(dec.y_var_lags_reduced, column, dec.x_var_lags_reduced, bins_to_plot=bins_to_plot, min_r_squared_to_plot=0.3)\n",
    "    # if i == 3:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  plot one neural cluster vs one behavioral var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one neural cluster against one behavioral variable\n",
    "cluster_num, behavioral_column = 6, 'monkey_speed'\n",
    "bins_to_plot = range(1000, 1200)\n",
    "x_values = dec.binned_spikes_df.loc[bins_to_plot, f'unit_{cluster_num}'].values\n",
    "y_values = dec.pursuit_data[behavioral_column][bins_to_plot]\n",
    "reg = LinearRegression().fit(x_values.reshape(-1, 1), y_values)\n",
    "\n",
    "plt.scatter(x_values, y_values, color='blue', s=1)\n",
    "plt.plot(x_values, reg.predict(x_values.reshape(-1, 1)), color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGAM (unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorize variables\n",
    "dec.y_var_reduced.columns\n",
    "temporal_vars = ['time_rel_to_stop',\n",
    " 'time_when_nxt_ff_first_seen_rel_to_stop',\n",
    " 'time_when_cur_ff_first_seen_rel_to_stop',\n",
    " 'time_when_nxt_ff_last_seen_rel_to_stop',\n",
    " 'time_when_cur_ff_last_seen_rel_to_stop',\n",
    " ]\n",
    "\n",
    "spatial_vars = [x for x in dec.y_var_reduced.columns if x not in temporal_vars]\n",
    "spatial_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparsity of neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.binned_spikes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect neural data\n",
    "\n",
    "bins = dec.binned_spikes_df\n",
    "\n",
    "# Calculate percentage of non-zero rows for each column\n",
    "non_zero_percentages = (bins != 0).mean() * 100\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "non_zero_df = pd.DataFrame({\n",
    "    'Column': non_zero_percentages.index,\n",
    "    'Percent_Non_Zero': non_zero_percentages.values\n",
    "})\n",
    "\n",
    "# Sort by percentage in descending order\n",
    "non_zero_df = non_zero_df.sort_values('Percent_Non_Zero', ascending=False)\n",
    "\n",
    "print(\"Percentage of non-zero values in each column:\")\n",
    "print(non_zero_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.drop(columns='bin').mean(axis=1).describe()\n",
    "\n",
    "# plot the percentile of values of mean firing rates across neurons at each time bin\n",
    "mean_rates = bins.drop(columns='bin').mean(axis=1)\n",
    "\n",
    "# Calculate percentiles from 0 to 100\n",
    "percentiles = np.arange(0, 101, 1)\n",
    "percentile_values = np.percentile(mean_rates, percentiles)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(percentiles, percentile_values)\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Mean Firing Rate')\n",
    "plt.title('Distribution of Mean Firing Rates Across Neurons')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y var (behavioral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_vif = drop_high_vif_vars.get_vif_df(dec.y_var)\n",
    "print(y_var_vif.head(8))\n",
    "\n",
    "# calculate the correlation coefficient among the columns with VIF > 5\n",
    "# specific_columns = vif_df[vif_df['vif'] > 5].feature.values\n",
    "specific_columns = y_var_vif.feature.values[:10]\n",
    "corr_coeff = dec.y_var[specific_columns].corr()\n",
    "#plt.figure(figsize = (6, 6))\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try y_var_reduced\n",
    "\n",
    "y_var_vif = drop_high_vif_vars.get_vif_df(dec.y_var_reduced)\n",
    "print(y_var_vif.head(8))\n",
    "\n",
    "# calculate the correlation coefficient among the columns with VIF > 5\n",
    "# specific_columns = vif_df[vif_df['vif'] > 5].feature.values\n",
    "specific_columns = y_var_vif.feature.values[:10]\n",
    "corr_coeff = dec.y_var[specific_columns].corr()\n",
    "#plt.figure(figsize = (6, 6))\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trial segments in pursuit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.matplotlib_tools import plot_trials,\n",
    "dec.make_PlotTrials_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]                     \n",
    "\n",
    "max_plot_to_make = 2\n",
    "plot_counter = 0\n",
    "\n",
    "for index, row in dec.single_vis_target_df.iloc[2:].iterrows():\n",
    "\n",
    "    duration = [row['last_vis_time'], row['ff_caught_time']]\n",
    "\n",
    "    returned_info = plot_trials.PlotTrials(\n",
    "                duration, \n",
    "                *dec.PlotTrials_args,  \n",
    "                adjust_xy_limits=True,       \n",
    "                minimal_margin=50,\n",
    "                show_reward_boundary=True,\n",
    "                show_alive_fireflies=False,\n",
    "                show_visible_fireflies=True,\n",
    "                show_in_memory_fireflies=True,\n",
    "                show_believed_target_positions=True,\n",
    "                )\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plot_counter += 1\n",
    "    if plot_counter >= max_plot_to_make:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check target_rel_x and y\n",
    "(The look correct after checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pursuit_sub = dec.pursuit_data.loc[dec.pursuit_data['target_index']==65].copy()\n",
    "pursuit_sub['target_angle_deg'] = pursuit_sub['target_angle'] * 180/pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pursuit_sub[['point_index', 'target_angle_deg', 'target_distance', 'target_rel_x', 'target_rel_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing columns in lags: experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dec.y_var_lags_reduced.copy()\n",
    "# for target_feature in vif_df[vif_df['vif'] > 5].feature.values:\n",
    "for index, row in vif_df.iterrows():\n",
    "    target_feature = row.feature\n",
    "    print(f'\\n\\n {target_feature}: VIF = {row.vif}')\n",
    "    contributions = drop_high_vif_vars.check_vif_contribution(df, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check subset's vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use y_var_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in dec.y_var_lags.columns if ('LD' in col) or (\n",
    "                'RD' in col) or ('gaze_mky_view_angle' in col)]\n",
    "\n",
    "df_sub = dec.y_var_lags[columns].copy()\n",
    "print(df_sub.columns)\n",
    "sub_vif = drop_high_vif_vars.get_vif_df(df_sub)\n",
    "sub_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use y_var_lags_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [col for col in dec.y_var_lags_reduced.columns if ('LD' in col) or (\n",
    "#                 'RD' in col) or ('gaze_mky_view_angle' in col)]\n",
    "\n",
    "columns = [col for col in dec.y_var_lags_reduced.columns if ('x_r' in col) or ('y_r' in col)]\n",
    "\n",
    "df_sub = dec.y_var_lags_reduced[columns].copy()\n",
    "print(df_sub.columns)\n",
    "sub_vif = drop_high_vif_vars.get_vif_df(df_sub)\n",
    "sub_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp on subsets to reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.reduce_y_var_lags(filter_vif_by_feature=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_key_words, all_column_subsets = dec.get_subset_key_words_and_all_column_subsets_for_vif(\n",
    "            dec.y_var_lags_reduced)\n",
    "subset_key_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows, na_cols = general_utils.find_rows_with_na(dec.y_var)\n",
    "duplicate_rows = general_utils.find_duplicate_rows(dec.y_var, column_subset=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test speed of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run profiler and save output to file\n",
    "cProfile.run('dec.streamline_making_behav_and_neural_data()', 'profile_output')\n",
    "\n",
    "# Load stats and sort by total time\n",
    "p = pstats.Stats('profile_output')\n",
    "p.strip_dirs().sort_stats('tottime').print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.strip_dirs().sort_stats('tottime').print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See sizes of biggest variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "\n",
    "sizes = []\n",
    "for attr in dir(dec):\n",
    "    if attr.startswith('__') and attr.endswith('__'):\n",
    "        continue  # skip dunder attributes\n",
    "    try:\n",
    "        val = getattr(dec, attr)\n",
    "        size = asizeof.asizeof(val)\n",
    "        sizes.append((attr, size))\n",
    "    except Exception:\n",
    "        pass  # ignore any errors\n",
    "\n",
    "# Sort and display largest attributes in MB\n",
    "for name, size in sorted(sizes, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import warnings\n",
    "from pympler import asizeof\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "filtered = {\n",
    "    k: v for k, v in globals().items()\n",
    "    if not isinstance(v, types.ModuleType)\n",
    "}\n",
    "\n",
    "sizes = []\n",
    "for name, val in filtered.items():\n",
    "    try:\n",
    "        sizes.append((name, asizeof.asizeof(val)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for name, size in sorted(sizes, key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"{name}: {size / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more columns (possibly get in the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get also get: (but to be honest, it doesn't make that much sense to get them....so let's skip for now.)\n",
    "'distance traversed since target last visible',\n",
    "'d angle since target last visible', 'target_at_right',\n",
    "'time_till_capture', 'time from last visible to capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there might be multicollinearity. For example, duration from last visible to capture = time since target last visible + time till capture\n",
    "\n",
    "Similarly, target angle = target angle last seen frozen - d angle since target last visible\n",
    "\n",
    "(For distance it's not exactly the same because of the difference between distance and distance traversed, but it's still similar)\n",
    "\n",
    "The multicollinearity is fine in linear regression (when each feature here is a y var), but need to be dealt with in cca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should i actually align each section, as if they are trials???\n",
    "maybe i can try both that and continuous time... both can shed light on different behavioral variables\n",
    "but for aligning trials, it may require alignment or warping since trial durations vary.\n",
    "\n",
    "btw, what does it mean stitch data?\n",
    "\n",
    "also, what does it look like to use RNN to model it?\n",
    "I thought about the paper that Noah presented on\n",
    "\n",
    "\n",
    "btw.......IME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why ratio of bin/target_index approaches constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_lengths = dec.pursuit_data[['target_index', 'bin']].groupby('target_index').count()\n",
    "trial_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = dec.y_var_reduced[['time', 'bin', 'target_index']]\n",
    "sub['factor'] = dec.y_var_reduced['bin']/dec.y_var_reduced['target_index']\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.diff(dec.ff_caught_T_sorted), bins=30)\n",
    "plt.xlabel('Time difference')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of time differences between caught events')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.ff_caught_T_sorted/np.arange(len(dec.ff_caught_T_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compared with neural_data_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = neural_vs_behavioral_class.NeuralVsBehavioralClass(raw_data_folder_path=raw_data_folder_path)\n",
    "dec.streamline_preparing_neural_and_behavioral_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.final_behavioral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in dec.y_var.columns if col not in dec.y_var_reduced.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in dec.y_var_reduced.columns if col not in dec.y_var.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check gpfa's binned spikes vs my own binned spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_segs_df = fit_gpfa_utils.make_spike_segs_df(dec.spike_df, dec.single_vis_target_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get binned spikes (seqs) from gpfa_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_index = 9\n",
    "cluster_index = 12\n",
    "seg = dec.spiketrain_corr_segs[seg_index]\n",
    "cluster = dec.spike_segs_df.cluster.unique()[cluster_index]\n",
    "\n",
    "spiketrain = dec.spiketrains[seg_index][cluster_index]\n",
    "seqs = gpfa_util.get_seqs([spiketrain], dec.bin_width_w_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take out my own binned spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sub = dec.pursuit_data_all[dec.pursuit_data_all['segment']==seg]\n",
    "binned_spikes_sub = dec.binned_spikes_df[dec.binned_spikes_df['bin'].isin(p_sub['bin'])].copy()\n",
    "binned_spikes_sub['bin'] = binned_spikes_sub.index\n",
    "binned_spikes_sub2 = binned_spikes_sub.merge(p_sub[['bin', 'time']], on='bin', how='left')\n",
    "binned_spikes_sub3 = binned_spikes_sub2[['bin', 'time', f'unit_{cluster}']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = len(binned_spikes_sub3)\n",
    "if dec.align_at_beginning:\n",
    "    binned_spikes_sub3['gpfa'] = seqs[0][1][0][trial_length:]\n",
    "else:\n",
    "    binned_spikes_sub3['gpfa'] = seqs[0][1][0][-trial_length:] # when getting latent dimension for neural data, [-trial_length:] was also used\n",
    "binned_spikes_sub3['same'] = binned_spikes_sub3[f'unit_{cluster}'] == binned_spikes_sub3['gpfa']\n",
    "binned_spikes_sub3[binned_spikes_sub3['same']!=True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find out why there are rows of NA in dec.y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows, na_cols = general_utils.find_rows_with_na(dec.y_var)\n",
    "duplicate_rows = general_utils.find_duplicate_rows(dec.y_var, column_subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.behav_data_all.loc[118189:118195, ['bin', 'time', 'target_rel_x', 'target_rel_y','time_since_target_last_seen', 'target_last_seen_distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare old and new target df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df_ori = pd.read_csv('/Users/dusiyi/Documents/Multifirefly-Project/all_monkey_data/patterns_and_features/monkey_Schro/data_0416/target_df_ori.csv')\n",
    "df = target_df_ori[['target_index', 'point_index', 'time']].copy()\n",
    "for col in ['target_distance', 'time_since_target_last_seen']:\n",
    "    df[f'old_{col}'] = target_df_ori[col]   \n",
    "    df[f'new_{col}'] = dec.target_df[col]  \n",
    "\n",
    "df['old_target_last_seen_distance'] = target_df_ori['target_last_seen_distance_frozen']\n",
    "df['new_target_last_seen_distance'] = dec.target_df['target_last_seen_distance']\n",
    "\n",
    "df2 = df.loc[10068:]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['point_index']>= 139910]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "139913"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
