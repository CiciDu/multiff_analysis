{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install elephant neo quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_folder = '/Users/dusiyi/Documents/Multifirefly-Project'\n",
    "os.chdir(project_folder)\n",
    "sys.path.append(os.path.join(project_folder, 'multiff_analysis', 'methods'))\n",
    "\n",
    "from data_wrangling import specific_utils, process_monkey_information\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_trials, cluster_analysis, organize_patterns_and_features\n",
    "from visualization.matplotlib_tools import plot_behaviors_utils\n",
    "from non_behavioral_analysis.neural_data_analysis.get_neural_data import neural_data_processing\n",
    "from non_behavioral_analysis.neural_data_analysis.visualize_neural_data import plot_neural_data, plot_modeling_result\n",
    "from non_behavioral_analysis.neural_data_analysis.model_neural_data import cca_class, pgam_class, neural_data_modeling, drop_high_corr_vars, drop_high_vif_vars\n",
    "from non_behavioral_analysis.neural_data_analysis.neural_vs_behavioral import prep_monkey_data, prep_target_data, neural_vs_behavioral_class\n",
    "from non_behavioral_analysis.neural_data_analysis.planning_neural import planning_neural_class, planning_neural_utils\n",
    "from non_behavioral_analysis.neural_data_analysis.decode_targets import behav_features_to_keep, decode_target_class, plot_gpfa_utils, decode_target_utils\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from scipy import linalg, interpolate\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy.io import loadmat\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from numpy import pi\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.multivariate.cancorr import CanCorr\n",
    "\n",
    "# Neuroscience specific imports\n",
    "import neo\n",
    "import rcca\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"done\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/monkey_Schro/data_0416\"\n",
    "dec = decode_target_class.DecodeTargetClass(raw_data_folder_path=raw_data_folder_path,\n",
    "                                                               bin_width=0.1, window_width=0.25)\n",
    "dec.streamline_making_behav_and_neural_data()\n",
    "dec.get_x_and_y_var()\n",
    "\n",
    "dec.pursuit_data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce columns in lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.reduce_x_var_lags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.reduce_y_var_lags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check result of reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## also check correlations between x vars without lags\n",
    "high_corr_pair_df, top_n_corr_df = drop_high_corr_vars.get_pairs_of_columns_w_high_corr(\n",
    "            dec.x_var, corr_threshold=0.8)\n",
    "top_n_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression (didn't modify yet)\n",
    "\n",
    "Regressing the behavioral variables individually (as y_var) against all neural activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put results in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_or_retrieve_y_var_lr_result_df(exists_ok=True)\n",
    "data_item.y_var_lr_result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def make_or_retrieve_y_var_lr_result_df(self, exists_ok=True):\n",
    "        df_path = os.path.join(self.lr_result_df_path,\n",
    "                               'y_var_lr_result_df.csv')\n",
    "        if exists_ok & exists(df_path):\n",
    "            self.y_var_lr_result_df = pd.read_csv(df_path)\n",
    "        else:\n",
    "            self.y_var_lr_result_df = neural_data_modeling.get_y_var_lr_result_df(\n",
    "                self.x_var, self.y_var)\n",
    "            self.y_var_lr_result_df.to_csv(df_path, index=False)\n",
    "            print('Made new y_var_lr_result_df')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  plot all neural clusters vs one behavioral var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in dec.y_var_lags_reduced.columns if 'bin' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_lags_reduced['bin'] = dec.y_var_lags_reduced['bin_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_lags_reduced['bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_lags_reduced['bin'] = dec.y_var_lags_reduced['bin_5'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct linear regression on X and y\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "bins_to_plot = range(dec.y_var_lags_reduced.bin.max())\n",
    "dec.y_var_lags_reduced['bin'] = dec.y_var_lags_reduced['bin_5'].astype(int)\n",
    "for i, column in enumerate(dec.y_var_lags_reduced.columns):\n",
    "\n",
    "    \n",
    "    plot_neural_data.plot_regression(dec.y_var_lags_reduced, column, dec.x_var, bins_to_plot=None, min_r_squared_to_plot=0.3)\n",
    "    # if i == 3:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  plot one neural cluster vs one behavioral var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one neural cluster against one behavioral variable\n",
    "cluster_num, behavioral_column = 6, 'monkey_speed'\n",
    "bins_to_plot = range(1000, 1200)\n",
    "x_values, y_values = data_item.binned_spikes_matrix[bins_to_plot, cluster_num], data_item.final_behavioral_data[behavioral_column][bins_to_plot]\n",
    "reg = LinearRegression().fit(x_values.reshape(-1, 1), y_values)\n",
    "\n",
    "plt.scatter(x_values, y_values, color='blue', s=1)\n",
    "plt.plot(x_values, reg.predict(x_values.reshape(-1, 1)), color='red', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## elephant example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_segments = []\n",
    "\n",
    "for index, row in dec.single_vis_target_df.iterrows():\n",
    "    mask = dec.spike_df.time.between(row['last_vis_time'], row['ff_caught_time'])\n",
    "    spikes_sub = dec.spike_df[mask].copy()\n",
    "    spikes_sub['segment'] = index\n",
    "    spikes_sub['segment_start_time'] = row['last_vis_time']\n",
    "    spikes_sub['segment_stop_time'] = row['ff_caught_time']\n",
    "    spike_segments.append(spikes_sub)\n",
    "\n",
    "spike_segs_df = pd.concat(spike_segments, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_segs_df['t_duration'] = spike_segs_df['segment_stop_time'] - spike_segs_df['segment_start_time']\n",
    "max(spike_segs_df['t_duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 20 * pq.ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique clusters and segments\n",
    "clusters = spike_segs_df.cluster.unique()\n",
    "segments = spike_segs_df.segment.unique()\n",
    "\n",
    "# Create spiketrain objects (in Neo)\n",
    "spiketrains = []\n",
    "common_t_stop = max(spike_segs_df['t_duration'])\n",
    "\n",
    "# Process each segment and cluster combination\n",
    "for seg in segments:\n",
    "    # Get data for this segment\n",
    "    spike_df_trial = spike_segs_df[spike_segs_df.segment == seg]\n",
    "    \n",
    "    # Get segment start and stop times (should be the same for all rows in this segment)\n",
    "    seg_start_time = spike_df_trial.segment_start_time.iloc[0]\n",
    "\n",
    "    seg_spiketrain = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        # Get spikes for this cluster in this segment\n",
    "        sub = spike_df_trial[spike_df_trial.cluster == cluster]\n",
    "        \n",
    "        # Calculate relative spike times\n",
    "        spike_time = sub.time - seg_start_time\n",
    "\n",
    "        # Create SpikeTrain object\n",
    "        spiketrain = neo.SpikeTrain(\n",
    "            times=spike_time.values * pq.s,  # Convert to quantities\n",
    "            t_start=0 * pq.s,\n",
    "            t_stop=common_t_stop\n",
    "        )\n",
    "        seg_spiketrain.append(spiketrain)\n",
    "\n",
    "    spiketrains.append(seg_spiketrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(spiketrains))\n",
    "print(len(spiketrains[0]))\n",
    "print(len(spiketrains[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from elephant.gpfa import GPFA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import quantities as pq\n",
    "import neo\n",
    "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dimensionality = 3\n",
    "gpfa_3dim = GPFA(bin_size=bin_size, x_dim=latent_dimensionality)\n",
    "trajectories = gpfa_3dim.fit_transform(spiketrains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpfa_utils.plot_gpfa_traj_3d_uniform_color(trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, enable interactive mode in your notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create the interactive plot\n",
    "fig, ax = plot_gpfa_utils.plot_gpfa_traj_3d(\n",
    "    trajectories=trajectories,\n",
    "    figsize=(15, 5),\n",
    "    linewidth_single_trial=0.75,\n",
    "    alpha_single_trial=0.3,\n",
    "    linewidth_trial_average=2,\n",
    "    title='Latent dynamics extracted by GPFA',\n",
    "    view_azim=-5,\n",
    "    view_elev=60\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_gpfa_utils.plot_gpfa_traj_3d_plotly(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import var\n",
    "\n",
    "traj_stack = np.stack(trajectories, axis=0)  # shape: (n_trials, 3, T)\n",
    "var_by_dim = var(traj_stack, axis=(0, 2))    # variance across trials and time\n",
    "var_by_dim /= var_by_dim.sum()               # normalize to get explained variance ratio\n",
    "print(\"Variance explained by each latent dimension:\", var_by_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "ax.set_title('Latent dynamics extracted by GPFA')\n",
    "ax.set_xlabel('Time [s]')\n",
    "\n",
    "time = np.arange(len(average_trajectory[0])) * 0.02  # assuming all trajectories have the same length\n",
    "\n",
    "for i, x in enumerate(average_trajectory):\n",
    "    ax.plot(time, x, label=f'Dim {i+1}')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.behav_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def time_resolved_regression(latent_trajectories, behavioral_data, time_step=0.02, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Time-resolved regression predicting multiple continuous behavioral variables.\n",
    "\n",
    "    Parameters:\n",
    "    - latent_trajectories: np.array (n_trials, n_timepoints, n_latent_dims)\n",
    "    - behavioral_data: np.array (n_trials, n_timepoints, n_behaviors), continuous behavioral variables\n",
    "    - time_step: float, time bin size in seconds\n",
    "    - cv_folds: int, cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    - scores_by_time: np.array (n_timepoints, n_behaviors), cross-validated R² scores\n",
    "    - times: np.array (n_timepoints,), time points vector\n",
    "    \"\"\"\n",
    "\n",
    "    n_trials, n_timepoints, n_latent_dims = latent_trajectories.shape\n",
    "    _, _, n_behaviors = behavioral_data.shape\n",
    "\n",
    "    scores_by_time = np.zeros((n_timepoints, n_behaviors))\n",
    "\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for t in range(n_timepoints):\n",
    "        X = latent_trajectories[:, t, :]  # (n_trials, n_latent_dims)\n",
    "        Y = behavioral_data[:, t, :]       # (n_trials, n_behaviors)\n",
    "\n",
    "        for b in range(n_behaviors):\n",
    "            y = Y[:, b]\n",
    "\n",
    "            # Ridge regression with built-in CV for alpha, wrapped in cross_val_score for out-of-sample R²\n",
    "            model = RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "            cv_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "            scores_by_time[t, b] = cv_scores.mean()\n",
    "\n",
    "    times = np.arange(n_timepoints) * time_step\n",
    "    return scores_by_time, times\n",
    "\n",
    "# Example dummy data usage:\n",
    "np.random.seed(0)\n",
    "n_trials = 100\n",
    "n_timepoints = 50\n",
    "n_latent_dims = 5\n",
    "n_behaviors = 2\n",
    "\n",
    "latent_trajectories = np.random.randn(n_trials, n_timepoints, n_latent_dims)\n",
    "behavioral_data = np.random.randn(n_trials, n_timepoints, n_behaviors)  # continuous behaviors\n",
    "\n",
    "scores, times = time_resolved_regression(latent_trajectories, behavioral_data)\n",
    "\n",
    "# Plot results for each behavior:\n",
    "plt.figure(figsize=(10, 6))\n",
    "for b in range(n_behaviors):\n",
    "    plt.plot(times, scores[:, b], label=f'Behavior {b+1}')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Cross-validated R²')\n",
    "plt.title('Time-resolved regression: neural latent -> continuous behavior')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA\n",
    "\n",
    "https://medium.com/@pozdrawiamzuzanna/canonical-correlation-analysis-simple-explanation-and-python-example-a5b8e97648d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No lagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_no_lag = cca_class.CCAclass(X1=dec.x_var, X2=dec.y_var_reduced, lagging_included=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_no_lag.conduct_cca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_lags = cca_class.CCAclass(X1=dec.x_var_lags, X2=dec.y_var_lags_reduced, lagging_included=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_lags_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_lags.conduct_cca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare lag vs no lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "canon_df = pd.DataFrame(cca_no_lag.canon_corr, columns = ['no_lag'])\n",
    "canon_df[f'lag_{dec.max_lag_number}'] = cca_lags.canon_corr\n",
    "canon_df['component'] = [f'CC {i+1}' for i in range(cca_lags.n_components)]\n",
    "# convert canon_df to long format\n",
    "canon_df_long = pd.melt(canon_df, id_vars=['component'], var_name='lag', value_name='canon_coeff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sns bar plot on canon_df_long\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='component', y='canon_coeff', data=canon_df_long, hue='lag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cca_inst (choose one between lags and no lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose lags\n",
    "cca_inst = cca_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose no lag\n",
    "cca_inst = cca_no_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X1', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X2', squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squared loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_loadings(X1_or_X2='X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abs weights ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights(X1_or_X2='X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot real weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights(abs_value=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.plot_ranked_weights(X1_or_X2='X2', abs_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca_inst.X2_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_sc_df = pd.DataFrame(cca_inst.X2_sc, columns = cca_inst.X2.columns)\n",
    "X2_sc_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X2_sc_df.columns:\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    sns.boxplot(X2_sc_df[column], orient='h')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap of weights\n",
    "raw canonical coefficients are interpreted in a manner analogous to interpreting regression coefficients. For example: a one unit increase in reading leads to a .0446 decrease in the first canonical variate of set 2 when all of the other variables are held constant (in some other data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = cca_inst.X2_weight_df.copy()\n",
    "weight_df = weight_df.set_index('feature').drop(columns='feature_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(15, 25))\n",
    "sns.heatmap(weight_df.iloc[:20, :10], cmap='coolwarm', annot=True, linewidths=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1, train2, test2 = train_test_split(cca_inst.X1_sc, cca_inst.X2_sc, test_size=0.3, random_state=42)\n",
    "# use training and testing set\n",
    "nComponents = 10\n",
    "cca2 = rcca.CCA(kernelcca = False, reg = 0., numCC = nComponents)\n",
    "cca2.train([train1, train2])\n",
    "testcorrs = cca2.validate([test1, test2])\n",
    "testcorrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca2.compute_ev([test1, test2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_cca = CanCorr(cca_inst.X1_sc, cca_inst.X2_sc)\n",
    "print(stats_cca.corr_test().summary())\n",
    "neural_data_modeling.print_weights('X', stats_cca.x_cancoef)\n",
    "neural_data_modeling.print_weights('Z', stats_cca.y_cancoef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGAM (unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorize variables\n",
    "dec.y_var_reduced.columns\n",
    "temporal_vars = ['time_rel_to_stop',\n",
    " 'time_when_nxt_ff_first_seen_rel_to_stop',\n",
    " 'time_when_cur_ff_first_seen_rel_to_stop',\n",
    " 'time_when_nxt_ff_last_seen_rel_to_stop',\n",
    " 'time_when_cur_ff_last_seen_rel_to_stop',\n",
    " ]\n",
    "\n",
    "spatial_vars = [x for x in dec.y_var_reduced.columns if x not in temporal_vars]\n",
    "spatial_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparsity of neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.binned_spikes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect neural data\n",
    "\n",
    "bins = dec.binned_spikes_df\n",
    "\n",
    "# Calculate percentage of non-zero rows for each column\n",
    "non_zero_percentages = (bins != 0).mean() * 100\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "non_zero_df = pd.DataFrame({\n",
    "    'Column': non_zero_percentages.index,\n",
    "    'Percent_Non_Zero': non_zero_percentages.values\n",
    "})\n",
    "\n",
    "# Sort by percentage in descending order\n",
    "non_zero_df = non_zero_df.sort_values('Percent_Non_Zero', ascending=False)\n",
    "\n",
    "print(\"Percentage of non-zero values in each column:\")\n",
    "print(non_zero_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.drop(columns='bin').mean(axis=1).describe()\n",
    "\n",
    "# plot the percentile of values of mean firing rates across neurons at each time bin\n",
    "mean_rates = bins.drop(columns='bin').mean(axis=1)\n",
    "\n",
    "# Calculate percentiles from 0 to 100\n",
    "percentiles = np.arange(0, 101, 1)\n",
    "percentile_values = np.percentile(mean_rates, percentiles)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(percentiles, percentile_values)\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Mean Firing Rate')\n",
    "plt.title('Distribution of Mean Firing Rates Across Neurons')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y var (behavioral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var_vif = drop_high_vif_vars.get_vif_df(dec.y_var)\n",
    "print(y_var_vif.head(8))\n",
    "\n",
    "# calculate the correlation coefficient among the columns with VIF > 5\n",
    "# specific_columns = vif_df[vif_df['vif'] > 5].feature.values\n",
    "specific_columns = y_var_vif.feature.values[:10]\n",
    "corr_coeff = dec.y_var[specific_columns].corr()\n",
    "#plt.figure(figsize = (6, 6))\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try y_var_reduced\n",
    "\n",
    "y_var_vif = drop_high_vif_vars.get_vif_df(dec.y_var_reduced)\n",
    "print(y_var_vif.head(8))\n",
    "\n",
    "# calculate the correlation coefficient among the columns with VIF > 5\n",
    "# specific_columns = vif_df[vif_df['vif'] > 5].feature.values\n",
    "specific_columns = y_var_vif.feature.values[:10]\n",
    "corr_coeff = dec.y_var[specific_columns].corr()\n",
    "#plt.figure(figsize = (6, 6))\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(corr_coeff, cmap='coolwarm', annot=True, linewidths=1, vmin=-1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trial segments in pursuit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.matplotlib_tools import plot_trials,\n",
    "dec.make_PlotTrials_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]                     \n",
    "\n",
    "max_plot_to_make = 2\n",
    "plot_counter = 0\n",
    "\n",
    "for index, row in dec.single_vis_target_df.iloc[2:].iterrows():\n",
    "\n",
    "    duration = [row['last_vis_time'], row['ff_caught_time']]\n",
    "\n",
    "    returned_info = plot_trials.PlotTrials(\n",
    "                duration, \n",
    "                *dec.PlotTrials_args,  \n",
    "                adjust_xy_limits=True,       \n",
    "                minimal_margin=50,\n",
    "                show_reward_boundary=True,\n",
    "                show_alive_fireflies=False,\n",
    "                show_visible_fireflies=True,\n",
    "                show_in_memory_fireflies=True,\n",
    "                show_believed_target_positions=True,\n",
    "                )\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plot_counter += 1\n",
    "    if plot_counter >= max_plot_to_make:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check target_rel_x and y\n",
    "(The look correct after checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pursuit_sub = dec.pursuit_data.loc[dec.pursuit_data['target_index']==65].copy()\n",
    "pursuit_sub['target_angle_deg'] = pursuit_sub['target_angle'] * 180/pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pursuit_sub[['point_index', 'target_angle_deg', 'target_distance', 'target_rel_x', 'target_rel_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more columns (possibly get in the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get also get: (but to be honest, it doesn't make that much sense to get them....so let's skip for now.)\n",
    "'distance traversed since target last visible',\n",
    "'d angle since target last visible', 'target_at_right',\n",
    "'time_till_capture', 'time from last visible to capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there might be multicollinearity. For example, duration from last visible to capture = time since target last visible + time till capture\n",
    "\n",
    "Similarly, target angle = target angle last seen frozen - d angle since target last visible\n",
    "\n",
    "(For distance it's not exactly the same because of the difference between distance and distance traversed, but it's still similar)\n",
    "\n",
    "The multicollinearity is fine in linear regression (when each feature here is a y var), but need to be dealt with in cca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should i actually align each section, as if they are trials???\n",
    "maybe i can try both that and continuous time... both can shed light on different behavioral variables\n",
    "but for aligning trials, it may require alignment or warping since trial durations vary.\n",
    "\n",
    "btw, what does it mean stitch data?\n",
    "\n",
    "also, what does it look like to use RNN to model it?\n",
    "I thought about the paper that Noah presented on\n",
    "\n",
    "\n",
    "btw.......IME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why ratio of bin/target_index approaches constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_lengths = dec.pursuit_data[['target_index', 'bin']].groupby('target_index').count()\n",
    "trial_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = dec.y_var_reduced[['time', 'bin', 'target_index']]\n",
    "sub['factor'] = dec.y_var_reduced['bin']/dec.y_var_reduced['target_index']\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.diff(dec.ff_caught_T_sorted), bins=30)\n",
    "plt.xlabel('Time difference')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of time differences between caught events')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.ff_caught_T_sorted/np.arange(len(dec.ff_caught_T_sorted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compared with neural_data_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item = neural_vs_behavioral_class.NeuralVsBehavioralClass(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.streamline_preparing_neural_and_behavioral_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.final_behavioral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.y_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.y_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.y_var_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in data_item.y_var.columns if col not in dec.y_var_reduced.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in dec.y_var_reduced.columns if col not in data_item.y_var.columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
