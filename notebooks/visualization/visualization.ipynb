{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive # import drive from google colab\n",
    "drive.mount(\"/content/drive\") \n",
    "!pip install neo\n",
    "!pip install matplotlib_scalebar\n",
    "!pip install ffmpeg\n",
    "!pip install Ipython --upgrade\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using google drive\n",
    "%cd /content/drive/MyDrive/ff_repo/Multifirefly-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "if Path.cwd().parts[-1] != 'Multifirefly-Project':\n",
    "    os.chdir('..')\n",
    "    from add_path import find_path\n",
    "    current_path = find_path()\n",
    "    os.chdir(current_path)\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "from data_wrangling import basic_func, process_raw_data, base_processing_class, monkey_data_classes\n",
    "from pattern_discovery import pattern_by_trials, pattern_by_points, make_ff_dataframe, ff_dataframe_utils, pattern_by_trials, pattern_by_points, cluster_analysis, organize_patterns_and_features, category_class\n",
    "from decision_making_analysis.cluster_replacement import cluster_replacement_class, cluster_replacement_utils, plot_cluster_replacement\n",
    "from decision_making_analysis.decision_making import decision_making_utils, plot_decision_making, intended_targets_classes\n",
    "from decision_making_analysis.GUAT import GUAT_helper_class, GUAT_collect_info_class, GUAT_combine_info_class, process_GUAT_trials_class, GUAT_and_TAFT\n",
    "from decision_making_analysis import free_selection, replacement, trajectory_info\n",
    "from visualization import animation_func, animation_utils, plot_trials, plot_polar, additional_plots, plot_behaviors_utils, plot_statistics\n",
    "from machine_learning import machine_learning_utils, hyperparameter_tuning_class\n",
    "from machine_learning.RL.env_related import env_for_lstm, env_utils, base_env, collect_agent_data_utils\n",
    "from machine_learning.RL.lstm import GRU_functions, LSTM_functions\n",
    "from machine_learning.RL.SB3 import interpret_neural_network, sb3_for_multiff_class, rl_for_multiff_utils, SB3_functions\n",
    "from null_behaviors import show_null_trajectory, find_best_arc, curvature_utils, curvature_class\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from scipy import stats\n",
    "from IPython.display import HTML\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, multilabel_confusion_matrix, fbeta_score, precision_score, recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from math import pi\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.max_rows = 101\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve monkey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(base_processing_class)\n",
    "reload(process_raw_data)\n",
    "reload(decision_making_class)\n",
    "reload(pattern_by_trials)\n",
    "reload(plot_trials)\n",
    "reload(intended_targets_classes)\n",
    "reload(animation_func)\n",
    "reload(monkey_data_classes)\n",
    "\n",
    "\n",
    "\n",
    "PLAYER = \"monkey\"\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/individual_monkey_data/monkey_Bruno/data_0330\"\n",
    "data_item = monkey_data_classes.ProcessMonkeyData(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.retrieve_or_make_monkey_data(exists_ok=True, min_distance_to_calculate_angle=10)\n",
    "data_item.make_or_retrieve_ff_dataframe(num_missed_index=0, exists_ok=True)\n",
    "data_item.find_patterns()\n",
    "\n",
    "monkey_information = data_item.monkey_information\n",
    "ff_dataframe = data_item.ff_dataframe\n",
    "\n",
    "ff_life_sorted = data_item.ff_life_sorted\n",
    "ff_real_position_sorted = data_item.ff_real_position_sorted\n",
    "ff_believed_position_sorted = data_item.ff_believed_position_sorted\n",
    "cluster_around_target_indices = data_item.cluster_around_target_indices\n",
    "ff_caught_T_new = data_item.ff_caught_T_new\n",
    "caught_ff_num = len(ff_caught_T_new)\n",
    "ff_flash_sorted = data_item.ff_flash_sorted\n",
    "ff_flash_end_sorted = data_item.ff_flash_end_sorted\n",
    "max_point_index = data_item.max_point_index\n",
    "min_point_index = data_item.min_point_index\n",
    "\n",
    "monkey_name = data_item.model_folder_name.split('/')[2]\n",
    "data_name = data_item.model_folder_name.split('/')[3]\n",
    "\n",
    "\n",
    "\n",
    "filepath = os.path.join(data_item.data_folder_name, 'target_cluster_df.csv')\n",
    "if exists(filepath):\n",
    "    target_cluster_df = pd.read_csv(filepath).drop([\"Unnamed: 0\"], axis=1)\n",
    "else:\n",
    "    target_cluster_df = cluster_analysis.find_target_cluster_df(monkey_information, ff_real_position_sorted, ff_caught_T_new, ff_life_sorted, ff_dataframe, max_distance=50)\n",
    "    target_cluster_df.to_csv(filepath)\n",
    "target_cluster_df['ff_index'] = target_cluster_df['target_index']\n",
    "\n",
    "# data_item.make_or_retrieve_all_trial_patterns(exists_ok=True)\n",
    "# data_item.make_or_retrieve_pattern_frequencies(exists_ok=True)\n",
    "# data_item.make_or_retrieve_all_trial_features(exists_ok=True)\n",
    "# data_item.make_or_retrieve_feature_statistics(exists_ok=True)\n",
    "# data_item.make_info_of_monkey()\n",
    "# data_item.make_or_retrieve_target_closest()\n",
    "# data_item.make_or_retrieve_target_angle_smallest()\n",
    "# data_item.make_distance_dataframe()\n",
    "# data_item.make_angle_dataframe()\n",
    "\n",
    "# target_closest = data_item.target_closest\n",
    "# target_angle_smallest = data_item.target_angle_smallest\n",
    "# all_trial_patterns = data_item.all_trial_patterns\n",
    "# all_trial_features = data_item.all_trial_features\n",
    "# pattern_frequencies = data_item.pattern_frequencies\n",
    "# feature_statistics = data_item.feature_statistics\n",
    "# distance_dataframe = data_item.distance_dataframe\n",
    "# angle_dataframe = data_item.angle_dataframe\n",
    "# trial_vs_distance = data_item.trial_vs_distance\n",
    "# trial_vs_angle = data_item.trial_vs_angle\n",
    "# info_of_monkey = data_item.info_of_monkey\n",
    "# max_point_index = data_item.max_point_index\n",
    "# min_point_index = data_item.min_point_index\n",
    "\n",
    "# data_item.make_PlotTrials_args()\n",
    "# data_item.make_PlotTrials_kargs(classic_plot_kwargs, combined_plot_kwargs, animation_plot_kwargs)\n",
    "\n",
    "\n",
    "PlotTrials_args = (monkey_information, ff_dataframe, ff_life_sorted, ff_real_position_sorted, ff_believed_position_sorted, cluster_around_target_indices, ff_caught_T_new)\n",
    "\n",
    "\n",
    "plot_polar_args = (monkey_information,\n",
    "                    ff_dataframe, \n",
    "                    ff_life_sorted,\n",
    "                    ff_real_position_sorted,\n",
    "                    ff_caught_T_new,\n",
    "                    ff_flash_sorted,)\n",
    "\n",
    "\n",
    "trial_total_num = 2\n",
    "PLAYER = \"monkey\"\n",
    "\n",
    "classic_plot_kwargs = {'player': PLAYER,\n",
    "                       'show_stops': True,\n",
    "                       'show_believed_target_positions': True,\n",
    "                       'show_reward_boundary': True,\n",
    "                       'show_scale_bar': True,\n",
    "                       'show_eye_positions': True,\n",
    "                       'show_eye_positions_on_the_right': True,\n",
    "                       'show_connect_path_eye_positions': True,\n",
    "                       #=== below is different from animation_plot_kwargs\n",
    "                       'hitting_arena_edge_ok': False,\n",
    "                       'trial_too_short_ok': False}\n",
    "\n",
    "combined_plot_kwargs = {'player': PLAYER,\n",
    "                        'combined_plot': True,\n",
    "                        'show_alive_fireflies': False,\n",
    "                        'show_title': False,\n",
    "                        'show_start': False}\n",
    "\n",
    "plot_chunks_kwargs = {'player': PLAYER,\n",
    "                      'show_stops': True,\n",
    "                      'show_believed_target_positions': True,\n",
    "                      'show_reward_boundary': True,\n",
    "                      'show_scale_bar': True,\n",
    "                      'hitting_arena_edge_ok': True,\n",
    "                      'trial_too_short_ok': True,\n",
    "                      'show_connect_path_ff': True,\n",
    "                      'trail_color_var': 'abs_ddw',\n",
    "                      'show_colorbar': True}\n",
    "\n",
    "\n",
    "plot_polar_kwargs = {'rmax': 400,\n",
    "                    'show_visible_ff': True,\n",
    "                    'hitting_arena_edge_ok': True,\n",
    "                    'return_axes': True,\n",
    "                    'colors_show_overall_time': True,\n",
    "                    'show_ff_in_memory': True,\n",
    "                    'show_target_at_being_caught': True\n",
    "}\n",
    "\n",
    "print(\"player is\", PLAYER)\n",
    "\n",
    "animation_plot_kwargs = classic_plot_kwargs.copy()\n",
    "animation_plot_kwargs['as_part_of_animation'] = True\n",
    "animation_plot_kwargs['show_eye_positions_on_the_right'] = False   \n",
    "animation_plot_kwargs['hitting_arena_edge_ok'] = True\n",
    "animation_plot_kwargs['trial_too_short_ok'] = True\n",
    "animation_plot_kwargs['images_dir'] = None\n",
    "\n",
    "  \n",
    "data_item.make_PlotTrials_args()\n",
    "data_item.make_PlotTrials_kargs(classic_plot_kwargs, combined_plot_kwargs, animation_plot_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "plotting_kwargs = {'player': 'monkey',\n",
    " 'show_stops': True,\n",
    " 'show_believed_target_positions': True,\n",
    " 'show_reward_boundary': True,\n",
    " 'show_scale_bar': True,\n",
    " 'hitting_arena_edge_ok': True,\n",
    " 'trial_too_short_ok': True,\n",
    " 'show_connect_path_ff': True,\n",
    " 'vary_color_for_connecting_path_ff': True,\n",
    " 'show_points_when_ff_stop_being_visible': False,\n",
    " #'show_connect_path_ff_memory': True,\n",
    " 'show_alive_fireflies': False,\n",
    " 'show_visible_fireflies': True,\n",
    " 'show_in_memory_fireflies': True,\n",
    " 'connect_path_ff_max_distance': 400,\n",
    " #'show_connect_path_ff_except_targets': True,\n",
    " 'adjust_xy_limits': True,\n",
    " 'show_null_agent_trajectory': True,\n",
    " 'show_only_ff_that_monkey_has_passed_by_closely': True,\n",
    " 'show_null_trajectory_reaching_boundary_ok': False,\n",
    " 'vary_color_for_connecting_path_ff': False}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotTrials_args = (monkey_information, ff_dataframe, ff_life_sorted, ff_real_position_sorted, ff_believed_position_sorted, cluster_around_target_indices, ff_caught_T_new)\n",
    "\n",
    "\n",
    "# plot_polar_args = (monkey_information,\n",
    "#                     ff_dataframe, \n",
    "#                     ff_life_sorted,\n",
    "#                     ff_real_position_sorted,\n",
    "#                     ff_caught_T_new,\n",
    "#                     ff_flash_sorted,)\n",
    "\n",
    "# data_item.make_PlotTrials_args()\n",
    "# data_item.make_PlotTrials_kargs(classic_plot_kwargs, combined_plot_kwargs, animation_plot_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ff_basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ff_basic_info\n",
    "num_missing_elements = len(ff_real_position_sorted) - len(ff_caught_T_new)\n",
    "ff_caught_T_new_extended = np.concatenate([ff_caught_T_new, np.repeat(np.nan, num_missing_elements)])\n",
    "ff_believed_position_sorted_extended = np.concatenate([ff_believed_position_sorted, np.repeat(np.nan, num_missing_elements*2).reshape(-1,2)])\n",
    "\n",
    "ff_basic_info_bundle = (ff_real_position_sorted, ff_believed_position_sorted_extended, ff_life_sorted, ff_caught_T_new_extended, ff_flash_end_sorted)\n",
    "\n",
    "ff_basic_info = pd.DataFrame({'ff_index': range(len(ff_real_position_sorted)),\n",
    "                            'caught_T': ff_caught_T_new_extended,\n",
    "                            'real_x': ff_real_position_sorted[:,0],\n",
    "                            'real_y': ff_real_position_sorted[:,1],\n",
    "                            'believed_x': ff_believed_position_sorted_extended[:,0],\n",
    "                            'believed_y': ff_believed_position_sorted_extended[:,1],\n",
    "                            'life_start': ff_life_sorted[:,0],\n",
    "                            'life_end': ff_life_sorted[:,1],\n",
    "                            'flash_end': ff_flash_end_sorted,\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "ff_basic_info.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dm (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_class)\n",
    "reload(decision_making_utils)\n",
    "reload(intended_targets_classes)\n",
    "reload(basic_func)\n",
    "furnish_with_trajectory_data = False\n",
    "\n",
    "dm = decision_making_class.DecisionMaking(ff_dataframe, ff_caught_T_new, ff_real_position_sorted, monkey_information)\n",
    "dm.retrieve_manual_anno()\n",
    "\n",
    "dm.manual_anno = pseudo_manual_anno\n",
    "\n",
    "dm.separate_manual_anno()\n",
    "dm.eliminate_crossing_boundary_cases(n_seconds_after_crossing_boundary = 2.5)\n",
    "\n",
    "dm.get_replacement_data()\n",
    "dm.get_free_selection_data()\n",
    "\n",
    "# dm.changing_pursued_ff_data = changing_pursued_ff_data\n",
    "# dm.changing_pursued_ff_data_diff = changing_pursued_ff_data_diff\n",
    "# dm.replacement_time = replacement_time\n",
    "# dm.replacement_inputs = replacement_inputs\n",
    "# dm.replacement_labels = replacement_labels\n",
    "\n",
    "# dm.free_selection_inputs_df = free_selection_inputs_df\n",
    "# dm.free_selection_inputs = free_selection_inputs\n",
    "# dm.free_selection_labels = free_selection_labels\n",
    "# dm.free_selection_time = free_selection_time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB()\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# bagging = BaggingClassifier()\n",
    "dm.prepare_data_for_machine_learning(kind=\"replacement\", furnish_with_trajectory_data=furnish_with_trajectory_data, trajectory_data_kind=\"position\") \n",
    "dm.split_data_to_train_and_test(scaling_data=True)\n",
    "dm.use_machine_learning_model(model=None)   \n",
    "dm.get_pred_results_df()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.prepare_data_for_machine_learning(kind=\"free selection\", furnish_with_trajectory_data=furnish_with_trajectory_data, trajectory_data_kind=\"position\") \n",
    "dm.split_data_to_train_and_test(scaling_data=True)\n",
    "dm.use_machine_learning_model(model=None)   \n",
    "dm.get_pred_results_df() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manual_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cases = None\n",
    "dm.plot_prediction_results(dm.wrong_predictions[:10], also_show_regular_plot=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moit (dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pattern_by_trials)\n",
    "reload(decision_making_class)\n",
    "reload(intended_targets_classes)\n",
    "\n",
    "ff_dataframe_truncated = ff_dataframe[ff_dataframe['time_since_last_vis'] <= 2.5]\n",
    "\n",
    "mot = intended_targets_classes.ModelOfIntendedTargets(ff_dataframe, ff_caught_T_new, ff_real_position_sorted, monkey_information, ff_flash_sorted, ff_life_sorted,\n",
    "                                                       time_range_of_trajectory=[-0.5, 2.5], num_time_points_for_trajectory=10)\n",
    "\n",
    "# moit.get_and_process_manual_anno_long(n_seconds_after_crossing_boundary=2.5)\n",
    "\n",
    "moit.manual_anno = pseudo_manual_anno\n",
    "moit.manual_anno_long = pseudo_manual_anno_long\n",
    "moit.eliminate_crossing_boundary_cases(n_seconds_after_crossing_boundary=2.5)\n",
    "\n",
    "moit.get_input_data(num_ff_per_row=5, keeping_1_out_of_n_rows=5)\n",
    "\n",
    "# store df that takes a long time to get\n",
    "free_selection_inputs_df = moit.free_selection_inputs_df\n",
    "free_selection_inputs = moit.free_selection_inputs\n",
    "free_selection_labels = moit.free_selection_labels\n",
    "free_selection_time = moit.free_selection_time\n",
    "cases_for_inspection = moit.cases_for_inspection\n",
    "sequence_of_obs_ff_indices = moit.sequence_of_obs_ff_indices\n",
    "chosen_rows_of_df = moit.chosen_rows_of_df\n",
    "non_chosen_rows_of_df = moit.non_chosen_rows_of_df\n",
    "\n",
    "# moit.free_selection_inputs_df = free_selection_inputs_df\n",
    "# moit.free_selection_inputs = free_selection_inputs\n",
    "# moit.free_selection_labels = free_selection_labels\n",
    "# moit.free_selection_time = free_selection_time\n",
    "# moit.cases_for_inspection = cases_for_inspection\n",
    "# moit.sequence_of_obs_ff_indices = sequence_of_obs_ff_indices\n",
    "# moit.chosen_rows_of_df = chosen_rows_of_df\n",
    "# moit.non_chosen_rows_of_df = non_chosen_rows_of_df\n",
    "\n",
    "moit.prepare_data_for_machine_learning(furnish_with_trajectory_data=False, trajectory_data_kind=\"position\")\n",
    "#moit.turn_y_label_into_multi_class(manual_anno_mul = None, allow_multi_label = False)\n",
    "moit.split_data_to_train_and_test(scaling_data=True, keep_whole_chunks=True)\n",
    "moit.use_machine_learning_model(model=None)\n",
    "moit.get_pred_results_df()\n",
    "moit.plot_prediction_results(selected_cases=moit.wrong_predictions[:3], PlotTrials_args=PlotTrials_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moit.plot_prediction_results(selected_cases=moit.wrong_predictions[:8], PlotTrials_args=PlotTrials_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_utils)\n",
    "moit.plot_prediction_results(selected_cases=moit.wrong_predictions[8:30], PlotTrials_args=PlotTrials_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check cases for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moit.cases_for_inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of time points that are less than 15ms apart\n",
    "t_diff = np.diff(monkey_information['monkey_t'])\n",
    "len(np.where(t_diff < 0.015)[0])/len(t_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range_of_trajectory=[-0.5, 0.5]\n",
    "num_time_points_for_trajectory=10\n",
    "bin_width = (time_range_of_trajectory[1] - time_range_of_trajectory[0]) / (num_time_points_for_trajectory-1)\n",
    "bin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monkey_dt = (monkey_information['monkey_t'].iloc[-1] - monkey_information['monkey_t'].iloc[0]) / (len(monkey_information)-1)\n",
    "monkey_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stops_convolved[stops_convolved > 0]) / len(monkey_stops) # that's the percentage of having stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keep_part_covered_by_manual_anno_mul = True\n",
    "\n",
    "reload(pattern_by_trials)\n",
    "reload(decision_making_class)\n",
    "reload(decision_making_utils)\n",
    "reload(intended_targets_classes)\n",
    "ff_dataframe_truncated = ff_dataframe[ff_dataframe['time_since_last_vis'] <= 2.5]\n",
    "manual_anno_mul = pd.read_csv('multiff_analysis/manual_anno_multi_label.csv')\n",
    "\n",
    "moit2 = intended_targets_classes.ModelOfMultipleIntendedTargets(ff_dataframe, ff_caught_T_new, ff_real_position_sorted, monkey_information, ff_flash_sorted, ff_life_sorted,\n",
    "                                                       time_range_of_trajectory=[-0.5, 2.5], num_time_points_for_trajectory=20)\n",
    "moit2.get_and_process_manual_anno_long(n_seconds_after_crossing_boundary = 2.5)\n",
    "\n",
    "if only_keep_part_covered_by_manual_anno_mul:\n",
    "    moit2.manual_anno_long = moit2.manual_anno_long[moit2.manual_anno_long['starting_point_index'] >= manual_anno_mul.starting_point_index.min()-100]\n",
    "\n",
    "\n",
    "# ## when using the class for the first time\n",
    "moit2.get_input_data(num_ff_per_row=5, keeping_1_out_of_n_rows=1)\n",
    "inputs_df = moit2.inputs_df\n",
    "inputs = moit2.inputs\n",
    "labels = moit2.labels\n",
    "time_all = moit2.time_all\n",
    "cases_for_inspection = moit2.cases_for_inspection\n",
    "sequence_of_obs_ff_indices = moit2.sequence_of_obs_ff_indices\n",
    "chosen_rows_of_df = moit2.chosen_rows_of_df\n",
    "non_chosen_rows_of_df = moit2.non_chosen_rows_of_df\n",
    "\n",
    "\n",
    "# moit2.inputs_df = inputs_df\n",
    "# moit2.inputs = inputs\n",
    "# moit2.labels = labels\n",
    "# moit2.time_all = time_all\n",
    "# moit2.cases_for_inspection = cases_for_inspection\n",
    "# moit2.sequence_of_obs_ff_indices = sequence_of_obs_ff_indices\n",
    "# moit2.chosen_rows_of_df = chosen_rows_of_df\n",
    "# moit2.non_chosen_rows_of_df = non_chosen_rows_of_df\n",
    "\n",
    "\n",
    "moit2.prepare_data_for_machine_learning(furnish_with_trajectory_data=True, trajectory_data_kind=\"position\", allow_multi_label=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which are wrongly classified cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_class)\n",
    "reload(decision_making_utils)\n",
    "moit2.split_data_to_train_and_test(scaling_data=True, test_size=0.4)\n",
    "moit2.use_neural_network(n_epochs = 150, batch_size = 100)\n",
    "#moit2.use_knn()\n",
    "moit2.get_pred_results_df()\n",
    "print(\"number of wrong predictions: \", len(moit2.wrong_predictions))\n",
    "#moit2.plot_prediction_results(selected_cases = moit2.wrong_predictions[:3], max_plot_to_make=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work on animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_utils)\n",
    "anno_ff_indices_dict, pred_ff_indices_dict = decision_making_utils.make_anno_and_pred_ff_indices_dict(moit2)\n",
    "\n",
    "# where does train_test_split happen?\n",
    "# find the starting_point_index of the test chunk\n",
    "test_chunk_starting_point_index = moit2.chosen_rows_of_df.iloc[moit2.indices_test[0]].starting_point_index.astype(int)\n",
    "test_chunk_starting_time = moit2.chosen_rows_of_df.iloc[moit2.indices_test[0]].time\n",
    "test_chunk_ending_point_index = moit2.chosen_rows_of_df.iloc[moit2.indices_test[-1]].starting_point_index.astype(int)\n",
    "test_chunk_ending_time = moit2.chosen_rows_of_df.iloc[moit2.indices_test[-1]].time\n",
    "print('test_chunk_starting_point_index: ', test_chunk_starting_point_index)\n",
    "print('test_chunk_starting_time: ', test_chunk_starting_time)\n",
    "print('test_chunk_ending_point_index: ', test_chunk_ending_point_index)\n",
    "print('test_chunk_ending_time: ', test_chunk_ending_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_plotting_kwargs = {'show_eye_positions': False,\n",
    "                     'show_eye_positions_on_the_right': False,\n",
    "                     'show_connect_path_eye_positions': False,\n",
    "                     'show_ff_indices': True,\n",
    "}\n",
    "\n",
    "temp_animation_plot_kwargs = data_item.animation_plot_kwargs.copy()\n",
    "temp_animation_plot_kwargs['images_dir'] = None\n",
    "\n",
    "for key, value in additional_plotting_kwargs.items():\n",
    "    temp_animation_plot_kwargs[key] = value\n",
    "temp_animation_plot_kwargs['show_connect_path_ff'] = False\n",
    "temp_animation_plot_kwargs['show_connect_path_ff_memory'] = False\n",
    "temp_animation_plot_kwargs['show_stops'] = False\n",
    "\n",
    "temp_animation_plot_kwargs['trail_color_var'] = \"grey\"\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change the default figure size\n",
    "# reload(animation_func)\n",
    "# reload(animation_utils)\n",
    "# reload(show_null_trajectory)\n",
    "# reload(base_processing_class)\n",
    "# reload(monkey_data_classes)\n",
    "# plt.rcParams['animation.ffmpeg_path'] = './ffmpeg'\n",
    "# plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "\n",
    "\n",
    "# #duration = [i, i+5]\n",
    "# duration = [1010, 1020]\n",
    "# print(duration)\n",
    "# video_dir = \"/Users/dusiyi/Documents/Multifirefly-Project/for_manual_annotation_2\"\n",
    "# #os.makedirs(video_dir, exist_ok = True)\n",
    "# file_name = f\"time_{duration[0]}_to_{duration[1]}.mp4\"\n",
    "# k = 3\n",
    "# data_item.make_animation(duration=duration, save_video=True, video_dir=video_dir, file_name=file_name, show_ff_indices=True, \n",
    "#                             static_plot_on_the_left=False, animation_plot_kwargs=temp_animation_plot_kwargs, plot_time_index=True, show_speed_through_path_color=True,\n",
    "#                             max_num_frames=None, max_duration=None, min_duration=1, set_xy_limits=False, anno_ff_indices_dict=anno_ff_indices_dict, pred_ff_indices_dict=None, #pred_ff_indices_dict, \n",
    "#                             anno_but_not_obs_ff_indices_dict = moit2.anno_but_not_obs_ff_indices_dict, k=k, fps=int(62/k/2), \n",
    "#                             plot_show_null_trajectory_for_anno_ff = True, margin=150, show_ff_with_best_aligned_arc=True, monkey_information=monkey_information,\n",
    "#                             rotated=True, show_direction_through_triangle=True) # slow down the video by 2x\n",
    "#                             # let's increase the margin to 150 and see how it goes\n",
    "# HTML(data_item.anim.to_html5_video()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the default figure size\n",
    "reload(animation_func)\n",
    "reload(animation_utils)\n",
    "reload(show_null_trajectory)\n",
    "reload(base_processing_class)\n",
    "reload(monkey_data_classes)\n",
    "plt.rcParams['animation.ffmpeg_path'] = './ffmpeg'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "\n",
    "for i in range(math.ceil(test_chunk_starting_time), math.floor(test_chunk_ending_time), 25):\n",
    "    #duration = [i, i+5]\n",
    "    duration = [i, i+30]\n",
    "    \n",
    "    print(duration)\n",
    "    video_dir = \"/Users/dusiyi/Documents/Multifirefly-Project/for_manual_annotation_2\"\n",
    "    #os.makedirs(video_dir, exist_ok = True)\n",
    "    file_name = f\"time_{duration[0]}_to_{duration[1]}.mp4\"\n",
    "    k = 3\n",
    "    data_item.make_animation(duration=duration, save_video=True, video_dir=video_dir, file_name=file_name, show_ff_indices=True, \n",
    "                                static_plot_on_the_left=False, animation_plot_kwargs=temp_animation_plot_kwargs, plot_time_index=True, show_speed_through_path_color=True,\n",
    "                                max_num_frames=None, max_duration=None, min_duration=1, set_xy_limits=False, anno_ff_indices_dict=anno_ff_indices_dict, pred_ff_indices_dict=None, #pred_ff_indices_dict, \n",
    "                                anno_but_not_obs_ff_indices_dict = moit2.anno_but_not_obs_ff_indices_dict, k=k, fps=int(62/k/2), \n",
    "                                plot_show_null_trajectory_for_anno_ff = True, margin=150, show_ff_with_best_aligned_arc=True, monkey_information=monkey_information,\n",
    "                                rotated=True, show_direction_through_triangle=True) # slow down the video by 2x\n",
    "                                # let's increase the margin to 150 and see how it goes\n",
    "    HTML(data_item.anim.to_html5_video()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_anno_long[manual_anno_long['ff_index']!=manual_anno_long['sub_ff_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Also, i can turn manual_anno_multi into a simimlar df and also trim off the points past the last capture time\n",
    "# but speaking of that, there's another huge problem: should i track the starting point and ending point of each ff in a multi-label case?\n",
    "# for example, if monkey has past one of the ff in multi-ff...shouldn't that ff be taken out?\n",
    "# or like, update the multi-label?\n",
    "# ...ehhh...maybe i can look into this later, after I solve the problem of the single-label case\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch \n",
    "https://boascents2.medium.com/step-by-step-multi-label-image-classification-with-pytorch-gpu-e34d0aa6d578\n",
    "\n",
    "# it uses ResNet 50, and it's pre-trained\n",
    "https://debuggercafe.com/multi-label-image-classification-with-pytorch-and-deep-learning/\n",
    "\n",
    "# on iris dataset\n",
    "https://machinelearningmastery.com/building-a-multiclass-classification-model-in-pytorch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below are modified from the following link:\n",
    "# https://boascents2.medium.com/step-by-step-multi-label-image-classification-with-pytorch-gpu-e34d0aa6d578\n",
    "\n",
    "def F_score_macro(output, label, threshold=0.5, beta=1): # this is the original function from the website\n",
    "    prob = output > threshold\n",
    "    label = label > threshold\n",
    "\n",
    "    TP = (prob & label).sum(1).float()\n",
    "    TN = ((~prob) & (~label)).sum(1).float()\n",
    "    FP = (prob & (~label)).sum(1).float()\n",
    "    FN = ((~prob) & label).sum(1).float()\n",
    "\n",
    "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
    "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
    "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
    "    return F2.mean(0)\n",
    "\n",
    "\n",
    "def F_score(output, label, threshold=0.5, beta=1):\n",
    "    prob = output > threshold\n",
    "    label = label > threshold\n",
    "\n",
    "    TP = (prob & label).sum().float()\n",
    "    TN = ((~prob) & (~label)).sum().float()\n",
    "    FP = (prob & (~label)).sum().float()\n",
    "    FN = ((~prob) & label).sum().float()\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-12)\n",
    "    recall = TP / (TP + FN + 1e-12)\n",
    "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
    "    return F2.mean(0)\n",
    "\n",
    "\n",
    "# The code below are modified from the following link:\n",
    "# https://machinelearningmastery.com/building-a-multiclass-classification-model-in-pytorch/\n",
    "\n",
    "\n",
    "X = moit2.X_all_sc\n",
    "y = y_all_multi\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    " \n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    " \n",
    "class MultilabelModel(nn.Module):\n",
    "    def __init__(self, n_features=4, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(n_features, 200)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(200, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "n_epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "\n",
    "# loss metric and optimizer\n",
    "model = MultilabelModel(n_features=X.shape[1], n_classes=y.shape[1])\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    " \n",
    "# prepare model and training parameters\n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "print(f\"Training on {len(X_train)} samples for {n_epochs} epochs with {batches_per_epoch} batches per epoch.\")\n",
    " \n",
    "best_acc = - np.inf   # init to negative infinity\n",
    "best_weights = None\n",
    "train_loss_hist = []\n",
    "train_acc_hist = []\n",
    "test_loss_hist = []\n",
    "test_acc_hist = []\n",
    " \n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    # set model in training mode and run through each batch\n",
    "    model.train()\n",
    "    for i in range(batches_per_epoch):\n",
    "        # take a batch\n",
    "        start = i * batch_size\n",
    "        X_batch = X_train[start:start+batch_size]\n",
    "        y_batch = y_train[start:start+batch_size]\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        # backward pass\n",
    "        optimizer.zero_grad() # Otherwise, the gradient would be a combination of the old gradient, which you have already used to update your model parameters and the newly-computed gradient.\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # compute and store metrics\n",
    "        ## acc = (torch.argmax(y_pred, 1) == torch.argmax(y_batch, 1)).float().mean()\n",
    "        acc = decision_making_utils.F_score(y_pred, y_batch)\n",
    "        epoch_loss.append(float(loss))\n",
    "        epoch_acc.append(float(acc))\n",
    "\n",
    "    # set model in evaluation mode and run through the test set\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    ce = loss_fn(y_pred, y_test)\n",
    "    #acc = (torch.argmax(y_pred, 1) == torch.argmax(y_test, 1)).float().mean()\n",
    "    acc = decision_making_utils.F_score(y_pred, y_test)\n",
    "    ce = float(ce)\n",
    "    acc = float(acc)\n",
    "    train_loss_hist.append(np.mean(epoch_loss))\n",
    "    train_acc_hist.append(np.mean(epoch_acc))\n",
    "    test_loss_hist.append(ce)\n",
    "    test_acc_hist.append(acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "    if epoch % 10 == 0:\n",
    "        y_test_np = y_test.detach().numpy()\n",
    "        y_pred_np = y_pred.detach().numpy().round().astype(int)\n",
    "        accuracy = accuracy_score(y_test_np, y_pred_np)\n",
    "        print(f\"Epoch {epoch} | Train F2={np.mean(epoch_acc):.4f} | Test F2={acc:.4f} | Test accuracy={accuracy:.4f}\")\n",
    " \n",
    "# Restore best model\n",
    "model.load_state_dict(best_weights)\n",
    " \n",
    "# Plot the loss and accuracy\n",
    "plt.plot(train_loss_hist, label=\"train\")\n",
    "plt.plot(test_loss_hist, label=\"test\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cross entropy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    " \n",
    "plt.plot(train_acc_hist, label=\"train\")\n",
    "plt.plot(test_acc_hist, label=\"test\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"F score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = y_test.detach().numpy()\n",
    "y_pred_np = y_pred.detach().numpy().round().astype(int)\n",
    "multilabel_confusion_matrix(y_test_np, y_pred_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn\n",
    "\n",
    "Turned out that, at the moment, the result of multi-class classification and the result of multi-label classification are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## https://www.kaggle.com/code/residentmario/multi-label-classification-with-neural-networks\n",
    "\n",
    "X = moit2.X_all_sc\n",
    "y = y_all_multi\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a multi-label classifier\n",
    "classifier = MultiOutputClassifier(KNeighborsClassifier())\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and Hamming loss\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy) # In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "print(\"Hamming Loss:\", hamming_loss(y_test, y_pred)) # Hamming loss is the fraction of wrong labels to the total number of labels.\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average=\"micro\", zero_division=np.nan))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average=\"micro\", zero_division=np.nan))\n",
    "print(\"F2 score:\", fbeta_score(y_test, y_pred, beta=1, average=\"micro\", zero_division=np.nan))\n",
    "print(\"Multilabel confusion matrix:\\n\", multilabel_confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add stops & capture info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stopping_info_to_trajectory_info(traj_time, monkey_information):\n",
    "    bin_width = traj_time[1]-traj_time[0]\n",
    "    min_time = traj_time[0] - bin_width/2\n",
    "    max_time = traj_time[-1] + bin_width/2\n",
    "    time_bins = np.arange(min_time, max_time+bin_width/2, bin_width)  # add bin_width/2 to max_time so that max_time will be included in the array\n",
    "    num_bins = len(time_bins)-1\n",
    "    monkey_information_temp = monkey_information.copy()\n",
    "    corresponding_bins = np.searchsorted(time_bins, monkey_information['monkey_t'])\n",
    "    monkey_information_temp['corresponding_bins'] = corresponding_bins\n",
    "    monkey_sub = monkey_information_temp[monkey_information_temp['corresponding_bins'].between(1, num_bins)]\n",
    "    monkey_sub = monkey_sub[['corresponding_bins', 'monkey_speeddummy']].groupby('corresponding_bins').min() == 1 \n",
    "    # True means there has been stops in the bin\n",
    "    stopping_info = monkey_sub['monkey_speeddummy'].values\n",
    "    return stopping_info\n",
    "\n",
    "stopping_info = add_stopping_info_to_trajectory_info(traj_time, monkey_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width = traj_time[1]-traj_time[0]\n",
    "min_time = traj_time[0] - bin_width/2\n",
    "max_time = traj_time[-1] + bin_width/2\n",
    "time_bins = np.arange(min_time, max_time+bin_width/2, bin_width)  # add bin_width/2 to max_time so that max_time will be included in the array\n",
    "num_bins = len(time_bins)-1\n",
    "monkey_information_temp = monkey_information.copy()\n",
    "corresponding_bins = np.searchsorted(time_bins, monkey_information['monkey_t'])\n",
    "monkey_information_temp['corresponding_bins'] = corresponding_bins\n",
    "monkey_sub = monkey_information_temp[monkey_information_temp['corresponding_bins'].between(1, num_bins)]\n",
    "monkey_sub = monkey_sub[['corresponding_bins', 'monkey_speeddummy']].groupby('corresponding_bins').min() == 1 \n",
    "# True means there has been stops in the bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_sub['monkey_speeddummy'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find if there are stops within bins\n",
    "monkey_information['monkey_speeddummy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time = math.floor(spike_df.time.min())\n",
    "max_time = math.ceil(spike_df.time.max())\n",
    "bin_width = 0.25\n",
    "time_bins = np.arange(min_time, max_time, bin_width) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find point_vs_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the ff that monkey has passed by closely\n",
    "ff_dataframe_sifted = pd.DataFrame()\n",
    "for time in range(math.floor(ff_dataframe.time.min()), math.ceil(ff_dataframe.time.max()), 15):\n",
    "    duration = [time, time+20] # note, we try to create some overlaps between intervals\n",
    "    ff_dataframe_portion_to_keep = ff_dataframe_utils.keep_only_ff_that_monkey_has_passed_by_closely(ff_dataframe, duration=duration, max_distance_to_ff=100)\n",
    "    ff_dataframe_sifted = pd.concat([ff_dataframe_sifted, ff_dataframe_portion_to_keep])\n",
    "ff_dataframe_sifted = ff_dataframe_sifted.drop_duplicates()\n",
    "\n",
    "ff_dataframe_sifted_visible = ff_dataframe_sifted[ff_dataframe_sifted['visible'] == True]\n",
    "\n",
    "# change the second element of data_item.PlotTrials_args\n",
    "new_PlotTrials_args = list(data_item.PlotTrials_args)\n",
    "new_PlotTrials_args[1] = ff_dataframe_sifted\n",
    "new_PlotTrials_args = tuple(new_PlotTrials_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(data_item.data_folder_name, 'point_vs_cluster.csv')\n",
    "if exists(filepath): \n",
    "    point_vs_cluster = pd.read_csv(filepath).drop([\"Unnamed: 0\"], axis=1)     \n",
    "    point_vs_cluster = point_vs_cluster.set_index(['point_index', 'cluster_label']) \n",
    "else:\n",
    "    point_vs_cluster = cluster_analysis.make_point_vs_cluster(ff_dataframe_sifted_visible, max_ff_distance_from_monkey = 400, max_cluster_distance = 100, max_time_past = 0, \n",
    "                            print_progress = True, data_folder_name = None)\n",
    "    point_vs_cluster['time'] = monkey_information.loc[point_vs_cluster['point_index'], 'monkey_t'].values\n",
    "    point_vs_cluster.to_csv(filepath)\n",
    "    point_vs_cluster = point_vs_cluster.set_index(['point_index', 'cluster_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]                     \n",
    "max_plot_to_make = 2\n",
    "num_trials = 1\n",
    "mean_interval_between_plots = 0.3\n",
    "prev_plot_time = -99\n",
    "plot_counter = 0\n",
    " \n",
    " # use trials after 330\n",
    "point_vs_cluster_subset = point_vs_cluster[point_vs_cluster['target_index'] >= 330]   \n",
    "\n",
    "#print(\"Note: only showing the ff that monkey has passed by closely\")  \n",
    "# when checking the code later, I don't see how the sentence above was implemented. Maybe I need to run the code to see what happened\n",
    "\n",
    "# iterate through rows in point_vs_cluster\n",
    "for (point_index, cluster_label), row in point_vs_cluster_subset.iterrows():\n",
    "\n",
    "    # category_item.plot_trajectories(trials=[row.target_index.astype('int')])\n",
    "    # break\n",
    "\n",
    "    # also make sure that two plots are not too close to each other in time\n",
    "    if row.time > prev_plot_time + mean_interval_between_plots:\n",
    "        prev_plot_time = row.time\n",
    "        ff_indices = point_vs_cluster[point_vs_cluster.index == (point_index, cluster_label)].ff_index.to_numpy()\n",
    "        plotting_kwargs_temp = plotting_kwargs.copy()\n",
    "        plotting_kwargs_temp['indices_of_ff_to_mark_2nd_kind'] = ff_indices\n",
    "        plotting_kwargs_temp['absolute_steps_to_be_marked_2nd_kind'] = point_index\n",
    "\n",
    "        \n",
    "        trial = row.target_index.astype('int')\n",
    "        duration = [data_item.ff_caught_T_new[trial-num_trials], max(data_item.ff_caught_T_new[trial], row.time+4)]\n",
    "\n",
    "        returned_info = plot_trials.PlotTrials(\n",
    "                    duration, \n",
    "                    *new_PlotTrials_args,\n",
    "                    **plotting_kwargs_temp,\n",
    "                    null_agent_starting_time = row.time,\n",
    "                    currentTrial = trial,\n",
    "                    num_trials = num_trials,                   \n",
    "                    )\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        plot_counter += 1\n",
    "        if plot_counter >= max_plot_to_make:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot with eye positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]                     \n",
    "max_plot_to_make = 2\n",
    "num_trials = 1\n",
    "mean_interval_between_plots = 0.3\n",
    "prev_plot_time = -99\n",
    "plot_counter = 0\n",
    " \n",
    " # use trials after 330 \n",
    "point_vs_cluster_subset = point_vs_cluster[point_vs_cluster['target_index'] >= 330]   \n",
    "\n",
    "print(\"Note: only showing the ff that monkey has passed by closely\")   \n",
    "# when checking the code later, I don't see how the sentence above was implemented. Maybe I need to run the code to see what happened\n",
    "\n",
    "# iterate through rows in point_vs_cluster\n",
    "for (point_index, cluster_label), row in point_vs_cluster_subset.iterrows():\n",
    "\n",
    "    # category_item.plot_trajectories(trials=[row.target_index.astype('int')])\n",
    "    # break\n",
    "    if row.time > prev_plot_time + mean_interval_between_plots:\n",
    "        prev_plot_time = row.time\n",
    "        ff_indices = point_vs_cluster[point_vs_cluster.index == (point_index, cluster_label)].ff_index.to_numpy()\n",
    "        plotting_kwargs_temp = plotting_kwargs.copy()\n",
    "        plotting_kwargs_temp['indices_of_ff_to_mark_2nd_kind'] = ff_indices\n",
    "        plotting_kwargs_temp['absolute_steps_to_be_marked_2nd_kind'] = point_index\n",
    "        plotting_kwargs_temp['show_eye_positions'] = True\n",
    "        plotting_kwargs_temp['show_connect_path_eye_positions'] = True\n",
    "        \n",
    "        \n",
    "        \n",
    "        trial = row.target_index.astype('int')\n",
    "        duration = [data_item.ff_caught_T_new[trial-num_trials], max(data_item.ff_caught_T_new[trial], row.time+4)]\n",
    "\n",
    "        returned_info = plot_trials.PlotTrials(\n",
    "                    duration, \n",
    "                    *data_item.PlotTrials_args,\n",
    "                    **plotting_kwargs_temp,\n",
    "                    null_agent_starting_time = row.time,\n",
    "                    currentTrial = trial,\n",
    "                    num_trials = num_trials,                   \n",
    "                    )\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        plot_counter += 1\n",
    "        if plot_counter >= max_plot_to_make:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model of the intended target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first time using class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moit.traj_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moit.prepare_data_for_machine_learning(furnish_with_trajectory_data=True, trajectory_data_kind=\"position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moit.traj_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pattern_by_trials)\n",
    "reload(decision_making_class)\n",
    "ff_dataframe_truncated = ff_dataframe[ff_dataframe['time_since_last_vis'] <= 2.5]\n",
    "\n",
    "moit = intended_targets_classes.ModelOfIntendedTargets(ff_dataframe, ff_caught_T_new, ff_real_position_sorted, monkey_information, ff_flash_sorted, ff_life_sorted,\n",
    "                                                       time_range_of_trajectory=[-0.5, 2.5], num_time_points_for_trajectory=10)\n",
    "moit.retrieve_manual_anno() \n",
    "moit.get_and_process_manual_anno_long(n_seconds_after_crossing_boundary=2.5)\n",
    "moit.get_input_data(num_ff_per_row=5, keeping_1_out_of_n_rows=5)\n",
    "\n",
    "# store df that takes a long time to get\n",
    "free_selection_inputs_df = moit.free_selection_inputs_df\n",
    "free_selection_inputs = moit.free_selection_inputs\n",
    "free_selection_labels = moit.free_selection_labels\n",
    "free_selection_time = moit.free_selection_time\n",
    "cases_for_inspection = moit.cases_for_inspection\n",
    "sequence_of_obs_ff_indices = moit.sequence_of_obs_ff_indices\n",
    "chosen_rows_of_df = moit.chosen_rows_of_df\n",
    "non_chosen_rows_of_df = moit.non_chosen_rows_of_df\n",
    "\n",
    "\n",
    "moit.prepare_data_for_machine_learning(furnish_with_trajectory_data=True, trajectory_data_kind=\"position\")\n",
    "moit.split_data_to_train_and_test(scaling_data=True)\n",
    "moit.use_machine_learning_model(model=None)\n",
    "moit.get_pred_results_df()\n",
    "moit.plot_prediction_results(selected_cases=moit.wrong_predictions[:3], PlotTrials_args=PlotTrials_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cases for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moit.cases_for_inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reuse class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_class)\n",
    "ff_dataframe_truncated = ff_dataframe[ff_dataframe['time_since_last_vis'] <= 2.5]\n",
    "\n",
    "moit = intended_targets_classes.ModelOfIntendedTargets(ff_dataframe_truncated, ff_caught_T_new, ff_real_position_sorted, monkey_information, ff_flash_sorted, ff_life_sorted,\n",
    "                                                        time_range_of_trajectory=[-0.5, 2.5], num_time_points_for_trajectory=10)\n",
    "moit.get_and_process_manual_anno_long(n_seconds_after_crossing_boundary = 2.5)\n",
    "#moit.get_input_data(num_ff_per_row=5)\n",
    "moit.free_selection_inputs_df = free_selection_inputs_df\n",
    "moit.free_selection_inputs = free_selection_inputs\n",
    "moit.free_selection_labels = free_selection_labels\n",
    "moit.free_selection_time = free_selection_time\n",
    "moit.cases_for_inspection = cases_for_inspection\n",
    "moit.sequence_of_obs_ff_indices = sequence_of_obs_ff_indices\n",
    "moit.chosen_rows_of_df = chosen_rows_of_df\n",
    "moit.non_chosen_rows_of_df = non_chosen_rows_of_df\n",
    "\n",
    "moit.prepare_data_for_machine_learning(furnish_with_trajectory_data=True, trajectory_data_kind=\"position\")\n",
    "moit.split_data_to_train_and_test(scaling_data=True)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)                                       \n",
    "moit.use_machine_learning_model(model=mlp)\n",
    "moit.get_pred_results_df()\n",
    "#moit.plot_prediction_results(selected_cases=range(5))\n",
    "moit.plot_prediction_results(selected_cases=moit.wrong_predictions[:2], PlotTrials_args=PlotTrials_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision making\n",
    "\n",
    "There are two situations: \n",
    "\n",
    "Free selection:\tOne is when the monkey makes a decision among all possible ff (after catching the previous target)\n",
    "\n",
    "Replacement:\tThe other is when the monkey changes its mind (or decides not to change the current course) while pursuing another ff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use class for first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_class)\n",
    "reload(intended_targets_classes)\n",
    "\n",
    "dm = decision_making_class.DecisionMaking(ff_dataframe, ff_caught_T_new, ff_real_position_sorted, monkey_information,\n",
    "                                            time_range_of_trajectory=[-2.5, 0], num_time_points_for_trajectory=5)\n",
    "dm.retrieve_manual_anno()\n",
    "dm.separate_manual_anno()\n",
    "dm.eliminate_crossing_boundary_cases(n_seconds_after_crossing_boundary = 2.5)\n",
    "dm.get_replacement_data()\n",
    "dm.get_free_selection_data()\n",
    "\n",
    "dm.prepare_data_for_machine_learning(kind=\"free selection\", furnish_with_trajectory_data=True, trajectory_data_kind=\"position\") \n",
    "dm.split_data_to_train_and_test(scaling_data=True)\n",
    "dm.use_machine_learning_model(None)   \n",
    "dm.get_pred_results_df()  \n",
    "\n",
    "\n",
    "\n",
    "replacement_df = dm.replacement_df\n",
    "free_selection_df = dm.free_selection_df\n",
    "non_chosen_df = dm.non_chosen_df\n",
    "\n",
    "changing_pursued_ff_data = dm.changing_pursued_ff_data\n",
    "changing_pursued_ff_data_diff = dm.changing_pursued_ff_data_diff\n",
    "replacement_time = dm.replacement_time\n",
    "replacement_inputs = dm.replacement_inputs\n",
    "replacement_labels = dm.replacement_labels\n",
    "\n",
    "free_selection_inputs_df = dm.free_selection_inputs_df\n",
    "free_selection_inputs = dm.free_selection_inputs\n",
    "free_selection_labels = dm.free_selection_labels\n",
    "free_selection_time = dm.free_selection_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reuse class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(decision_making_class)\n",
    "reload(intended_targets_classes)\n",
    "\n",
    "dm = decision_making_class.DecisionMaking(ff_dataframe, ff_caught_T_new, ff_real_position_sorted, monkey_information)\n",
    "dm.retrieve_manual_anno()\n",
    "dm.separate_manual_anno()\n",
    "dm.eliminate_crossing_boundary_cases(n_seconds_after_crossing_boundary = 2.5)\n",
    "# dm.get_replacement_data()\n",
    "# dm.get_free_selection_data()\n",
    "\n",
    "dm.changing_pursued_ff_data = changing_pursued_ff_data\n",
    "dm.changing_pursued_ff_data_diff = changing_pursued_ff_data_diff\n",
    "dm.replacement_time = replacement_time\n",
    "dm.replacement_inputs = replacement_inputs\n",
    "dm.replacement_labels = replacement_labels\n",
    "\n",
    "dm.free_selection_inputs_df = free_selection_inputs_df\n",
    "dm.free_selection_inputs = free_selection_inputs\n",
    "dm.free_selection_labels = free_selection_labels\n",
    "dm.free_selection_time = free_selection_time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# bagging = BaggingClassifier()\n",
    "dm.prepare_data_for_machine_learning(kind=\"free selection\", furnish_with_trajectory_data=True, trajectory_data_kind=\"position\") \n",
    "dm.split_data_to_train_and_test(scaling_data=True)\n",
    "dm.use_machine_learning_model(model=None)   \n",
    "dm.get_pred_results_df()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.free_selection_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cases = None\n",
    "dm.plot_prediction_results(selected_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## incorporate trajectories\n",
    "\n",
    "2 ways: plot -0.1, -0.2, -0.3 linear speed & angular speed\n",
    "\n",
    "or plot the positions (in the polar plot relative to the present)\n",
    "\n",
    "the number of time points, as well as dt, can vary\n",
    "\n",
    "as for the model of the intended target of the agent, we can use both path before and path after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.changing_pursued_ff_data_diff[['ff_angle_diff', 'ff_angle_boundary_diff']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.changing_pursued_ff_data_diff['ff_angle_diff'], dm.changing_pursued_ff_data_diff['ff_angle_boundary_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.changing_pursued_ff_data_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_time = dm.replacement_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether for replacement or free selection, for each row, let's return the time as well (in the functions)\n",
    "\n",
    "# with the time, we can find the positions/speed prior to that time (write a function to do it)\n",
    "\n",
    "# and then, when preparing machine learning data, add the trajectory data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use machine learning for classification\n",
    "X_train = dm.X_train\n",
    "X_test = dm.X_test\n",
    "y_train = dm.y_train\n",
    "y_test = dm.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print probability of each option\n",
    "y_pred_prob = gnb.predict_proba(X_test)\n",
    "# show the correctness of each prediction along with probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = svm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = dt.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging decision-tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier()\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = bagging.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosted decision tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boosting = AdaBoostClassifier()\n",
    "boosting.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = boosting.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = boosting.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train the model\n",
    "rf = RandomForestClassifier(n_estimators=40, max_depth=10, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = rf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(20,20))\n",
    "# plot_tree(rf.estimators_[5], feature_names = X.columns, filled = True, rounded = True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# voting classification\n",
    "\n",
    "voting = VotingClassifier(estimators=[('logreg', logreg), ('svm', svm), ('dt', dt), ('bagging', bagging), ('boosting', boosting), ('rf', rf)], \n",
    "                          #voting='hard' # based on majority vote\n",
    "                          voting='soft' # based on sum of probability\n",
    "                          )\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = voting.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = voting.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "y_pred_prob = mlp.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## errors in manual_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_time_since_last_vis = dm.changing_pursued_ff_data['old_time_since_last_vis']\n",
    "weird_ones = old_time_since_last_vis[old_time_since_last_vis > 10] # see the weird values\n",
    "weird_one_time = dm.replacement_time[weird_ones.index].values\n",
    "weird_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_time_since_last_vis = dm.changing_pursued_ff_data['time_since_last_vis']\n",
    "weird_ones = old_time_since_last_vis[old_time_since_last_vis > 10] # see the weird values\n",
    "weird_one_time = dm.replacement_time[weird_ones.index].values\n",
    "weird_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df_reset = replacement_df.reset_index()\n",
    "replaced_ff_rows = replacement_df_reset[replacement_df_reset['time'].isin(weird_one_time)]\n",
    "replaced_ff_rows\n",
    "# the rest might come from the second half of replacement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_to_replacement_df = dm.prior_to_replacement_df\n",
    "prior_to_replacement_df.iloc[replaced_ff_rows.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_chosen_df_reset = non_chosen_df.reset_index()\n",
    "non_chosen_df_reset_rows = non_chosen_df_reset[non_chosen_df_reset['time'].isin(weird_one_time)]\n",
    "non_chosen_df_reset_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ff_row = manual_anno[manual_anno['time'] <= weird_one_time[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ff_row.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each ff, plot the arrow from old_ff to new_ff\n",
    "# max_num_lines = 20\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "# ax = plot_behaviors_utils.set_polar_background_for_plotting(ax, 400, color_visible_area_in_background=True)\n",
    "\n",
    "# # plot arrows in polar plot\n",
    "\n",
    "# changing_pursued_ff_sub = changing_pursued_ff_data.iloc[:max_num_lines]\n",
    "\n",
    "# old_ff_size = np.clip(2-changing_pursued_ff_sub['old_time_since_last_vis'].values, 0.2, 2)*25\n",
    "# new_ff_size = np.clip(2-changing_pursued_ff_sub['time_since_last_vis'].values, 0.2, 2)*25\n",
    "# ax.scatter(changing_pursued_ff_sub['old_ff_angle'], changing_pursued_ff_sub['old_ff_distance'], c=\"red\", alpha=0.7, zorder=2, s=old_ff_size, marker='o') # originally it was s=15\n",
    "# ax.scatter(changing_pursued_ff_sub['ff_angle'], changing_pursued_ff_sub['ff_distance'], c=\"green\", alpha=0.7, zorder=2, s=new_ff_size, marker='o') # originally it was s=15\n",
    "\n",
    "# for index, row in changing_pursued_ff_sub.iterrows():\n",
    "#     ax.plot(row[['old_ff_angle', 'ff_angle']], row[['old_ff_distance', 'ff_distance']], alpha=0.7, zorder=1)\n",
    "\n",
    "# plt.show()\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visible_before_last_one (target_cluster)\n",
    "\n",
    "Difference between target_cluster_visible_before_last_one trial and others\n",
    "\n",
    "Compare both at the time when target_cluster was last visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cluster_VBLO = pattern_by_trials.find_target_cluster_visible_before_last_one(target_cluster_df, ff_caught_T_new)\n",
    "# VBLO: visible before last one\n",
    "\n",
    "sort_1_df = target_cluster_VBLO\n",
    "sort_1_trials = target_cluster_VBLO.target_index.values\n",
    "#sort_1_name = \"VBLO\"\n",
    "sort_1_name = \"Visible Before Last Capture\"\n",
    "\n",
    "target_cluster_info_else = target_cluster_df[~target_cluster_df['target_index'].isin(target_cluster_VBLO.target_index.values)]\n",
    "sort_2_df = target_cluster_info_else\n",
    "sort_2_trials = target_cluster_info_else.target_index.values\n",
    "sort_2_name = \"Others\"\n",
    "\n",
    "category_item = category_class.ProcessCategoryData(sort_1_trials=sort_1_trials, sort_1_name=sort_1_name, sort_1_df=sort_1_df,\n",
    "                                                              sort_2_trials=sort_2_trials, sort_2_name=sort_2_name, sort_2_df=sort_2_df,\n",
    "                                                              PlotTrials_args=PlotTrials_args, ff_flash_sorted=ff_flash_sorted)\n",
    "category_item.clean_out_cross_boundary_trials()\n",
    "category_item.clean_out_trials_where_target_cluster_was_not_seen_for_a_long_time_before_capture()\n",
    "category_item.make_polar_plot_of_target_last_seen_positions()\n",
    "category_item.make_histograms_of_target_last_seen_attributes()\n",
    "category_item.make_histogram_of_distances_from_previous_targets()\n",
    "category_item.make_polar_plot_of_positions_from_previous_targets()\n",
    "category_item.plot_trajectories(trials=category_item.sort_1_trials[17:18])\n",
    "category_item.plot_distributions_of_visible_ff_and_in_memory_ff()\n",
    "category_item.make_and_visualize_free_selection_predictions_using_trained_model(trained_model = gnb, max_plot_to_make = 2)\n",
    "#category_item.inspect_special_cases(weird_trials=[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a few times vs. give up after trying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared with the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(decision_making_class)\n",
    "# reload(plot_trials)\n",
    "# reload(intended_targets_classes)\n",
    "\n",
    "try_a_few_times_trials = data_item.try_a_few_times_trials\n",
    "target_cluster_TAFT = target_cluster_df[target_cluster_df['target_index'].isin(try_a_few_times_trials)]\n",
    "sort_1_df = target_cluster_TAFT\n",
    "sort_1_trials = sort_1_df.target_index.values\n",
    "sort_1_name = \"Try a few times\"\n",
    "\n",
    "\n",
    "target_cluster_non_TAFT = target_cluster_df[~target_cluster_df['target_index'].isin(try_a_few_times_trials)]\n",
    "sort_2_df = target_cluster_non_TAFT\n",
    "sort_2_trials = sort_2_df.target_index.values\n",
    "sort_2_name = \"Non-TAFT\"\n",
    "\n",
    "category_item = category_class.ProcessCategoryData(sort_1_trials=sort_1_trials, sort_2_trials=sort_2_trials,\n",
    "                                                              sort_1_name=sort_1_name, sort_2_name=sort_2_name, \n",
    "                                                              sort_1_df=sort_1_df, sort_2_df=sort_2_df,                                                     \n",
    "                                                              PlotTrials_args=PlotTrials_args, ff_flash_sorted=ff_flash_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_item.clean_out_cross_boundary_trials()\n",
    "category_item.clean_out_trials_where_target_cluster_was_not_seen_for_a_long_time_before_capture()\n",
    "category_item.make_polar_plot_of_target_last_seen_positions()\n",
    "category_item.make_histograms_of_target_last_seen_attributes()\n",
    "category_item.make_histogram_of_distances_from_previous_targets()\n",
    "category_item.make_polar_plot_of_positions_from_previous_targets()\n",
    "category_item.plot_trajectories(trials=category_item.sort_1_trials[17:18])\n",
    "category_item.plot_distributions_of_visible_ff_and_in_memory_ff()\n",
    "category_item.make_and_visualize_free_selection_predictions_using_trained_model(trained_model = gnb, max_plot_to_make = 2)\n",
    "\n",
    "#category_item.inspect_special_cases(weird_trials=[98, 180, 212, 649])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give up after trying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pattern_by_trials)\n",
    "reload(decision_making_class)\n",
    "reload(plot_trials)\n",
    "reload(intended_targets_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_item = process_GUAT_trials_class.ProcessGUATtrials(data_item.give_up_after_trying_info_bundle, PlotTrials_args, max_distance_to_stop_for_GUAT_target=50, max_allowed_time_since_last_vis=2.5)\n",
    "GUAT_item.find_possible_objects_of_pursuit_in_GUAT()\n",
    "GUAT_item.find_GUAT_ff_aimed_at_from_manual_anno(pseudo_manual_anno)\n",
    "GUAT_item.check_GUAT_object_with_manual_anno(verbose=False)\n",
    "#GUAT_item.inspect_clusters_w_o_matching_ff()\n",
    "GUAT_item.make_GUAT_cluster_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_item.check_GUAT_object_with_manual_anno(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUAT_item.inspect_clusters_w_o_matching_ff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See some trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_item.plot_trajectories(trials=category_item.sort_2_trials[:10], show_connect_path_ff_except_targets=True})\n",
    "\n",
    "additional_kwargs = {'show_connect_path_ff_except_targets':True,\n",
    "                     'show_ff_indices':True,\n",
    "                     'absolute_steps_to_be_marked': relevant_indices}\n",
    "category_item.plot_trajectories(trials=650, additional_kwargs=additional_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with TAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_a_few_times_trials = data_item.try_a_few_times_trials\n",
    "give_up_after_trying_trials = data_item.give_up_after_trying_trials\n",
    "\n",
    "target_cluster_TAFT = target_cluster_df[target_cluster_df['target_index'].isin(try_a_few_times_trials)]\n",
    "target_cluster_GUAT = target_cluster_df[target_cluster_df['target_index'].isin(give_up_after_trying_trials)]\n",
    "\n",
    "sort_1_df = target_cluster_TAFT\n",
    "sort_1_trials = sort_1_df.target_index.values\n",
    "sort_1_name = \"Try a few times\"\n",
    "\n",
    "sort_2_df = GUAT_cluster_df\n",
    "sort_2_trials = sort_2_df.target_index.values\n",
    "sort_2_name = \"Give up after trying\"\n",
    "sort_2_ff_indices = GUAT_cluster_df.last_vis_ff_index.values\n",
    "\n",
    "category_item = category_class.ProcessCategoryData(sort_1_trials=sort_1_trials, sort_2_trials=sort_2_trials,\n",
    "                                                              sort_1_name=sort_1_name, sort_2_name=sort_2_name, \n",
    "                                                              sort_1_df=sort_1_df, sort_2_df=sort_2_df,\n",
    "                                                              sort_2_ff_indices = sort_2_ff_indices,\n",
    "                                                              sort_2_ff_positions = ff_real_position_sorted[sort_2_ff_indices],                                                             \n",
    "                                                              PlotTrials_args=PlotTrials_args, ff_flash_sorted=ff_flash_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_item.clean_out_cross_boundary_trials()\n",
    "category_item.clean_out_trials_where_target_cluster_was_not_seen_for_a_long_time_before_capture()\n",
    "category_item.make_polar_plot_of_target_last_seen_positions()\n",
    "category_item.make_histograms_of_target_last_seen_attributes()\n",
    "category_item.make_histogram_of_distances_from_previous_targets()\n",
    "category_item.make_polar_plot_of_positions_from_previous_targets()\n",
    "category_item.plot_trajectories(trials=category_item.sort_1_trials[17:18])\n",
    "category_item.plot_distributions_of_visible_ff_and_in_memory_ff()\n",
    "## the below function (free selection prediction) shouldn't be used on GUAT. So far, it should only be used on VBLO.\n",
    "# category_item.make_and_visualize_free_selection_predictions_using_trained_model(trained_model = gnb, max_plot_to_make = 99)\n",
    "# category_item.inspect_special_cases(weird_trials=[98, 180, 212, 649])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared with the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reload(decision_making_class)\n",
    "reload(plot_trials)\n",
    "reload(intended_targets_classes)\n",
    "\n",
    "\n",
    "give_up_after_trying_trials = data_item.give_up_after_trying_trials\n",
    "\n",
    "target_cluster_non_GUAT = target_cluster_df[~target_cluster_df['target_index'].isin(give_up_after_trying_trials)]\n",
    "target_cluster_GUAT = target_cluster_df[target_cluster_df['target_index'].isin(give_up_after_trying_trials)]\n",
    "\n",
    "sort_1_df = target_cluster_non_GUAT\n",
    "sort_1_trials = sort_1_df.target_index.values\n",
    "sort_1_name = \"Non-GUAT\"\n",
    "\n",
    "sort_2_df = GUAT_cluster_df\n",
    "sort_2_trials = sort_2_df.target_index.values\n",
    "sort_2_name = \"Give up after trying\"\n",
    "sort_2_ff_indices = GUAT_cluster_df.last_vis_ff_index.values\n",
    "sort_2_time_for_predicting_ff = GUAT_cluster_df.latest_visible_time_before_last_stop.values\n",
    "\n",
    "category_item = category_class.ProcessCategoryData(sort_1_trials=sort_1_trials, sort_2_trials=sort_2_trials,\n",
    "                                                              sort_1_name=sort_1_name, sort_2_name=sort_2_name, \n",
    "                                                              sort_1_df=sort_1_df, sort_2_df=sort_2_df,\n",
    "                                                              sort_2_ff_indices = sort_2_ff_indices,\n",
    "                                                              sort_2_ff_positions = ff_real_position_sorted[sort_2_ff_indices],   \n",
    "                                                              sort_2_time_for_predicting_ff = sort_2_time_for_predicting_ff,                                                       \n",
    "                                                              PlotTrials_args=PlotTrials_args, ff_flash_sorted=ff_flash_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_item.clean_out_cross_boundary_trials()\n",
    "category_item.clean_out_trials_where_target_cluster_was_not_seen_for_a_long_time_before_capture()\n",
    "category_item.make_polar_plot_of_target_last_seen_positions()\n",
    "category_item.make_histograms_of_target_last_seen_attributes()\n",
    "category_item.make_histogram_of_distances_from_previous_targets()\n",
    "category_item.make_polar_plot_of_positions_from_previous_targets()\n",
    "category_item.plot_trajectories(trials=category_item.sort_1_trials[17:18])\n",
    "category_item.plot_distributions_of_visible_ff_and_in_memory_ff()\n",
    "## The function below (free selection prediction) shouldn't be used on GUAT. So far, it should only be used on VBLO.\n",
    "category_item.make_and_visualize_free_selection_predictions_using_trained_model(trained_model = gnb, use_sort_1=False, use_sort_2=True, max_plot_to_make=3)\n",
    "# category_item.inspect_special_cases(weird_trials=[98, 180, 212, 649])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_plotting_kwargs = {'show_eye_positions': False,\n",
    "                     'show_eye_positions_on_the_right': False,\n",
    "                     'show_connect_path_eye_positions': False,\n",
    "                     'show_ff_indices': True,\n",
    "}\n",
    "\n",
    "temp_animation_plot_kwargs = data_item.animation_plot_kwargs.copy()\n",
    "temp_animation_plot_kwargs['images_dir'] = None\n",
    "\n",
    "for key, value in additional_plotting_kwargs.items():\n",
    "    temp_animation_plot_kwargs[key] = value\n",
    "temp_animation_plot_kwargs['show_connect_path_ff'] = False\n",
    "temp_animation_plot_kwargs['show_connect_path_ff_memory'] = False\n",
    "temp_animation_plot_kwargs['show_stops'] = False\n",
    "\n",
    "temp_animation_plot_kwargs['trail_color_var'] = \"grey\"\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['animation.ffmpeg_path'] = '/Users/dusiyi/Documents/Multifirefly-Project/ffmpeg'\n",
    "# plt.rcParams['animation.ffmpeg_path'] = './ffmpeg' #this works too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the default figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "\n",
    "for i in range(2200, 3000, 20):\n",
    "    #duration = [i, i+5]\n",
    "    duration = [i, i+25]\n",
    "    print(duration)\n",
    "    video_dir = \"/Users/dusiyi/Documents/Multifirefly-Project/for_manual_annotation2\"\n",
    "    file_name = f\"time_{duration[0]}_to_{duration[1]}.mp4\"\n",
    "    data_item.make_animation(duration=duration, save_video=True, video_dir=video_dir, file_name=file_name, show_ff_indices=True, \n",
    "                                static_plot_on_the_left=False, animation_plot_kwargs=temp_animation_plot_kwargs, plot_time_index=True, show_speed_through_path_color=True,\n",
    "                                max_num_frames=None, max_duration=None, min_duration=1, set_xy_limits=False)\n",
    "    HTML(data_item.anim.to_html5_video()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## based on trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_item.make_animation_from_a_category(\"ignore_sudden_flash\", max_trial_to_plot=1, num_trials=1, sampling_frame_ratio = 5, additional_kwargs=additional_plotting_kwargs,\n",
    "                                                        save_video=False, exists_ok=True, dt=0.016, plot_eye_position=True, plot_time_index=True)\n",
    "# note: if save_video is True, but video_dir is None, then video_dir is set to be the same as self.video_dir eventually\n",
    "HTML(data_item.anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item.make_animation_from_a_category(\"disappear_latest\", max_trial_to_plot=1, num_trials=2, sampling_frame_ratio = 5, save_video=False, exists_ok=True,\n",
    "                                                        with_annotation=True, dt=0.016)\n",
    "# note: if save_video is True, but video_dir is None, then video_dir is set to be the same as self.video_dir eventually\n",
    "HTML(data_item.anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(intended_targets_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_numbers = range(20, 21)\n",
    "additional_kwargs = {'show_connect_path_ff_memory': True,\n",
    "                     'show_connect_path_ff_except_targets': True,\n",
    "                     'show_path_when_target_visible': True,\n",
    "                     'show_connect_path_eye_positions': False}\n",
    "data_item.make_animation_of_chunks(points_w_more_than_2_ff, monkey_information, chunk_numbers = chunk_numbers, sampling_frame_ratio = 3, \n",
    "                                       additional_kwargs=additional_kwargs, exists_ok=True, save_video=False)\n",
    "HTML(data_item.anim.to_html5_video())                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmax = 55\n",
    "print(\"Using ff_angle_boundary\")\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_rlabel_position(292.5)\n",
    "ax.set_ylim(0, rmax)\n",
    "# Draw the boundary of the monkey's vision (use width = np.pi*4/9 for 40 degrees of vision)\n",
    "ax.bar(0, rmax, width=np.pi/2, bottom=0.0, color=\"grey\", alpha=0.1)\n",
    "plt.setp(ax, rorigin=0, rmin=0, rmax=rmax)   # rmax can be changed\n",
    "\n",
    "# Change the labels for the angles \n",
    "labels = list(ax.get_xticks())\n",
    "labels[5], labels[6], labels[7] = -labels[3], -labels[2], -labels[1]\n",
    "labels_degrees = [str(int(math.degrees(label))) + chr(176) for label in labels]\n",
    "ax.set_xticks(ax.get_xticks().tolist())\n",
    "ax.set_xticklabels(labels_degrees)\n",
    "\n",
    "# Plot the locations of the FFs\n",
    "ax.scatter(ignored_ff_target_pairs['ff_angle_boundary'], ignored_ff_target_pairs['ff_distance'], marker='.', s=20, alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through all the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_numbers = range(20, 30)\n",
    "additional_kwargs = {'show_connect_path_ff_memory': True,\n",
    "                     'show_connect_path_ff_except_targets': True,\n",
    "                     'show_path_when_target_visible': True,\n",
    "                     'show_connect_path_eye_positions': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a function to iterate through all the folders\n",
    "# raw_data_dir_name = 'all_monkey_data/raw_monkey_data/individual_monkey_data'\n",
    "# num_trials = 3\n",
    "# #for monkey_name in os.listdir(raw_data_dir_name):\n",
    "# for monkey_name in ['monkey_Bruno', 'monkey_Schro', 'monkey_Quigley']:\n",
    "#     monkey_path = os.path.join(raw_data_dir_name, monkey_name)\n",
    "#     for data_name in os.listdir(monkey_path):\n",
    "#         raw_data_folder_path = os.path.join(monkey_path, data_name)\n",
    "#         print('monkey_name:', monkey_name)\n",
    "#         print('data_name:', data_name)\n",
    "\n",
    "#         data_item = monkey_data_classes.ProcessMonkeyData(raw_data_folder_path=raw_data_folder_path)\n",
    "#         data_item.save_important_files(exists_ok=Truef)\n",
    "#         # data_item.retrieve_or_make_monkey_data()\n",
    "#         # data_item.make_or_retrieve_ff_dataframe(exists_ok=True)\n",
    "#         # data_item.find_patterns()\n",
    "#         # data_item.make_PlotTrials_apattern_by_points.find_points_w_more_than_n_ffbined_plot_kwargs, animation_plot_kwargs)\n",
    "        \n",
    "#         ## To save animation from a category across all folders\n",
    "#         # data_item.make_animation_from_a_category(\"give_up_after_trying\", max_trial_to_plot=5, \n",
    "#         #                                                         num_trials=2, sampling_frame_ratio = 5, exists_ok=True)\n",
    "\n",
    "#         # # To save animation from chunks with more than 2 ff across all folders\n",
    "#         # points_w_more_than_2_ff = pattern_by_points.find_points_w_more_than_n_ff(data_item.ff_dataframe, data_item.monkey_information, data_item.ff_caught_T_new)\n",
    "#         # points_w_more_than_2_ff = pattern_by_points.decrease_overlaps_between_chunks(points_w_more_than_2_ff, data_item.monkey_information, min_interval_between_chunks=5)\n",
    "#         # data_item.make_animation_of_chunks(points_w_more_than_2_ff, data_item.monkey_information, chunk_numbers = chunk_numbers, \\\n",
    "#         #                                        sampling_frame_ratio = 3, additional_kwargs=additional_kwargs, exists_ok=True) \n",
    "\n",
    "#         # To plot the distribution of ff over time\n",
    "#         #plot_behaviors_utils.plot_ff_distribution_in_arena(data_item.ff_real_position_sorted, data_item.ff_life_sorted, data_item.ff_caught_T_new, images_dir=None)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 7) \n",
    "data_item.plot_trials_from_a_category('two_in_a_row', max_trial_to_plot=2, images_dir=None, figsize=(7,7)) # if want to save the plot, pass in an argument for images_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunks and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_w_more_than_2_ff = pattern_by_points.find_points_w_more_than_n_ff(ff_dataframe, monkey_information, ff_caught_T_new)\n",
    "points_w_more_than_2_ff = pattern_by_points.decrease_overlaps_between_chunks(points_w_more_than_2_ff, monkey_information, min_interval_between_chunks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chunks_kwargs1 = plot_chunks_kwargs.copy()\n",
    "plot_chunks_kwargs1['trail_color_var'] = None\n",
    "plot_chunks_kwargs1['show_colorbar'] = False\n",
    "plot_chunks_kwargs1['show_alive_fireflies'] = False\n",
    "plot_chunks_kwargs1['show_legend'] = True\n",
    "plot_chunks_kwargs1['vary_color_for_connecting_path_ff'] = True\n",
    "plot_chunks_kwargs1['show_null_agent_trajectory'] = True\n",
    "plot_chunks_kwargs1['show_null_agent_trajectory_2nd_time'] = True\n",
    "plot_chunks_kwargs1['minimal_margin'] = 50\n",
    "\n",
    "\n",
    "plot_chunks_kwargs2 = {'player': 'monkey',\n",
    " 'show_stops': True,\n",
    " 'show_believed_target_positions': True,\n",
    " 'show_alive_fireflies': False,\n",
    " 'show_scale_bar': False,\n",
    " 'hitting_arena_edge_ok': True,\n",
    " 'trial_too_short_ok': True,\n",
    " 'show_connect_path_ff': True,\n",
    " 'trail_color_var': None,\n",
    " 'show_colorbar': False,\n",
    " 'adjust_xy_limits': False,\n",
    " 'vary_color_for_connecting_path_ff': True,\n",
    " 'show_reward_boundary': True}\n",
    "\n",
    "points_w_more_than_2_ff.chunk.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array([324, 325, 328, 342, 366, 347, 398, 417, 419])-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By monkey\n",
    "\n",
    "# plot the chunks\n",
    "for chunk in range(190, 193):\n",
    "  with basic_func.initiate_plot(7, 7, 100):\n",
    "    chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk'] == chunk]\n",
    "    duration_points = [chunk_df['point_index'].min(), chunk_df['point_index'].max()]\n",
    "    #duration = [monkey_information['monkey_t'][duration_points[0]], monkey_information['monkey_t'][duration_points[1]]]\n",
    "    duration = [monkey_information['monkey_t'][duration_points[0]], monkey_information['monkey_t'][duration_points[0]]+2]\n",
    "    print(\"duration\", duration)\n",
    "\n",
    "    \n",
    "    for i in range(2):\n",
    "        fig = plt.figure()\n",
    "        returned_info = plot_trials.PlotTrials(duration,\n",
    "                  *PlotTrials_args,\n",
    "                  **plot_chunks_kwargs1,\n",
    "                  fig=fig)\n",
    "        fig, axes = returned_info['fig'], returned_info['axes']\n",
    "        R = returned_info['R']\n",
    "        cum_mxy_rotated_1 = returned_info['cum_mxy_rotated']\n",
    "        shown_ff_indices_1 = returned_info['shown_ff_indices']\n",
    "        if i == 1:\n",
    "                  duration2 = [duration[1], duration[1]+2]\n",
    "                  returned_info = plot_trials.PlotTrials(duration2,\n",
    "                  *PlotTrials_args,\n",
    "                  **plot_chunks_kwargs2,\n",
    "                  fig=fig,\n",
    "                  axes=axes,\n",
    "                  rotation_matrix=R)\n",
    "                  axes = returned_info['axes']\n",
    "                  cum_mxy_rotated_2 = returned_info['cum_mxy_rotated']\n",
    "                  shown_ff_indices_2 = returned_info['shown_ff_indices']\n",
    "\n",
    "                  plot_behaviors_utils.readjust_xy_limits_for_axes(axes, cum_mxy_rotated_1, cum_mxy_rotated_2, shown_ff_indices_1, shown_ff_indices_2, R, ff_real_position_sorted, minimal_margin=50)\n",
    "\n",
    "        axes.set_aspect('equal')\n",
    "        axes.set_title('Chunk ' + str(chunk), fontsize=17)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_interval = 10\n",
    "points_w_more_than_2_ff = pattern_by_points.find_points_w_more_than_n_ff(ff_dataframe, monkey_information, ff_caught_T_new)\n",
    "points_w_more_than_2_ff = pattern_by_points.decrease_overlaps_between_chunks(points_w_more_than_2_ff, monkey_information, min_interval_between_chunks=chunk_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_numbers = range(300, 301)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 7)\n",
    "\n",
    "\n",
    "for chunk in chunk_numbers:\n",
    "  chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk'] == chunk]\n",
    "  changing_dw_info = pattern_by_points.find_points_w_more_than_n_ff(chunk_df, monkey_information, ff_caught_T_new, chunk_interval=10,\n",
    "                                             minimum_time_before_capturing = 0.5)\n",
    "  changing_dw_info = pattern_by_points.increase_durations_between_points(changing_dw_info, min_duration=2)\n",
    "  for index, row in changing_dw_info.iterrows():\n",
    "      point_index = row['point_index']\n",
    "      ddw = row['ddw']\n",
    "      duration = [monkey_information['monkey_t'][point_index]-2, monkey_information['monkey_t'][point_index]]\n",
    "      # if dw > 0:\n",
    "      #     # flip so that the monkey is always going towards its right side\n",
    "      #     ff_dataframe_sub['ff_angle'] = -ff_dataframe_sub['ff_angle']\n",
    "    \n",
    "      # plot_trials.PlotTrials(duration,\n",
    "      #     *PlotTrials_args,\n",
    "      #     **plot_chunks_kwargs,\n",
    "      #     show_connect_path_ff_memory=True)\n",
    "      \n",
    "      duration2 = [duration[1], min(duration[1]+5, ff_caught_T_new[ff_caught_T_new > duration[1]][0]+0.5)]\n",
    "      # Make a polar plot from the monkey's perspective in the duration\n",
    "      for i in range(2):\n",
    "          fig, axes = plot_polar.PlotPolar(duration,\n",
    "                      *plot_polar_args,\n",
    "                      **plot_polar_kwargs,\n",
    "                      ff_colormap = 'viridis',\n",
    "                      target_colormap = 'viridis',\n",
    "                      figsize = (5, 5)\n",
    "                          )\n",
    "          if i == 1:\n",
    "              fig, axes = plot_polar.PlotPolar(duration2,\n",
    "                          *plot_polar_args,\n",
    "                          **plot_polar_kwargs,          \n",
    "                          ff_colormap = 'Reds',\n",
    "                          target_colormap = 'Reds',\n",
    "                          fig = fig,\n",
    "                          ax = axes,\n",
    "                          show_colorbar = False,\n",
    "                          figsize = (5, 5)\n",
    "                              )          \n",
    "          \n",
    "          \n",
    "          # Add an annotation at the bottom\n",
    "          if ddw > 0: \n",
    "            annotation = \"Angular Acceleration to the Left\"\n",
    "          else:\n",
    "            annotation = \"Angular Acceleration to the Right\"\n",
    "          axes.annotate(annotation, xy=(0.5, 0.25), xycoords=\"axes fraction\", fontsize=15, ha=\"center\", va=\"center\")\n",
    "          \n",
    "          plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the chunks\n",
    "for chunk in range(70,72):\n",
    "  with basic_func.initiate_plot(7, 7, 100):\n",
    "    chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk'] == chunk]\n",
    "    duration_points = [chunk_df['point_index'].min(), chunk_df['point_index'].max()]\n",
    "    #duration = [monkey_information['monkey_t'][duration_points[0]], monkey_information['monkey_t'][duration_points[1]]]\n",
    "    duration = [monkey_information['monkey_t'][duration_points[0]], monkey_information['monkey_t'][duration_points[0]]+5]\n",
    "    print(\"duration\", duration)\n",
    "\n",
    "    returned_info = plot_trials.PlotTrials(duration,\n",
    "               *PlotTrials_args,\n",
    "               **plot_chunks_kwargs)\n",
    "\n",
    "    axes = returned_info['axes']           \n",
    "    axes.set_title('Chunk ' + str(chunk), fontsize=17)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the chunks\n",
    "for chunk in range(70,73):\n",
    "  with basic_func.initiate_plot(7, 7, 100):\n",
    "    chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk'] == chunk]\n",
    "    duration_points = [chunk_df['point_index'].min(), chunk_df['point_index'].max()]\n",
    "    #duration = [monkey_information['monkey_t'][duration_points[0]], monkey_information['monkey_t'][duration_points[1]]]\n",
    "    duration = [monkey_information['monkey_t'][duration_points[0]], monkey_information['monkey_t'][duration_points[0]]+2]\n",
    "    print(\"duration\", duration)\n",
    "\n",
    "    fig, axes = plot_polar.PlotPolar(duration,\n",
    "                *plot_polar_args,\n",
    "                **plot_polar_kwargs,\n",
    "                ff_colormap = 'viridis',\n",
    "                target_colormap = 'viridis',\n",
    "                figsize=(5,5)\n",
    "                    )\n",
    "    axes.set_title('Chunk ' + str(chunk), fontsize=17)\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot sub-chunks with high abs ddw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_interval = 10\n",
    "points_w_more_than_2_ff = pattern_by_points.find_points_w_more_than_n_ff(ff_dataframe, monkey_information, ff_caught_T_new)\n",
    "points_w_more_than_2_ff = pattern_by_points.decrease_overlaps_between_chunks(points_w_more_than_2_ff, monkey_information, min_interval_between_chunks=chunk_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_numbers = range(320, 321)\n",
    "\n",
    "for chunk in chunk_numbers:\n",
    "\n",
    "  chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk'] == chunk]\n",
    "  changing_dw_info = pattern_by_points.find_points_w_more_than_n_ff(chunk_df, monkey_information, ff_caught_T_new, chunk_interval=10,\n",
    "                                             minimum_time_before_capturing = 0.5)\n",
    "  changing_dw_info = pattern_by_points.increase_durations_between_points(changing_dw_info, min_duration=2)\n",
    "  for index, row in changing_dw_info.iterrows():\n",
    "      point_index = row['point_index']\n",
    "      ddw = row['ddw']\n",
    "      duration = [monkey_information['monkey_t'][point_index]-2, monkey_information['monkey_t'][point_index]]\n",
    "      # if dw > 0:\n",
    "      #     # flip so that the monkey is always going towards its right side\n",
    "      #     ff_dataframe_sub['ff_angle'] = -ff_dataframe_sub['ff_angle']\n",
    "    \n",
    "      # plot_trials.PlotTrials(duration,\n",
    "      #     *PlotTrials_args,\n",
    "      #     **plot_chunks_kwargs,\n",
    "      #     show_connect_path_ff_memory=True)\n",
    "      \n",
    "      duration2 = [duration[1], min(duration[1]+5, ff_caught_T_new[ff_caught_T_new > duration[1]][0]+0.5)]\n",
    "      # Make a polar plot from the monkey's perspective in the duration\n",
    "      for i in range(2):\n",
    "          fig, axes = plot_polar.PlotPolar(duration,\n",
    "                      *plot_polar_args,\n",
    "                      **plot_polar_kwargs,\n",
    "                      ff_colormap = 'viridis',\n",
    "                      target_colormap = 'viridis',\n",
    "                      figsize = (5,5)\n",
    "                          )\n",
    "          if i == 1:\n",
    "              fig, axes = plot_polar.PlotPolar(duration2,\n",
    "                          *plot_polar_args,\n",
    "                          **plot_polar_kwargs,          \n",
    "                          ff_colormap = 'Reds',\n",
    "                          target_colormap = 'Reds',\n",
    "                          fig = fig,\n",
    "                          ax = axes,\n",
    "                          show_colorbar = False\n",
    "                              )          \n",
    "          \n",
    "          \n",
    "          # Add an annotation at the bottom\n",
    "          if ddw > 0: \n",
    "            annotation = \"Angular Acceleration to the Left\"\n",
    "          else:\n",
    "            annotation = \"Angular Acceleration to the Right\"\n",
    "          axes.annotate(annotation, xy=(0.5, 0.25), xycoords=\"axes fraction\", fontsize=15, ha=\"center\", va=\"center\")\n",
    "          \n",
    "          \n",
    "          plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlap: viridis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "chunk_numbers = range(110, 115)\n",
    "counter = 0\n",
    "max_overlapped_plots = 100\n",
    "plot_polar_kwargs_temp = plot_polar_kwargs.copy()\n",
    "plot_polar_kwargs_temp['show_ff_in_memory'] = False\n",
    "\n",
    "color_visible_area_in_background = True\n",
    "show_colorbar = True\n",
    "for chunk in chunk_numbers:\n",
    "  chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk']== chunk]\n",
    "  changing_dw_info = pattern_by_points.find_points_w_more_than_n_ff(chunk_df, monkey_information, ff_caught_T_new, chunk_interval=10,\n",
    "                                             minimum_time_before_capturing = 0.5)\n",
    "  changing_dw_info = pattern_by_points.increase_durations_between_points(changing_dw_info, min_duration=2)\n",
    "  \n",
    "  for index, row in changing_dw_info.iterrows():\n",
    "      counter += 1\n",
    "      point_index = row['point_index']\n",
    "      ddw = row['ddw']\n",
    "      duration = [monkey_information['monkey_t'][point_index]-0.2, monkey_information['monkey_t'][point_index]]\n",
    "      ff_dataframe_temp = ff_dataframe.copy()\n",
    "      if ddw > 0:\n",
    "          # flip so that the monkey is always going towards its right side\n",
    "          ff_dataframe_temp['ff_angle'] = -ff_dataframe_temp['ff_angle']\n",
    "    \n",
    "\n",
    "      fig, axes = plot_polar.PlotPolar(duration,\n",
    "                  monkey_information,\n",
    "                  ff_dataframe_temp, \n",
    "                  ff_life_sorted,\n",
    "                  ff_real_position_sorted,\n",
    "                  ff_caught_T_new,\n",
    "                  ff_flash_sorted,\n",
    "                  **plot_polar_kwargs_temp,\n",
    "                  ff_colormap = 'viridis',\n",
    "                  target_colormap = 'viridis',\n",
    "                  show_legend = False,\n",
    "                  fig = fig,\n",
    "                  ax = axes,\n",
    "                  color_visible_area_in_background = color_visible_area_in_background,\n",
    "                  show_colorbar = show_colorbar,\n",
    "                  size_increase_for_visible_ff= 5,\n",
    "                      )        \n",
    "      \n",
    "      color_visible_area_in_background = False # it only needs to be true when plotting for the first time\n",
    "      show_colorbar = False\n",
    "      # # Add an annotation at the bottom\n",
    "      # if ddw > 0: \n",
    "      #   annotation = \"Angular Acceleration to the Left\"\n",
    "      # else:\n",
    "      #   annotation = \"Angular Acceleration to the Right\"\n",
    "      # axes.annotate(annotation, xy=(0.5, 0.25), xycoords=\"axes fraction\", fontsize=15, ha=\"center\", va=\"center\")\n",
    "\n",
    "      if counter > max_overlapped_plots:\n",
    "          print(\"Reached max_overlapped_plots\")\n",
    "          break\n",
    "\n",
    "\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlap: different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_colormaps = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "                      'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "                      'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "sequential_colormaps = [map + '_r' for map in sequential_colormaps]  \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "chunk_numbers = range(110, 115)\n",
    "counter = 0\n",
    "max_overlapped_plots = len(sequential_colormaps)*2-1\n",
    "plot_polar_kwargs_temp = plot_polar_kwargs.copy()\n",
    "plot_polar_kwargs_temp['show_ff_in_memory'] = False\n",
    "stop = False\n",
    "\n",
    "color_visible_area_in_background = True\n",
    "show_colorbar = True\n",
    "for chunk in chunk_numbers:\n",
    "  if stop:\n",
    "      break\n",
    "  chunk_df = points_w_more_than_2_ff[points_w_more_than_2_ff['chunk'] == chunk]\n",
    "  changing_dw_info = pattern_by_points.find_points_w_more_than_n_ff(chunk_df, monkey_information, ff_caught_T_new, chunk_interval=10,\n",
    "                                             minimum_time_before_capturing = 0.5)\n",
    "  changing_dw_info = pattern_by_points.increase_durations_between_points(changing_dw_info, min_duration=2)\n",
    "  for index, row in changing_dw_info.iterrows():\n",
    "      counter += 1\n",
    "      point_index = row['point_index']\n",
    "      ddw = row['ddw']\n",
    "      duration = [monkey_information['monkey_t'][point_index]-0.2, monkey_information['monkey_t'][point_index]]\n",
    "      ff_dataframe_temp = ff_dataframe.copy()\n",
    "      if ddw > 0:\n",
    "          # flip so that the monkey is always accelerating towards the right\n",
    "          ff_dataframe_temp['ff_angle'] = -ff_dataframe_temp['ff_angle']\n",
    "    \n",
    "\n",
    "      fig, axes = plot_polar.PlotPolar(duration,\n",
    "                  monkey_information,\n",
    "                  ff_dataframe_temp, \n",
    "                  ff_life_sorted,\n",
    "                  ff_real_position_sorted,\n",
    "                  ff_caught_T_new,\n",
    "                  ff_flash_sorted,\n",
    "                  **plot_polar_kwargs_temp,\n",
    "                  ff_colormap = sequential_colormaps[counter%len(sequential_colormaps)],\n",
    "                  target_colormap = sequential_colormaps[counter%len(sequential_colormaps)],\n",
    "                  show_legend = False,\n",
    "                  fig = fig,\n",
    "                  ax = axes,\n",
    "                  color_visible_area_in_background=color_visible_area_in_background,\n",
    "                  show_colorbar = show_colorbar,\n",
    "                  size_increase_for_visible_ff= 8\n",
    "                      )        \n",
    "      \n",
    "      color_visible_area_in_background = False\n",
    "      show_colorbar = False\n",
    "      # # Add an annotation at the bottom\n",
    "      # if ddw > 0: \n",
    "      #   annotation = \"Angular Acceleration to the Left\"\n",
    "      # else:\n",
    "      #   annotation = \"Angular Acceleration to the Right\"\n",
    "      # axes.annotate(annotation, xy=(0.5, 0.25), xycoords=\"axes fraction\", fontsize=15, ha=\"center\", va=\"center\")\n",
    "\n",
    "      if counter == max_overlapped_plots:\n",
    "          print(\"Reached max_overlapped_plots\")\n",
    "          stop = True\n",
    "          break\n",
    "\n",
    "\n",
    "plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot parameter vs outcome\n",
    "\n",
    "Should I use marginal or conditional?\n",
    "\n",
    "Should I have 2 variables or 3 variables?\n",
    "\n",
    "Or should I just try all of the above?\n",
    "\n",
    "Or, can I make an interactive plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_folder = 'RL_models/SB3_stored_models/all_agents/gen_5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_medians_record = pd.read_csv(overall_folder + 'feature_medians_record.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "feature_means_record = pd.read_csv(overall_folder + 'feature_means_record.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "pattern_frequencies_record = pd.read_csv(overall_folder +'pattern_frequencies_record.csv').drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_interest = pattern_frequencies_record\n",
    "\n",
    "parameter_columns = ['action_noise_std', 'ffxy_noise_std', 'num_obs_ff', 'max_in_memory_time']\n",
    "outcome_columns = np.setdiff1d(df_of_interest.columns, parameter_columns)\n",
    "#plot_statistics.plot_correlations_in_record(df=df_of_interest, parameter_columns=parameter_columns, outcome_columns=outcome_columns)\n",
    "outcome_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking what works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_record = feature_medians_record[['action_noise_std', 'ffxy_noise_std', 'num_obs_ff', 'max_in_memory_time']].copy()\n",
    "# parameters_record['working'] = 9 # I will change this manually; 2 meanings working well; 1 means somewhat working; 0 meanings not working\n",
    "# parameters_record = parameters_record.sort_values(by=['action_noise_std', 'ffxy_noise_std', 'num_obs_ff', 'max_in_memory_time']).reset_index(drop=True)\n",
    "# parameters_record.to_csv(overall_folder + 'parameters_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_record = pd.read_csv(overall_folder + 'parameters_record.csv').drop([\"Unnamed: 0\"], axis=1)\n",
    "parameters_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_columns = ['action_noise_std', 'ffxy_noise_std', 'num_obs_ff', 'max_in_memory_time']\n",
    "outcome_columns = parameter_columns.copy()\n",
    "plot_statistics.plot_correlations_in_record(parameters_record, parameter_columns=parameter_columns, outcome_columns=outcome_columns,\\\n",
    "                            color_column='working')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interactions BETWEEN the parameter columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_columns = ['action_noise_std', 'ffxy_noise_std', 'num_obs_ff', 'max_in_memory_time']\n",
    "outcome_columns = parameter_columns.copy()\n",
    "plot_statistics.plot_correlations_in_record(feature_medians_record, parameter_columns=parameter_columns, outcome_columns=outcome_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_outcome_columns = ['abs_angle_last_vis', 'd_last_vis', 'n_ff_in_a_row', 'num_stops',\n",
    "'num_stops_near_target', 'num_stops_since_last_vis', 't', 't_last_vis']\n",
    "plot_statistics.plot_correlations_in_record(feature_medians_record, outcome_columns=feature_outcome_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_outcome_columns = ['abs_angle_last_vis', 'd_last_vis', 'n_ff_in_a_row', 'num_stops',\n",
    "'num_stops_near_target', 'num_stops_since_last_vis', 't', 't_last_vis']\n",
    "plot_statistics.plot_correlations_in_record(feature_means_record, outcome_columns=feature_outcome_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_outcome_columns = ['ff_capture_rate', 'stop_success_rate', 'two_in_a_row', 'waste_cluster_around_target', \n",
    " 'disappear_latest', 'visible_before_last_one', 'try_a_few_times', 'give_up_after_trying', 'ignore_sudden_flash']\n",
    " \n",
    "plot_statistics.plot_correlations_in_record(pattern_frequencies_record, outcome_columns=pattern_outcome_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try using a dummy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dict = {}\n",
    "for column in feature_medians_record.columns:\n",
    "    dummy_dict[column] = np.random.rand(10)\n",
    "dummy_dict\n",
    "dummy_df = pd.DataFrame(dummy_dict)\n",
    "\n",
    "\n",
    "plot_statistics.plot_correlations_in_record(df=dummy_df, parameter_columns=parameter_columns, outcome_columns=feature_outcome_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot statistics \n",
    "Compare monkey and agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate statistics of patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/individual_monkey_data/monkey_Bruno/data_0314\"\n",
    "data_item = monkey_data_classes.ProcessMonkeyData(raw_data_folder_path=raw_data_folder_path)\n",
    "data_item.retrieve_or_make_monkey_data()\n",
    "data_item.make_or_retrieve_ff_dataframe(exists_ok=True)\n",
    "\n",
    "data_item.find_patterns()\n",
    "data_item.make_PlotTrials_args()\n",
    "data_item.make_df_related_to_patterns_and_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and merging stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original intention of the code is that pattern_frequencies_a and feature_statistics_a will be freshly generated\n",
    "# But now, for the sake of demonstration...\n",
    "pattern_frequencies_a = data_item.pattern_frequencies.copy()\n",
    "pattern_frequencies_m = data_item.pattern_frequencies.copy()\n",
    "feature_statistics_a = data_item.feature_statistics.copy()\n",
    "feature_statistics_m = data_item.feature_statistics.copy()\n",
    "pattern_frequencies_a['Player'] = \"Agent\"\n",
    "feature_statistics_a['Player'] = \"Agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_monkey_pattern_frequencies = organize_patterns_and_features.combine_df_of_agent_and_monkey(pattern_frequencies_m, pattern_frequencies_a, agent_names = [\"Agent\", \"Agent2\", \"Agent3\"])\n",
    "agent_monkey_feature_statistics = organize_patterns_and_features.combine_df_of_agent_and_monkey(feature_statistics_m, feature_statistics_a, agent_names = [\"Agent\", \"Agent2\", \"Agent3\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting: same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_monkey_feature_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_merged_df(agent_monkey_pattern_frequencies, x=\"Label\", y=\"Rate\")\n",
    "plt.show()\n",
    "\n",
    "temp_agent_monkey_feature_statistics = agent_monkey_feature_statistics.copy()\n",
    "temp_agent_monkey_feature_statistics = temp_agent_monkey_feature_statistics[temp_agent_monkey_feature_statistics['Label']!='distance target last seen']\n",
    "plot_statistics.plot_merged_df(temp_agent_monkey_feature_statistics, x=\"Label\", y=\"Median\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting: individual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_merged_df_by_category(agent_monkey_feature_statistics, category_column_name=\"Label for median\", y=\"Median\", category_order=None, percentage=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting histograms\n",
    "\n",
    "**Get stored data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trial_features_m = pd.read_csv('all_monkey_data/raw_monkey_data/individual_monkey_data/monkey_Bruno/data_0219/processed_data/patterns_and_features/all_trial_features.csv')\n",
    "# all_trial_features_m = pd.read_csv(data_folder_name + '/patterns_and_features/all_trial_features.csv')\n",
    "all_trial_features_valid_m = all_trial_features_m[(all_trial_features_m['t_last_vis']<50) & (all_trial_features_m['hitting_arena_edge']==False)].reset_index()\n",
    "median_values_m = all_trial_features_valid_m.median(axis=0)\n",
    "all_trial_features_valid_m = all_trial_features_m[(all_trial_features_m['t_last_vis'] < 50) & (all_trial_features_m['hitting_arena_edge']==False)].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trial_features_valid = all_trial_features[(all_trial_features['t_last_vis'] < 50) & (all_trial_features['hitting_arena_edge']==False)].reset_index()\n",
    "plot_statistics.plot_feature_histograms_for_monkey_and_agent(all_trial_features_valid_m, all_trial_features_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative locations of all the ignored ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "rmax = 55\n",
    "print(\"Using ff_angle\")\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_rlabel_position(292.5)\n",
    "ax.set_ylim(0, rmax)\n",
    "# Draw the boundary of the monkey's vision (use width = np.pi*4/9 for 40 degrees of vision)\n",
    "ax.bar(0, rmax, width=np.pi/2, bottom=0.0, color=\"grey\", alpha=0.1)\n",
    "plt.setp(ax, rorigin=0, rmin=0, rmax=rmax)   # rmax can be changed\n",
    "\n",
    "# Change the labels for the angles \n",
    "labels = list(ax.get_xticks())\n",
    "labels[5], labels[6], labels[7] = -labels[3], -labels[2], -labels[1]\n",
    "labels_degrees = [str(int(math.degrees(label))) + chr(176) for label in labels]\n",
    "ax.set_xticks(ax.get_xticks().tolist())\n",
    "ax.set_xticklabels(labels_degrees)\n",
    "\n",
    "# Plot the locations of the FFs\n",
    "ax.scatter(ignored_ff_target_pairs['ff_angle'], ignored_ff_target_pairs['ff_distance'], marker='.', s=20, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target cluster attributes when last visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(data_item.data_folder_name, 'target_cluster_df.csv')\n",
    "if exists(filepath):\n",
    "    target_cluster_df = pd.read_csv(filepath).drop([\"Unnamed: 0\"], axis=1)\n",
    "else:\n",
    "    target_cluster_df = cluster_analysis.find_target_cluster_df(monkey_information, ff_real_position_sorted, ff_caught_T_new, ff_life_sorted, ff_dataframe, max_distance=50)\n",
    "    target_cluster_df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_distribution(target_cluster_df['time_since_last_vis'], xlim=(0, 10), x_of_vline=2, scale_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics.plot_distribution(target_cluster_df['last_vis_dist'], xlim=None, scale_factor=1, x_of_vline=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['last_vis_ang', 'last_vis_ang_to_bndry']:\n",
    "    plot_statistics.plot_distribution(target_cluster_df[column], xlim=None, scale_factor=180/pi, x_of_vline=None, plot_cdf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['abs_last_vis_ang', 'abs_last_vis_ang_to_bndry']:\n",
    "    plot_statistics.plot_distribution(target_cluster_df[column], xlim=None, scale_factor=180/pi, x_of_vline=None, plot_cdf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## angle vs time\n",
    "target_angle_to_boundary_last_vis VS. time_since_target_cluster_last_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "temp_time_since_last_vis = target_cluster_df['time_since_last_vis']\n",
    "temp_time_since_last_vis[temp_time_since_last_vis > 7] = 7\n",
    "plt.scatter(temp_time_since_last_vis, target_cluster_df['abs_last_vis_ang_to_bndry']*180/np.pi, s=1)\n",
    "plt.xlabel('Time since last visible (s)')\n",
    "plt.ylabel('Absolute angle to boundary (deg)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect angles to boundary\n",
    "\n",
    "Since target cluster became last visible, has the angle to boundary of the target ever become greater than 40 degrees?\n",
    "\n",
    "Note: here we use angle to reward boundary, not angle to visible boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_indices_of_each_cluster = cluster_analysis.find_alive_target_clusters(ff_real_position_sorted, ff_caught_T_new, ff_life_sorted, max_distance=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(basic_func)\n",
    "problematic_trials = []\n",
    "# for each target_index\n",
    "for index, row in target_cluster_df.iterrows():\n",
    "    if row.time_since_last_vis < 3:\n",
    "        target_index = row.target_index\n",
    "        #ff_indices = ff_indices_of_each_cluster[target_index]\n",
    "        ff_indices = np.array([target_index]) # I realized that ultimately ff_indices can only be target_index, cause otherwise there will likely be big angles\n",
    "        time_interval = [row.last_vis_time, row.caught_time]\n",
    "        # in this time interval, calculate all ff_angle_boundary of ff_indices and see if any is greater than 45\n",
    "        monkey_information_subset = monkey_information[(monkey_information.monkey_t >= time_interval[0]) & (monkey_information.monkey_t <= time_interval[1])]\n",
    "        monkey_xy_relevant = monkey_information_subset[['monkey_x', 'monkey_y']].values\n",
    "        monkey_angles_relevant = monkey_information_subset['monkey_angles'].values\n",
    "        for ff_index in ff_indices:\n",
    "            ff_distance_relevant = LA.norm(monkey_xy_relevant-ff_real_position_sorted[ff_index], axis=1)\n",
    "            angles_to_ff = basic_func.calculate_angles_to_ff_centers(ff_x=ff_real_position_sorted[ff_index, 0], ff_y=ff_real_position_sorted[ff_index, 1], mx=monkey_xy_relevant[:, 0], my=monkey_xy_relevant[:, 1], m_angle=monkey_angles_relevant)\n",
    "            angles_to_boundaries = basic_func.calculate_angles_to_ff_boundaries(angles_to_ff=angles_to_ff, distances_to_ff=ff_distance_relevant, \n",
    "                                                                                ff_radius=10)      \n",
    "            angles_to_boundaries = angles_to_boundaries*180/math.pi\n",
    "            big_angles_to_boundaries_indices = np.where(np.abs(angles_to_boundaries) >= 40)[0]\n",
    "            # if the angle only appears after the monkey passes the ff, then it's not a problem\n",
    "            if len(big_angles_to_boundaries_indices) > 0:\n",
    "                big_angles_to_boundaries = angles_to_boundaries[big_angles_to_boundaries_indices]\n",
    "                if not np.any(np.abs(angles_to_boundaries[:big_angles_to_boundaries_indices[0]]) < 30):\n",
    "                    problematic_trials.append(target_index)\n",
    "                    print('target_index: ', target_index)\n",
    "                    print('ff_index: ', ff_index, 'angles_to_boundaries: ', big_angles_to_boundaries)\n",
    "                    print('all angles_to_boundaries: ', angles_to_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a pandas dataframe into a series\n",
    "row.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row = target_cluster_df.loc[target_cluster_df['target_index']==426]\n",
    "for index, row in row.iterrows():\n",
    "    if row.time_since_last_vis < 3:\n",
    "        target_index = row.target_index\n",
    "        #ff_indices = ff_indices_of_each_cluster[target_index]\n",
    "        ff_indices = np.array([target_index]) # I realized that ultimately ff_indices can only be target_index, cause otherwise there will likely be big angles\n",
    "        time_interval = [row.last_vis_time, row.caught_time]\n",
    "        # in this time interval, calculate all ff_angle_boundary of ff_indices and see if any is greater than 45\n",
    "        monkey_information_subset = monkey_information[(monkey_information.monkey_t >= time_interval[0]) & (monkey_information.monkey_t <= time_interval[1])]\n",
    "        monkey_xy_relevant = monkey_information_subset[['monkey_x', 'monkey_y']].values\n",
    "        monkey_angles_relevant = monkey_information_subset['monkey_angles'].values\n",
    "        for ff_index in ff_indices:\n",
    "            ff_distance_relevant = LA.norm(monkey_xy_relevant-ff_real_position_sorted[ff_index], axis=1)\n",
    "            angles_to_ff = basic_func.calculate_angles_to_ff_centers(ff_x=ff_real_position_sorted[ff_index, 0], ff_y=ff_real_position_sorted[ff_index, 1], mx=monkey_xy_relevant[:, 0], my=monkey_xy_relevant[:, 1], m_angle=monkey_angles_relevant)\n",
    "            angles_to_boundaries = basic_func.calculate_angles_to_ff_boundaries(angles_to_ff=angles_to_ff, distances_to_ff=ff_distance_relevant, \n",
    "                                                                                ff_radius=10)      \n",
    "            angles_to_boundaries = angles_to_boundaries*180/math.pi\n",
    "            big_angles_to_boundaries_indices = np.where(np.abs(angles_to_boundaries) >= 40)[0]\n",
    "            # if the angle only appears after the monkey passes the ff, then it's not a problem\n",
    "            if len(big_angles_to_boundaries_indices) > 0:\n",
    "                big_angles_to_boundaries = angles_to_boundaries[big_angles_to_boundaries_indices]\n",
    "                problematic_trials.append(target_index)\n",
    "                print('target_index: ', target_index)\n",
    "                print('ff_index: ', ff_index, 'angles_to_boundaries: ', big_angles_to_boundaries)\n",
    "                print('all angles_to_boundaries: ', angles_to_boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1295500/210663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_dataframe[ff_dataframe['visible']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plot_behaviors_utils)\n",
    "num_trials = 2\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "for currentTrial in [664]:\n",
    "    plot_behaviors_utils.plot_a_trial(currentTrial, 2, ff_caught_T_new, PlotTrials_args, additional_kwargs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positions of target clusters when last visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n",
    "ax = plot_behaviors_utils.set_polar_background_for_plotting(ax, 400)\n",
    "ax.scatter(target_cluster_df['last_vis_ang'], target_cluster_df['last_vis_dist'], s=2, alpha=0.5, zorder=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See if monkey_information and smr are in sinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data_folder_path = \"all_monkey_data/raw_monkey_data/individual_monkey_data/monkey_Bruno/data_0330\"\n",
    "Channel_signal_output, marker_list, smr_sampling_rate = process_raw_data.smr_extractor(raw_data_folder_path = raw_data_folder_path).extract_data()\n",
    "# Considering the first smr file, use marker_list[0], Channel_signal_output[0]\n",
    "juice_timestamp = marker_list[0]['values'][marker_list[0]['labels'] == 4]\n",
    "Channel_signal_smr1 = Channel_signal_output[0]\n",
    "\n",
    "time = np.arange(1800, 1801, 0.016)\n",
    "smr_index = np.searchsorted(Channel_signal_smr1['Time'].values, time)\n",
    "smr_subset = Channel_signal_smr1.iloc[smr_index]\n",
    "info_index = np.searchsorted(monkey_information['monkey_t'].values, time)\n",
    "info_subset = monkey_information.iloc[info_index]\n",
    "# plot monkey x and y\n",
    "plt.scatter(smr_subset['Time'], smr_subset['MonkeyY'], s=2, alpha=0.5)\n",
    "plt.scatter(info_subset['monkey_t'], info_subset['monkey_y'], s=2, alpha=0.5)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Monkey Y')\n",
    "# plot monkey x and y\n",
    "plt.legend(['smr', 'info'])\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(smr_subset['Time'], smr_subset['MonkeyX'], s=2, alpha=0.5)\n",
    "plt.scatter(info_subset['monkey_t'], info_subset['monkey_x'], s=2, alpha=0.5)\n",
    "plt.legend(['smr', 'info'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Monkey X')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1\n",
    "for currentTrial in range(100, 105):\n",
    "    print(currentTrial)\n",
    "    #duration = [ff_caught_T_new[currentTrial-num_trials], ff_caught_T_new[currentTrial]]\n",
    "    duration = [ff_caught_T_new[currentTrial]-1.25, ff_caught_T_new[currentTrial]]\n",
    "\n",
    "\n",
    "    plot_polar.PlotPolar(duration,\n",
    "              monkey_information,\n",
    "              ff_dataframe, \n",
    "              ff_life_sorted,\n",
    "              ff_real_position_sorted,\n",
    "              ff_caught_T_new,\n",
    "              ff_flash_sorted,\n",
    "              rmax = 100,\n",
    "              currentTrial = currentTrial,\n",
    "              num_trials = num_trials,\n",
    "              show_visible_ff = True,\n",
    "              show_visible_target = True,\n",
    "              # show_ff_in_memory = True,\n",
    "              # show_target_in_memory = True,\n",
    "              ff_colormap = 'viridis',\n",
    "              target_colormap = 'viridis',\n",
    "              show_alive_ff = True,\n",
    "              colors_show_overall_time = True,\n",
    "              show_all_positions_of_all_fireflies = True,\n",
    "              show_colorbar = True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot side_by_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_name = \"RL/SB3_stored_models/all_agents/A0.2_O4_ff2_memory3/\"\n",
    "env_kwargs = {\"num_obs_ff\": 5}\n",
    "env = env.CollectInformation(**env_kwargs)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# For direct training\n",
    "sac_model = SAC(\"MlpPolicy\", \n",
    "            env,\n",
    "            gamma=0.995,\n",
    "            learning_rate=0.0015,\n",
    "            batch_size=1024,\n",
    "            target_update_interval=50,\n",
    "            buffer_size=1000000,\n",
    "            learning_starts=10000,\n",
    "            train_freq=10,\n",
    "            ent_coef='auto',\n",
    "            policy_kwargs=dict(activation_fn=nn.Tanh, net_arch=[128, 128])\n",
    "                )\n",
    "path = os.path.join(model_folder_name, 'best_model.zip')\n",
    "sac_model = sac_model.load(path,env=env) \n",
    "\n",
    "\n",
    "with basic_func.HiddenPrints():\n",
    "  # Note: must use env(Simulated)\n",
    "\n",
    "  agent_dt = 0.25\n",
    "  num_trials = 2\n",
    "\n",
    "  plotting_params = {\"show_stops\": True,\n",
    "                    \"show_believed_target_positions\": True,\n",
    "                    \"show_reward_boundary\": True,\n",
    "                    \"show_connect_path_ff\": True,\n",
    "                    \"show_scale_bar\": True,\n",
    "                    \"hitting_arena_edge_ok\": True,\n",
    "                    \"trial_too_short_ok\": True}\n",
    "\n",
    "  for currentTrial in [12, 69, 138, 221, 235, 259, 263, 265, 299, 393, 496, 523, 556, 601, 666, 698, 760, 805, 808, 930, 946, 955, 1002, 1003]:\n",
    "      info_of_agent, plot_whole_duration, rotation_matrix, num_imitation_steps_monkey, num_imitation_steps_agent = collect_agent_data_utils.find_corresponding_info_of_agent(info_of_monkey, currentTrial, num_trials, sac_model, agent_dt, LSTM=False, env_kwargs=env_kwargs)\n",
    "\n",
    "      with basic_func.initiate_plot(20,20,400):\n",
    "          additional_plots.PlotSidebySide(plot_whole_duration = plot_whole_duration,\n",
    "                          info_of_monkey = info_of_monkey,\n",
    "                          info_of_agent = info_of_agent,  \n",
    "                          num_imitation_steps_monkey = num_imitation_steps_monkey,\n",
    "                          num_imitation_steps_agent = num_imitation_steps_agent,                \n",
    "                          currentTrial = currentTrial,\n",
    "                          num_trials = num_trials, \n",
    "                          rotation_matrix = rotation_matrix,              \n",
    "                          plotting_params = plotting_params,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new way to define an episode\n",
    "\n",
    "A new chunk happens when the monkey/agent changes its speed from below half of the full speed to above half of the full speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_wrangling import more_data_wrangling_func\n",
    "chunk_numbers, new_chunk_indices = more_data_wrangling_func.reorganize_data_into_chunks(monkey_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotTrials_args = (monkey_information, ff_dataframe, ff_life_sorted, ff_real_position_sorted, \\\n",
    "                   ff_believed_position_sorted, cluster_around_target_indices, ff_caught_T_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a chunk\n",
    "starting_chunk = 150\n",
    "num_of_chunks = 6\n",
    "\n",
    "cum_iloc_indices = np.where((chunk_numbers >= starting_chunk) & (chunk_numbers < starting_chunk+num_of_chunks))[0]\n",
    "duration = [monkey_information['monkey_t'][cum_iloc_indices[0]], monkey_information['monkey_t'][cum_iloc_indices[-1]]]\n",
    "\n",
    "\n",
    "with basic_func.initiate_plot(7, 7, 100):\n",
    "    plot_trials.PlotTrials(duration, \n",
    "                *PlotTrials_args,\n",
    "                trail_color_var = \"speed\",\n",
    "                show_stops = True,\n",
    "                show_believed_target_positions = True,\n",
    "                show_reward_boundary = True,\n",
    "                show_connect_path_ff = True,\n",
    "                show_scale_bar = True,\n",
    "                hitting_arena_edge_ok = True,\n",
    "                trial_too_short_ok = True,\n",
    "                show_title = False,\n",
    "                steps_to_be_marked = new_chunk_indices[starting_chunk+1:starting_chunk+num_of_chunks]-cum_iloc_indices[0],\n",
    "                )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    plt.show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve more data\n",
    "all_trial_features_lstm = pd.read_csv('RL_models/LSTM_stored_models/all_agents/gen_0/LSTM_Aug_1_24/patterns_and_features/all_trial_features.csv')\n",
    "all_trial_features_valid_lstm = all_trial_features_lstm[(all_trial_features_lstm['t_last_vis']<50) & (all_trial_features_lstm['hitting_arena_edge']==False)].reset_index()\n",
    "median_values_lstm = all_trial_features_valid_lstm.median(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what percentage of points in monkey_information['monkey_t'].diff() is below 0.015?\n",
    "t_diff = monkey_information['monkey_t'].diff()\n",
    "np.where(t_diff < 0.015)[0].shape[0]/t_diff.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find trials where two ff within 100 cm of each other appear at the same time\n",
    "trials_for_planning = []\n",
    "for trial in ff_dataframe_visible.target_index.unique():\n",
    "    ff_subset = ff_dataframe_visible[ff_dataframe_visible.target_index == trial]\n",
    "    ff_counting = ff_subset.groupby('point_index').count()\n",
    "    ff_counting = ff_counting[ff_counting.ff_index > 1]\n",
    "    point_index = ff_counting.point_index\n",
    "    #ff_subset = ff_subset[ff_subset.point_index.isin(point_index)]\n",
    "    # for each point index, see if the ff are within 100 of each other\n",
    "    for point_index in relevant_point_index:\n",
    "        current_ff = ff_subset[ff_subset.point_index == point_index]\n",
    "        # find if they're within 100 cm of each other\n",
    "        # ... and then I decided to use find_point_vs_cluster\n",
    "    if len(ff_subset) > 0:\n",
    "        trials_for_planning.append(trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -Uqq ipdb\n",
    "import ipdb\n",
    "%pdb on"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1G3e4Qx3gpEEDPI9pKYXBK_bxXCmF4ZmN",
     "timestamp": 1681009405002
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
